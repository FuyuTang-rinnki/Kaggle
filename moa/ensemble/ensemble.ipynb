{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004991,
     "end_time": "2020-11-30T00:44:47.692669",
     "exception": false,
     "start_time": "2020-11-30T00:44:47.687678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* python scripts -- each time it should clear the cache and reset the state\n",
    "* submit sample submission to pass through initial commit\n",
    "* run all inference codes for whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-30T00:44:47.708372Z",
     "iopub.status.busy": "2020-11-30T00:44:47.707578Z",
     "iopub.status.idle": "2020-11-30T00:44:47.710528Z",
     "shell.execute_reply": "2020-11-30T00:44:47.710008Z"
    },
    "papermill": {
     "duration": 0.013652,
     "end_time": "2020-11-30T00:44:47.710629",
     "exception": false,
     "start_time": "2020-11-30T00:44:47.696977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T00:44:47.729741Z",
     "iopub.status.busy": "2020-11-30T00:44:47.728926Z",
     "iopub.status.idle": "2020-11-30T00:45:53.910895Z",
     "shell.execute_reply": "2020-11-30T00:45:53.910303Z"
    },
    "papermill": {
     "duration": 66.196091,
     "end_time": "2020-11-30T00:45:53.911009",
     "exception": false,
     "start_time": "2020-11-30T00:44:47.714918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/iterativestratification\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.18.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.4.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (0.14.1)\r\n",
      "Building wheels for collected packages: iterative-stratification\r\n",
      "  Building wheel for iterative-stratification (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for iterative-stratification: filename=iterative_stratification-0.1.6-py3-none-any.whl size=8406 sha256=8d4e249572792cdb5e21fe81a66bf55c5dcd449792f87b34224909d46d6206b6\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7x1r7i4c/wheels/24/5f/1c/0d3ab7a5fd18343f97d642b6a029ebc204e2a777b877950562\r\n",
      "Successfully built iterative-stratification\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n",
      "Looking in links: /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.6.0)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.23.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.18.5)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.45.0)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.4.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (0.14.1)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-2.0.0\r\n",
      "Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.7/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow_addons) (2.10.0)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "# pre-download\n",
    "# Iterative Stratification\n",
    "!pip install /kaggle/input/iterativestratification/ #iterative-stratification-master/\n",
    "\n",
    "# TabNet\n",
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n",
    "\n",
    "# keras adamW\n",
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T00:45:53.937362Z",
     "iopub.status.busy": "2020-11-30T00:45:53.936213Z",
     "iopub.status.idle": "2020-11-30T00:45:54.975536Z",
     "shell.execute_reply": "2020-11-30T00:45:54.976728Z"
    },
    "papermill": {
     "duration": 1.055421,
     "end_time": "2020-11-30T00:45:54.976912",
     "exception": false,
     "start_time": "2020-11-30T00:45:53.921491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-30T00:45:55.018342Z",
     "iopub.status.busy": "2020-11-30T00:45:55.017499Z",
     "iopub.status.idle": "2020-11-30T02:12:50.114744Z",
     "shell.execute_reply": "2020-11-30T02:12:50.113660Z"
    },
    "papermill": {
     "duration": 5215.121542,
     "end_time": "2020-11-30T02:12:50.114894",
     "exception": false,
     "start_time": "2020-11-30T00:45:54.993352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m not_ctl\r\n",
      "\u001b[34m Rank Gauss\r\n",
      "\u001b[34m PCA\r\n",
      "\u001b[34m One-Hot\r\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:04<00:00,  1.07it/s]\r\n",
      "\u001b[34mtrain_df.shape: \u001b[31m(21948, 947)\r\n",
      "\u001b[34mtest_df.shape: \u001b[31m(3624, 947)\r\n",
      "\u001b[34mX_test.shape: \u001b[31m(3624, 947)\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 1\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29986 | val_logits_ll: 0.02831 |  0:00:02s\r\n",
      "epoch 10 | loss: 0.01891 | val_logits_ll: 0.02104 |  0:00:24s\r\n",
      "epoch 20 | loss: 0.01746 | val_logits_ll: 0.01855 |  0:00:45s\r\n",
      "epoch 30 | loss: 0.017   | val_logits_ll: 0.01706 |  0:01:05s\r\n",
      "epoch 40 | loss: 0.01681 | val_logits_ll: 0.01676 |  0:01:27s\r\n",
      "epoch 50 | loss: 0.01649 | val_logits_ll: 0.01681 |  0:01:47s\r\n",
      "epoch 60 | loss: 0.01621 | val_logits_ll: 0.01676 |  0:02:07s\r\n",
      "epoch 70 | loss: 0.01589 | val_logits_ll: 0.01677 |  0:02:29s\r\n",
      "epoch 80 | loss: 0.01557 | val_logits_ll: 0.01671 |  0:02:49s\r\n",
      "epoch 90 | loss: 0.01536 | val_logits_ll: 0.0167  |  0:03:10s\r\n",
      "epoch 100| loss: 0.01491 | val_logits_ll: 0.01661 |  0:03:31s\r\n",
      "epoch 110| loss: 0.01462 | val_logits_ll: 0.01674 |  0:03:52s\r\n",
      "\r\n",
      "Early stopping occured at epoch 112 with best_epoch = 92 and best_val_logits_ll = 0.01652\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 2\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29695 | val_logits_ll: 0.02828 |  0:00:01s\r\n",
      "epoch 10 | loss: 0.01941 | val_logits_ll: 0.01938 |  0:00:23s\r\n",
      "epoch 20 | loss: 0.0177  | val_logits_ll: 0.01777 |  0:00:44s\r\n",
      "epoch 30 | loss: 0.01728 | val_logits_ll: 0.01741 |  0:01:04s\r\n",
      "epoch 40 | loss: 0.01691 | val_logits_ll: 0.01692 |  0:01:26s\r\n",
      "epoch 50 | loss: 0.01651 | val_logits_ll: 0.01684 |  0:01:46s\r\n",
      "epoch 60 | loss: 0.01658 | val_logits_ll: 0.01669 |  0:02:06s\r\n",
      "epoch 70 | loss: 0.01613 | val_logits_ll: 0.01659 |  0:02:28s\r\n",
      "epoch 80 | loss: 0.01605 | val_logits_ll: 0.01669 |  0:02:49s\r\n",
      "epoch 90 | loss: 0.01598 | val_logits_ll: 0.01672 |  0:03:09s\r\n",
      "\r\n",
      "Early stopping occured at epoch 99 with best_epoch = 79 and best_val_logits_ll = 0.01648\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 3\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29675 | val_logits_ll: 0.02843 |  0:00:01s\r\n",
      "epoch 10 | loss: 0.01888 | val_logits_ll: 0.02131 |  0:00:22s\r\n",
      "epoch 20 | loss: 0.0174  | val_logits_ll: 0.01778 |  0:00:42s\r\n",
      "epoch 30 | loss: 0.01695 | val_logits_ll: 0.0183  |  0:01:04s\r\n",
      "epoch 40 | loss: 0.01661 | val_logits_ll: 0.01698 |  0:01:24s\r\n",
      "epoch 50 | loss: 0.01653 | val_logits_ll: 0.01705 |  0:01:45s\r\n",
      "epoch 60 | loss: 0.01614 | val_logits_ll: 0.01683 |  0:02:06s\r\n",
      "epoch 70 | loss: 0.01588 | val_logits_ll: 0.01688 |  0:02:26s\r\n",
      "epoch 80 | loss: 0.01561 | val_logits_ll: 0.01663 |  0:02:48s\r\n",
      "epoch 90 | loss: 0.01527 | val_logits_ll: 0.01674 |  0:03:08s\r\n",
      "epoch 100| loss: 0.01492 | val_logits_ll: 0.01673 |  0:03:28s\r\n",
      "epoch 110| loss: 0.0149  | val_logits_ll: 0.01689 |  0:03:50s\r\n",
      "\r\n",
      "Early stopping occured at epoch 113 with best_epoch = 93 and best_val_logits_ll = 0.01656\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 4\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29831 | val_logits_ll: 0.02814 |  0:00:02s\r\n",
      "epoch 10 | loss: 0.01904 | val_logits_ll: 0.01856 |  0:00:22s\r\n",
      "epoch 20 | loss: 0.01739 | val_logits_ll: 0.01918 |  0:00:43s\r\n",
      "epoch 30 | loss: 0.01697 | val_logits_ll: 0.01758 |  0:01:04s\r\n",
      "epoch 40 | loss: 0.01664 | val_logits_ll: 0.01693 |  0:01:24s\r\n",
      "epoch 50 | loss: 0.01636 | val_logits_ll: 0.01706 |  0:01:45s\r\n",
      "epoch 60 | loss: 0.0161  | val_logits_ll: 0.01667 |  0:02:06s\r\n",
      "epoch 70 | loss: 0.01595 | val_logits_ll: 0.01666 |  0:02:27s\r\n",
      "epoch 80 | loss: 0.01574 | val_logits_ll: 0.01665 |  0:02:47s\r\n",
      "epoch 90 | loss: 0.01551 | val_logits_ll: 0.01655 |  0:03:09s\r\n",
      "epoch 100| loss: 0.01518 | val_logits_ll: 0.01661 |  0:03:29s\r\n",
      "\r\n",
      "Early stopping occured at epoch 106 with best_epoch = 86 and best_val_logits_ll = 0.01651\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 5\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29755 | val_logits_ll: 0.0282  |  0:00:02s\r\n",
      "epoch 10 | loss: 0.01869 | val_logits_ll: 0.01855 |  0:00:25s\r\n",
      "epoch 20 | loss: 0.01743 | val_logits_ll: 0.02198 |  0:00:46s\r\n",
      "epoch 30 | loss: 0.01706 | val_logits_ll: 0.01832 |  0:01:08s\r\n",
      "epoch 40 | loss: 0.01672 | val_logits_ll: 0.01722 |  0:01:31s\r\n",
      "epoch 50 | loss: 0.01653 | val_logits_ll: 0.017   |  0:01:53s\r\n",
      "epoch 60 | loss: 0.01632 | val_logits_ll: 0.01712 |  0:02:15s\r\n",
      "epoch 70 | loss: 0.01615 | val_logits_ll: 0.0169  |  0:02:37s\r\n",
      "epoch 80 | loss: 0.01595 | val_logits_ll: 0.01667 |  0:02:59s\r\n",
      "epoch 90 | loss: 0.01577 | val_logits_ll: 0.01689 |  0:03:22s\r\n",
      "epoch 100| loss: 0.01569 | val_logits_ll: 0.01682 |  0:03:44s\r\n",
      "epoch 110| loss: 0.01535 | val_logits_ll: 0.01687 |  0:04:06s\r\n",
      "epoch 120| loss: 0.01523 | val_logits_ll: 0.01668 |  0:04:29s\r\n",
      "epoch 130| loss: 0.01494 | val_logits_ll: 0.01659 |  0:04:50s\r\n",
      "epoch 140| loss: 0.01458 | val_logits_ll: 0.01676 |  0:05:13s\r\n",
      "\r\n",
      "Early stopping occured at epoch 143 with best_epoch = 123 and best_val_logits_ll = 0.01651\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 6\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29951 | val_logits_ll: 0.0294  |  0:00:01s\r\n",
      "epoch 10 | loss: 0.01919 | val_logits_ll: 0.01876 |  0:00:23s\r\n",
      "epoch 20 | loss: 0.01744 | val_logits_ll: 0.02274 |  0:00:46s\r\n",
      "epoch 30 | loss: 0.01692 | val_logits_ll: 0.01704 |  0:01:07s\r\n",
      "epoch 40 | loss: 0.01681 | val_logits_ll: 0.01672 |  0:01:29s\r\n",
      "epoch 50 | loss: 0.01668 | val_logits_ll: 0.01726 |  0:01:52s\r\n",
      "epoch 60 | loss: 0.01647 | val_logits_ll: 0.0168  |  0:02:14s\r\n",
      "epoch 70 | loss: 0.01625 | val_logits_ll: 0.01658 |  0:02:35s\r\n",
      "epoch 80 | loss: 0.01605 | val_logits_ll: 0.01651 |  0:02:58s\r\n",
      "epoch 90 | loss: 0.01561 | val_logits_ll: 0.01677 |  0:03:20s\r\n",
      "epoch 100| loss: 0.01551 | val_logits_ll: 0.01708 |  0:03:42s\r\n",
      "epoch 110| loss: 0.01516 | val_logits_ll: 0.01649 |  0:04:04s\r\n",
      "epoch 120| loss: 0.01482 | val_logits_ll: 0.01647 |  0:04:26s\r\n",
      "\r\n",
      "Early stopping occured at epoch 122 with best_epoch = 102 and best_val_logits_ll = 0.01629\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 7\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29704 | val_logits_ll: 0.0286  |  0:00:02s\r\n",
      "epoch 10 | loss: 0.01885 | val_logits_ll: 0.01856 |  0:00:25s\r\n",
      "epoch 20 | loss: 0.01739 | val_logits_ll: 0.01733 |  0:00:46s\r\n",
      "epoch 30 | loss: 0.01692 | val_logits_ll: 0.01703 |  0:01:08s\r\n",
      "epoch 40 | loss: 0.01671 | val_logits_ll: 0.01687 |  0:01:31s\r\n",
      "epoch 50 | loss: 0.0164  | val_logits_ll: 0.01675 |  0:01:53s\r\n",
      "epoch 60 | loss: 0.01611 | val_logits_ll: 0.01679 |  0:02:16s\r\n",
      "epoch 70 | loss: 0.01583 | val_logits_ll: 0.01662 |  0:02:38s\r\n",
      "epoch 80 | loss: 0.01557 | val_logits_ll: 0.01658 |  0:03:00s\r\n",
      "epoch 90 | loss: 0.01555 | val_logits_ll: 0.01668 |  0:03:23s\r\n",
      "epoch 100| loss: 0.01519 | val_logits_ll: 0.01659 |  0:03:45s\r\n",
      "\r\n",
      "Early stopping occured at epoch 106 with best_epoch = 86 and best_val_logits_ll = 0.0164\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 8\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29679 | val_logits_ll: 0.02883 |  0:00:01s\r\n",
      "epoch 10 | loss: 0.0188  | val_logits_ll: 0.01844 |  0:00:23s\r\n",
      "epoch 20 | loss: 0.01744 | val_logits_ll: 0.01762 |  0:00:43s\r\n",
      "epoch 30 | loss: 0.01722 | val_logits_ll: 0.01725 |  0:01:04s\r\n",
      "epoch 40 | loss: 0.01685 | val_logits_ll: 0.01724 |  0:01:25s\r\n",
      "epoch 50 | loss: 0.01635 | val_logits_ll: 0.01696 |  0:01:45s\r\n",
      "epoch 60 | loss: 0.01615 | val_logits_ll: 0.01688 |  0:02:06s\r\n",
      "epoch 70 | loss: 0.01603 | val_logits_ll: 0.01678 |  0:02:27s\r\n",
      "epoch 80 | loss: 0.0157  | val_logits_ll: 0.01667 |  0:02:48s\r\n",
      "epoch 90 | loss: 0.01539 | val_logits_ll: 0.01669 |  0:03:09s\r\n",
      "epoch 100| loss: 0.01523 | val_logits_ll: 0.01672 |  0:03:30s\r\n",
      "epoch 110| loss: 0.01486 | val_logits_ll: 0.01658 |  0:03:50s\r\n",
      "\r\n",
      "Early stopping occured at epoch 117 with best_epoch = 97 and best_val_logits_ll = 0.01648\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 9\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29672 | val_logits_ll: 0.02885 |  0:00:02s\r\n",
      "epoch 10 | loss: 0.01872 | val_logits_ll: 0.02207 |  0:00:24s\r\n",
      "epoch 20 | loss: 0.01752 | val_logits_ll: 0.01782 |  0:00:46s\r\n",
      "epoch 30 | loss: 0.0172  | val_logits_ll: 0.01759 |  0:01:09s\r\n",
      "epoch 40 | loss: 0.01685 | val_logits_ll: 0.01711 |  0:01:32s\r\n",
      "epoch 50 | loss: 0.01669 | val_logits_ll: 0.01703 |  0:01:53s\r\n",
      "epoch 60 | loss: 0.01636 | val_logits_ll: 0.0169  |  0:02:17s\r\n",
      "epoch 70 | loss: 0.01623 | val_logits_ll: 0.01682 |  0:02:39s\r\n",
      "epoch 80 | loss: 0.01597 | val_logits_ll: 0.01669 |  0:03:00s\r\n",
      "epoch 90 | loss: 0.01584 | val_logits_ll: 0.01673 |  0:03:24s\r\n",
      "epoch 100| loss: 0.01569 | val_logits_ll: 0.01676 |  0:03:46s\r\n",
      "\r\n",
      "Early stopping occured at epoch 103 with best_epoch = 83 and best_val_logits_ll = 0.01661\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34m FOLDS:  \u001b[31m 10\r\n",
      "\u001b[32m ************************************************************ \u001b[36m\r\n",
      "Device used : cuda\r\n",
      "epoch 0  | loss: 0.29918 | val_logits_ll: 0.02967 |  0:00:02s\r\n",
      "epoch 10 | loss: 0.01899 | val_logits_ll: 0.01883 |  0:00:25s\r\n",
      "epoch 20 | loss: 0.01753 | val_logits_ll: 0.02046 |  0:00:47s\r\n",
      "epoch 30 | loss: 0.01709 | val_logits_ll: 0.0177  |  0:01:09s\r\n",
      "epoch 40 | loss: 0.01676 | val_logits_ll: 0.01717 |  0:01:32s\r\n",
      "epoch 50 | loss: 0.01648 | val_logits_ll: 0.01683 |  0:01:54s\r\n",
      "epoch 60 | loss: 0.01627 | val_logits_ll: 0.01682 |  0:02:17s\r\n",
      "epoch 70 | loss: 0.016   | val_logits_ll: 0.01691 |  0:02:39s\r\n",
      "epoch 80 | loss: 0.01574 | val_logits_ll: 0.01693 |  0:03:01s\r\n",
      "epoch 90 | loss: 0.01549 | val_logits_ll: 0.01673 |  0:03:24s\r\n",
      "\r\n",
      "Early stopping occured at epoch 99 with best_epoch = 79 and best_val_logits_ll = 0.01664\r\n",
      "Best weights from best epoch are automatically used!\r\n",
      "\u001b[33m ------------------------------------------------------------\r\n",
      "\u001b[34mOverall AUC: \u001b[31m0.7538880140512061\r\n",
      "\u001b[34mAverage CV: \u001b[31m0.016500671429559147\r\n",
      "\u001b[34msubmission.shape: \u001b[31m(3982, 207)\r\n",
      "Memory usage of properties dataframe is : 159.15740966796875  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  79.71503067016602  MB\r\n",
      "This is  50.08565472161556 % of the initial size\r\n",
      "Memory usage of properties dataframe is : 37.60920715332031  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  4.860233306884766  MB\r\n",
      "This is  12.922987945667666 % of the initial size\r\n",
      "Memory usage of properties dataframe is : 26.61322021484375  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  13.340848922729492  MB\r\n",
      "This is  50.128653409963974 % of the initial size\r\n",
      "transformers_list:  [CatIntMapper(col='cp_type', dicti={'ctl_vehicle': 0, 'trt_cp': 1}), CatIntMapper(col='cp_dose', dicti={'D1': 1, 'D2': 0}), CatIntMapper(col='cp_time', dicti={24: 0, 48: 1, 72: 2}), NamedOutTWrapper(columns=None, inplace=True,\r\n",
      "                 transformer=QuantileTransformer(n_quantiles=100,\r\n",
      "                                                 output_distribution='normal',\r\n",
      "                                                 random_state=0)), ColumnDropper(cols=['cp_type', 'cp_time', 'cp_dose'])]\r\n",
      "Memory usage of properties dataframe is : 159.15740966796875  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  79.71503067016602  MB\r\n",
      "This is  50.08565472161556 % of the initial size\r\n",
      "Memory usage of properties dataframe is : 37.60920715332031  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  4.860233306884766  MB\r\n",
      "This is  12.922987945667666 % of the initial size\r\n",
      "Memory usage of properties dataframe is : 26.61322021484375  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  13.340848922729492  MB\r\n",
      "This is  50.128653409963974 % of the initial size\r\n",
      "pred_[1].shape: (27796, 3500)\r\n",
      "transformers_list:  [CatIntMapper(col='cp_type', dicti={'ctl_vehicle': 0, 'trt_cp': 1}), CatIntMapper(col='cp_dose', dicti={'D1': 1, 'D2': 0}), CatIntMapper(col='cp_time', dicti={24: 0, 48: 1, 72: 2}), NamedOutTWrapper(columns=None, inplace=True,\r\n",
      "                 transformer=QuantileTransformer(n_quantiles=100,\r\n",
      "                                                 output_distribution='normal',\r\n",
      "                                                 random_state=0)), ColumnDropper(cols=['cp_type', 'cp_time', 'cp_dose'])]\r\n",
      "len(train):  27796\r\n",
      "                  0         1         2     ...      3497      3498      3499\r\n",
      "sig_id                                      ...                              \r\n",
      "id_000644bb2  0.312471  0.332505  0.107200  ...  0.000000  0.110244  1.430548\r\n",
      "id_000779bfc  0.348509  0.341994  0.271660  ...  2.171434  0.204184  0.226142\r\n",
      "id_000a6266a  0.399113  0.339122  0.093491  ...  0.843078  0.788198  2.029073\r\n",
      "id_0015fd391  0.476005  0.326737  0.036821  ...  0.038635  0.000000  0.000000\r\n",
      "id_001626bd3  0.397296  0.298150  0.155458  ...  0.341874  1.257402  1.602842\r\n",
      "\r\n",
      "[5 rows x 3500 columns]\r\n",
      "296   2764    9.524943e-01\r\n",
      "      749     9.402584e-01\r\n",
      "624   3244    9.303066e-01\r\n",
      "749   2764    9.189305e-01\r\n",
      "296   3244    9.145282e-01\r\n",
      "                  ...     \r\n",
      "1479  3325    9.563704e-08\r\n",
      "3005  3203    7.229718e-08\r\n",
      "443   1841    5.911287e-08\r\n",
      "283   1856    1.678684e-08\r\n",
      "2243  2544    7.852762e-09\r\n",
      "Length: 6123250, dtype: float64\r\n",
      "3571\r\n",
      "Index(['296', '624', '749', '1008', '210', '2764', '223', '28', '2999', '267',\r\n",
      "       ...\r\n",
      "       '1104', '1145', '533', '1479', '2174', '1290', '1038', '1424', '434',\r\n",
      "       '1914'],\r\n",
      "      dtype='object', length=445)\r\n",
      "['296', '624', '749', '1008', '210', '2764', '223', '28', '2999', '267', '363', '793', '889', '1451', '1465', '360', '201', '1529', '1707', '230', '483', '299', '15', '722', '262', '1568', '1555', '583', '505', '2334', '6', '968', '2069', '1483', '94', '1216', '1922', '613', '944', '691', '1956', '3311', '3244', '288', '611', '3353', '1629', '161', '1835', '1498', '1231', '70', '2349', '2962', '1551', '1280', '2197', '1792', '2228', '2490', '1666', '92', '642', '1525', '2140', '278', '338', '1640', '713', '253', '2110', '1664', '962', '2436', '57', '143', '286', '240', '1772', '82', '1209', '2267', '2178', '3115', '1959', '471', '1307', '1018', '2181', '1822', '1191', '26', '553', '579', '943', '119', '349', '1606', '2687', '2183', '1648', '928', '1347', '2575', '536', '1728', '1156', '1153', '732', '2393', '764', '1389', '391', '2100', '2385', '1234', '1275', '116', '2045', '1390', '1405', '173', '2122', '727', '796', '750', '1167', '1441', '36', '1811', '1506', '1497', '2057', '672', '2042', '656', '1636', '1454', '2033', '1042', '1592', '1222', '1547', '857', '2286', '1958', '2344', '1422', '2306', '1186', '1691', '1940', '2020', '1611', '400', '1432', '1461', '1983', '1767', '1858', '1896', '1735', '1504', '1779', '996', '41', '114', '1880', '1881', '1103', '1455', '144', '617', '242', '90', '1178', '871', '904', '1106', '913', '517', '1163', '123', '275', '2237', '3212', '482', '1710', '633', '2086', '971', '390', '2063', '2333', '538', '661', '1433', '1138', '1889', '2055', '1471', '379', '717', '274', '1702', '2202', '3001', '1785', '1195', '101', '1157', '247', '1435', '3216', '3205', '440', '2119', '1603', '743', '1689', '3012', '1591', '187', '1524', '332', '197', '1848', '1746', '1971', '2413', '2006', '1019', '816', '921', '341', '2382', '2115', '565', '1075', '1586', '1364', '2146', '1252', '2245', '2124', '2073', '1967', '2722', '897', '3000', '1477', '653', '2070', '2081', '64', '1580', '1305', '1972', '2173', '1986', '1834', '2857', '2044', '1546', '1823', '1670', '2417', '980', '160', '393', '465', '1193', '1642', '1237', '2208', '2901', '843', '344', '2718', '1775', '1699', '126', '199', '1684', '298', '3137', '23', '1894', '541', '934', '959', '1866', '1254', '2407', '1025', '1668', '2098', '1302', '3471', '678', '614', '1658', '868', '1537', '1219', '359', '1803', '2570', '2251', '12', '2832', '1686', '494', '2757', '1673', '593', '693', '1852', '1752', '348', '2860', '556', '1511', '1381', '582', '1548', '1659', '478', '1380', '507', '1862', '2144', '2529', '2243', '3249', '152', '806', '1598', '634', '2577', '2882', '585', '956', '1488', '1508', '2518', '1349', '364', '2364', '307', '768', '1408', '2043', '2076', '87', '3063', '1166', '1118', '2885', '922', '1830', '214', '86', '194', '2335', '1348', '1614', '1187', '576', '2483', '833', '2141', '115', '910', '555', '106', '2012', '2112', '3125', '320', '1681', '2527', '1212', '1197', '2253', '2797', '882', '853', '1733', '2323', '619', '1874', '1859', '2336', '1916', '1223', '2223', '1427', '618', '1989', '2753', '1845', '1841', '907', '1784', '2636', '800', '265', '597', '1140', '668', '1124', '1631', '2309', '872', '1117', '1279', '2390', '2955', '287', '1712', '52', '786', '1470', '1774', '376', '3028', '3165', '1882', '1003', '2293', '1123', '655', '2305', '1647', '1104', '1145', '533', '1479', '2174', '1290', '1038', '1424', '434', '1914']\r\n",
      "559\r\n",
      "Memory usage of properties dataframe is : 159.15740966796875  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  79.71503067016602  MB\r\n",
      "This is  50.08565472161556 % of the initial size\r\n",
      "Memory usage of properties dataframe is : 37.60920715332031  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  4.860233306884766  MB\r\n",
      "This is  12.922987945667666 % of the initial size\r\n",
      "transformers_list:  [CatIntMapper(col='cp_dose', dicti={'D1': 1, 'D2': 0}), CatIntMapper(col='cp_time', dicti={24: 0, 48: 1, 72: 2}), DaeAdder(filename='features_0.2_altogether.fth'), SuppressControls()]\r\n",
      "Memory usage of properties dataframe is : 159.15740966796875  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  79.71503067016602  MB\r\n",
      "This is  50.08565472161556 % of the initial size\r\n",
      "Memory usage of properties dataframe is : 37.60920715332031  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  4.860233306884766  MB\r\n",
      "This is  12.922987945667666 % of the initial size\r\n",
      "Memory usage of properties dataframe is : 26.61322021484375  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  13.340848922729492  MB\r\n",
      "This is  50.128653409963974 % of the initial size\r\n",
      "['5-alpha_reductase_inhibitor', '11-beta-hsd1_inhibitor', 'acat_inhibitor', 'acetylcholine_receptor_agonist', 'acetylcholine_receptor_antagonist', 'acetylcholinesterase_inhibitor', 'adenosine_receptor_agonist', 'adenosine_receptor_antagonist', 'adenylyl_cyclase_activator', 'adrenergic_receptor_agonist', 'adrenergic_receptor_antagonist', 'akt_inhibitor', 'aldehyde_dehydrogenase_inhibitor', 'alk_inhibitor', 'ampk_activator', 'analgesic', 'androgen_receptor_agonist', 'androgen_receptor_antagonist', 'anesthetic_-_local', 'angiogenesis_inhibitor', 'angiotensin_receptor_antagonist', 'anti-inflammatory', 'antiarrhythmic', 'antibiotic', 'anticonvulsant', 'antifungal', 'antihistamine', 'antimalarial', 'antioxidant', 'antiprotozoal', 'antiviral', 'apoptosis_stimulant', 'aromatase_inhibitor', 'atm_kinase_inhibitor', 'atp-sensitive_potassium_channel_antagonist', 'atp_synthase_inhibitor', 'atpase_inhibitor', 'atr_kinase_inhibitor', 'aurora_kinase_inhibitor', 'autotaxin_inhibitor', 'bacterial_30s_ribosomal_subunit_inhibitor', 'bacterial_50s_ribosomal_subunit_inhibitor', 'bacterial_antifolate', 'bacterial_cell_wall_synthesis_inhibitor', 'bacterial_dna_gyrase_inhibitor', 'bacterial_dna_inhibitor', 'bacterial_membrane_integrity_inhibitor', 'bcl_inhibitor', 'bcr-abl_inhibitor', 'benzodiazepine_receptor_agonist', 'beta_amyloid_inhibitor', 'bromodomain_inhibitor', 'btk_inhibitor', 'calcineurin_inhibitor', 'calcium_channel_blocker', 'cannabinoid_receptor_agonist', 'cannabinoid_receptor_antagonist', 'carbonic_anhydrase_inhibitor', 'casein_kinase_inhibitor', 'caspase_activator', 'catechol_o_methyltransferase_inhibitor', 'cc_chemokine_receptor_antagonist', 'cck_receptor_antagonist', 'cdk_inhibitor', 'chelating_agent', 'chk_inhibitor', 'chloride_channel_blocker', 'cholesterol_inhibitor', 'cholinergic_receptor_antagonist', 'coagulation_factor_inhibitor', 'corticosteroid_agonist', 'cyclooxygenase_inhibitor', 'cytochrome_p450_inhibitor', 'dihydrofolate_reductase_inhibitor', 'dipeptidyl_peptidase_inhibitor', 'diuretic', 'dna_alkylating_agent', 'dna_inhibitor', 'dopamine_receptor_agonist', 'dopamine_receptor_antagonist', 'egfr_inhibitor', 'elastase_inhibitor', 'erbb2_inhibitor', 'estrogen_receptor_agonist', 'estrogen_receptor_antagonist', 'faah_inhibitor', 'farnesyltransferase_inhibitor', 'fatty_acid_receptor_agonist', 'fgfr_inhibitor', 'flt3_inhibitor', 'focal_adhesion_kinase_inhibitor', 'free_radical_scavenger', 'fungal_squalene_epoxidase_inhibitor', 'gaba_receptor_agonist', 'gaba_receptor_antagonist', 'gamma_secretase_inhibitor', 'glucocorticoid_receptor_agonist', 'glutamate_inhibitor', 'glutamate_receptor_agonist', 'glutamate_receptor_antagonist', 'gonadotropin_receptor_agonist', 'gsk_inhibitor', 'hcv_inhibitor', 'hdac_inhibitor', 'histamine_receptor_agonist', 'histamine_receptor_antagonist', 'histone_lysine_demethylase_inhibitor', 'histone_lysine_methyltransferase_inhibitor', 'hiv_inhibitor', 'hmgcr_inhibitor', 'hsp_inhibitor', 'igf-1_inhibitor', 'ikk_inhibitor', 'imidazoline_receptor_agonist', 'immunosuppressant', 'insulin_secretagogue', 'insulin_sensitizer', 'integrin_inhibitor', 'jak_inhibitor', 'kit_inhibitor', 'laxative', 'leukotriene_inhibitor', 'leukotriene_receptor_antagonist', 'lipase_inhibitor', 'lipoxygenase_inhibitor', 'lxr_agonist', 'mdm_inhibitor', 'mek_inhibitor', 'membrane_integrity_inhibitor', 'mineralocorticoid_receptor_antagonist', 'monoacylglycerol_lipase_inhibitor', 'monoamine_oxidase_inhibitor', 'monopolar_spindle_1_kinase_inhibitor', 'mtor_inhibitor', 'mucolytic_agent', 'neuropeptide_receptor_antagonist', 'nfkb_inhibitor', 'nicotinic_receptor_agonist', 'nitric_oxide_donor', 'nitric_oxide_production_inhibitor', 'nitric_oxide_synthase_inhibitor', 'norepinephrine_reuptake_inhibitor', 'nrf2_activator', 'opioid_receptor_agonist', 'opioid_receptor_antagonist', 'orexin_receptor_antagonist', 'p38_mapk_inhibitor', 'p-glycoprotein_inhibitor', 'parp_inhibitor', 'pdgfr_inhibitor', 'pdk_inhibitor', 'phosphodiesterase_inhibitor', 'phospholipase_inhibitor', 'pi3k_inhibitor', 'pkc_inhibitor', 'potassium_channel_activator', 'potassium_channel_antagonist', 'ppar_receptor_agonist', 'ppar_receptor_antagonist', 'progesterone_receptor_agonist', 'progesterone_receptor_antagonist', 'prostaglandin_inhibitor', 'prostanoid_receptor_antagonist', 'proteasome_inhibitor', 'protein_kinase_inhibitor', 'protein_phosphatase_inhibitor', 'protein_synthesis_inhibitor', 'protein_tyrosine_kinase_inhibitor', 'radiopaque_medium', 'raf_inhibitor', 'ras_gtpase_inhibitor', 'retinoid_receptor_agonist', 'retinoid_receptor_antagonist', 'rho_associated_kinase_inhibitor', 'ribonucleoside_reductase_inhibitor', 'rna_polymerase_inhibitor', 'serotonin_receptor_agonist', 'serotonin_receptor_antagonist', 'serotonin_reuptake_inhibitor', 'sigma_receptor_agonist', 'sigma_receptor_antagonist', 'smoothened_receptor_antagonist', 'sodium_channel_inhibitor', 'sphingosine_receptor_agonist', 'src_inhibitor', 'steroid', 'syk_inhibitor', 'tachykinin_antagonist', 'tgf-beta_receptor_inhibitor', 'thrombin_inhibitor', 'thymidylate_synthase_inhibitor', 'tlr_agonist', 'tlr_antagonist', 'tnf_inhibitor', 'topoisomerase_inhibitor', 'transient_receptor_potential_channel_antagonist', 'tropomyosin_receptor_kinase_inhibitor', 'trpv_agonist', 'trpv_antagonist', 'tubulin_inhibitor', 'tyrosine_kinase_inhibitor', 'ubiquitin_specific_protease_inhibitor', 'vegfr_inhibitor', 'vitamin_b', 'vitamin_d_receptor_agonist', 'wnt_inhibitor'] <class 'list'>\r\n",
      "(21948, 4580) (21948, 206)\r\n",
      "[[ 1.06200004  0.55769998 -0.24789999 ...  0.          0.11024351\r\n",
      "   1.43054771]\r\n",
      " [ 0.62800002  0.58170003  1.55400002 ...  0.84307814  0.78819847\r\n",
      "   2.02907324]\r\n",
      " [-0.32539999 -0.40090001  0.97000003 ...  0.34187368  1.2574017\r\n",
      "   1.60284221]\r\n",
      " ...\r\n",
      " [-1.32599998  0.34779999 -0.3743     ...  1.26070809  0.87310863\r\n",
      "   0.9898954 ]\r\n",
      " [ 0.66600001  0.2324      0.43920001 ...  0.6778475   0.\r\n",
      "   0.        ]\r\n",
      " [-0.85979998  1.02400005 -0.13609999 ...  2.13170838  0.\r\n",
      "   0.20217147]] [[ 0.0743      0.40869999  0.29910001 ...  2.17143393  0.20418364\r\n",
      "   0.22614196]\r\n",
      " [-0.51380002 -0.2491     -0.2656     ...  0.03863505  0.\r\n",
      "   0.        ]\r\n",
      " [-0.30140001  0.55449998 -0.25760001 ...  0.          1.2470386\r\n",
      "   0.81515294]\r\n",
      " ...\r\n",
      " [ 0.34799999  0.87650001 -0.7967     ...  1.84707177  0.9684664\r\n",
      "   1.10156596]\r\n",
      " [-1.01400006  0.1709     -0.42910001 ...  0.14520019  0.30283803\r\n",
      "   0.46312499]\r\n",
      " [ 1.73800004 -1.28999996 -0.4533     ...  0.98301733  0.41434842\r\n",
      "   0.2277672 ]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]]\r\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.6204724934052777\r\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.14180094003677368\r\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.03902050825087605\r\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.020112218621831674\r\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021675571472056815\r\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.018554349788106404\r\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021099812426679843\r\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.039162395092157215\r\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.02091660290151029\r\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.01737144747032569\r\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.01986908887487811\r\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.01730594218063813\r\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.019690030833353866\r\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.017225919434657462\r\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.019533401481001765\r\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.01707064639776945\r\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.019395130032019037\r\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.01677240044451677\r\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.019242084031370846\r\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.016757880695737325\r\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019173775019275176\r\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.016841465320724707\r\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.01907120823759485\r\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.01706015827277532\r\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.018973244253445317\r\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.016561696090950415\r\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.018799923385518627\r\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.01655805132423456\r\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.018662142793874483\r\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.0164399712274854\r\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01838064246584435\r\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.016359766372121297\r\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.018094293206829478\r\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.016515647061169147\r\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.0176695475505816\r\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016354431684773702\r\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.017246763070894254\r\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.016384324775292322\r\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01670120674109942\r\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.016346323017317515\r\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.015903599623187974\r\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016509195144933004\r\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.015113852578341155\r\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016483767602879267\r\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.014278411789721734\r\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.016539721176601373\r\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.013479280826711171\r\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.016525903358482398\r\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.012930098505741035\r\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.016580599646728773\r\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.012713895783432433\r\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.016604170776330508\r\n",
      "[[ 1.06200004  0.55769998 -0.24789999 ...  0.          0.11024351\r\n",
      "   1.43054771]\r\n",
      " [ 0.0743      0.40869999  0.29910001 ...  2.17143393  0.20418364\r\n",
      "   0.22614196]\r\n",
      " [ 0.62800002  0.58170003  1.55400002 ...  0.84307814  0.78819847\r\n",
      "   2.02907324]\r\n",
      " ...\r\n",
      " [ 0.1608     -1.04999995  0.25510001 ...  0.43850875  0.0486182\r\n",
      "   0.6049757 ]\r\n",
      " [-1.32599998  0.34779999 -0.3743     ...  1.26070809  0.87310863\r\n",
      "   0.9898954 ]\r\n",
      " [-0.85979998  1.02400005 -0.13609999 ...  2.13170838  0.\r\n",
      "   0.20217147]] [[-0.1119      0.90030003  0.39109999 ...  0.          0.21905027\r\n",
      "   0.79283977]\r\n",
      " [ 0.61110002 -0.29069999 -0.78530002 ...  0.          0.86657876\r\n",
      "   0.        ]\r\n",
      " [ 0.54970002 -0.91839999  0.95929998 ...  1.78609943  0.51228315\r\n",
      "   0.03152624]\r\n",
      " ...\r\n",
      " [ 0.9666      0.80589998  1.37199998 ...  0.09058466  4.08142996\r\n",
      "   1.39850652]\r\n",
      " [ 0.13940001 -0.0636     -0.1112     ...  0.80191302  1.14496624\r\n",
      "   0.35648423]\r\n",
      " [ 0.66600001  0.2324      0.43920001 ...  0.6778475   0.\r\n",
      "   0.        ]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]]\r\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.6196004375815392\r\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.15426859144981092\r\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.03923289686743472\r\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.020332860975311354\r\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.0217266906858296\r\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.01812372413965372\r\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.02070343212501423\r\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.01735860825731204\r\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.020117296258340012\r\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.016857098429822005\r\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.01973106568628872\r\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.016939069574268963\r\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.01960027268206751\r\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.016823511427411668\r\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.019447448839609686\r\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.017867717891931534\r\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.019379197796051566\r\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.016769964391222365\r\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.019254095449640945\r\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.016807210774948962\r\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019208244526305714\r\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.017028565590198223\r\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.01902633364236838\r\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.016599673276337292\r\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.018842598851266747\r\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.01661954540759325\r\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.018737909811976795\r\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.01661334244104532\r\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.018530741283619725\r\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.01636036506925638\r\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.018279978552380123\r\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.016243081396588914\r\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.017928117709989484\r\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.016313706811230917\r\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.017592348259043048\r\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.01614030135365633\r\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.017075568525674375\r\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.016123527446045324\r\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.01640548959777162\r\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.016067128628492355\r\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.015606466693350592\r\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.016251538378688004\r\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.014701094855931966\r\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.016244598735983554\r\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.013802380275887411\r\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.016351423369577296\r\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.012989817589924142\r\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.016460110433399677\r\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.012522068894090684\r\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.016495476763408918\r\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.012224368474169358\r\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.016508202355068464\r\n",
      "[[ 1.06200004  0.55769998 -0.24789999 ...  0.          0.11024351\r\n",
      "   1.43054771]\r\n",
      " [ 0.0743      0.40869999  0.29910001 ...  2.17143393  0.20418364\r\n",
      "   0.22614196]\r\n",
      " [ 0.62800002  0.58170003  1.55400002 ...  0.84307814  0.78819847\r\n",
      "   2.02907324]\r\n",
      " ...\r\n",
      " [-1.32599998  0.34779999 -0.3743     ...  1.26070809  0.87310863\r\n",
      "   0.9898954 ]\r\n",
      " [ 0.66600001  0.2324      0.43920001 ...  0.6778475   0.\r\n",
      "   0.        ]\r\n",
      " [-0.85979998  1.02400005 -0.13609999 ...  2.13170838  0.\r\n",
      "   0.20217147]] [[-0.61110002  0.29409999 -0.99010003 ...  0.42720345  2.48876286\r\n",
      "   0.55028242]\r\n",
      " [ 0.27110001  0.5133     -0.1327     ...  0.          0.\r\n",
      "   1.52183771]\r\n",
      " [ 0.0667     -0.64719999 -0.244      ...  0.62430948  0.4202854\r\n",
      "   1.12050414]\r\n",
      " ...\r\n",
      " [ 0.0894     -0.0382     -0.1666     ...  0.          0.74970853\r\n",
      "   1.49316585]\r\n",
      " [-0.75150001 -0.45590001  0.45539999 ...  0.55572259  0.97654486\r\n",
      "   1.14840555]\r\n",
      " [ 0.41229999 -0.1551      1.80999994 ...  0.61420399  1.36468887\r\n",
      "   0.        ]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]]\r\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.6211809383050816\r\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.16258541208047134\r\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.03856726730796131\r\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.01983871325277365\r\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021580408402794116\r\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.018465923217053596\r\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.020869035047252436\r\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.01725309120061306\r\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.020077806972974056\r\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.017366901326638002\r\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020262106573460874\r\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.016759340794613727\r\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.019559979287756456\r\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.01678577710229617\r\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.019498590833029232\r\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.016723584670286912\r\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.01934348580402297\r\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.017142822034657\r\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.019289605671892297\r\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.016655878736995734\r\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.0192065293825156\r\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.016492526046931744\r\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.01905250728029657\r\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.016520958680372972\r\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.018975092953926808\r\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.01650186886007969\r\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.018845549885284255\r\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.01636914464716728\r\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01858381662719153\r\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.016517167624372702\r\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.018360046695011692\r\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.016227968466969635\r\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.018049965788786475\r\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.016334504462205447\r\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.017702398997907702\r\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01623558346182108\r\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.01720717775861959\r\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.016124047697163545\r\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.016593609709997435\r\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.016173909418284893\r\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.015903023524662933\r\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.01619564089924097\r\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.015059726048462294\r\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.016293095138210516\r\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.014187657677039906\r\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.016311820668096725\r\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.013399703654687147\r\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.016380831599235535\r\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.012871719267521356\r\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.016412197755506404\r\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.012652887987929422\r\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.016402115687154807\r\n",
      "[[ 1.06200004  0.55769998 -0.24789999 ...  0.          0.11024351\r\n",
      "   1.43054771]\r\n",
      " [ 0.0743      0.40869999  0.29910001 ...  2.17143393  0.20418364\r\n",
      "   0.22614196]\r\n",
      " [ 0.62800002  0.58170003  1.55400002 ...  0.84307814  0.78819847\r\n",
      "   2.02907324]\r\n",
      " ...\r\n",
      " [-1.32599998  0.34779999 -0.3743     ...  1.26070809  0.87310863\r\n",
      "   0.9898954 ]\r\n",
      " [ 0.66600001  0.2324      0.43920001 ...  0.6778475   0.\r\n",
      "   0.        ]\r\n",
      " [-0.85979998  1.02400005 -0.13609999 ...  2.13170838  0.\r\n",
      "   0.20217147]] [[-0.47639999 -0.55129999  1.85599995 ...  1.50352371  0.41429764\r\n",
      "   1.33805919]\r\n",
      " [-0.0185      0.3547     -0.3312     ...  1.56190383  0.\r\n",
      "   0.        ]\r\n",
      " [ 0.1088     -0.0945     -0.0345     ...  0.          0.81683999\r\n",
      "   0.46964425]\r\n",
      " ...\r\n",
      " [ 0.4188      0.6609     -0.1328     ...  0.66509044  1.04221439\r\n",
      "   1.17452765]\r\n",
      " [-0.433      -1.35899997 -0.37009999 ...  0.85903096  1.06330907\r\n",
      "   0.68487942]\r\n",
      " [-0.115      -0.80369997  0.0988     ...  0.12753853  0.2392813\r\n",
      "   1.40382469]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]]\r\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.6206778404680459\r\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.17181567962353045\r\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.038702063916905505\r\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.02016843368227665\r\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021691521453494962\r\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.018119715584012177\r\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.02066454283792425\r\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.01753317306821163\r\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.019925265044376656\r\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.017540761556189794\r\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.01968027963428884\r\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.017145823042553205\r\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.019545258123528312\r\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.01698912687313098\r\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.019373168388532626\r\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.0171021196561364\r\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.01928773751432026\r\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.01704487347832093\r\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.019174759049673338\r\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.017188691462461766\r\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019128726241556374\r\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.0170203885779931\r\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.01896567934670964\r\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.01693161054012867\r\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01895108099120694\r\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.016812689387454435\r\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.018706544604454492\r\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.016690195251542788\r\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.018548745385094267\r\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.016729242383287504\r\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.018290664119696296\r\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.016551517953093235\r\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.017913096206816467\r\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.016458856443373058\r\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.017608570815944993\r\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.01649877751389375\r\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.017067582208059123\r\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.016483523309803925\r\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.01653881772496813\r\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016434200251331695\r\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.015779943507466768\r\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016461398667440966\r\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.014911161735653877\r\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.01655308357798136\r\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.014064681779190496\r\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.016649047342630532\r\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.013292504562618764\r\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.01667151915339323\r\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.012757533796232295\r\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.01670774043752597\r\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.012546878899573474\r\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.016746381512628153\r\n",
      "[[ 1.06200004  0.55769998 -0.24789999 ...  0.          0.11024351\r\n",
      "   1.43054771]\r\n",
      " [ 0.0743      0.40869999  0.29910001 ...  2.17143393  0.20418364\r\n",
      "   0.22614196]\r\n",
      " [ 0.62800002  0.58170003  1.55400002 ...  0.84307814  0.78819847\r\n",
      "   2.02907324]\r\n",
      " ...\r\n",
      " [-1.32599998  0.34779999 -0.3743     ...  1.26070809  0.87310863\r\n",
      "   0.9898954 ]\r\n",
      " [ 0.66600001  0.2324      0.43920001 ...  0.6778475   0.\r\n",
      "   0.        ]\r\n",
      " [-0.85979998  1.02400005 -0.13609999 ...  2.13170838  0.\r\n",
      "   0.20217147]] [[-0.32539999 -0.40090001  0.97000003 ...  0.34187368  1.2574017\r\n",
      "   1.60284221]\r\n",
      " [-0.2043      0.1946     -1.60500002 ...  0.1424491   0.65520793\r\n",
      "   0.79103673]\r\n",
      " [-0.0901      0.1814      0.58459997 ...  0.31677482  0.75225246\r\n",
      "   1.40817988]\r\n",
      " ...\r\n",
      " [-0.4901      0.1225     -0.90329999 ...  1.48333132  1.86881649\r\n",
      "   0.73933762]\r\n",
      " [-1.18099999 -0.52060002  0.0939     ...  0.          0.\r\n",
      "   1.06381273]\r\n",
      " [-0.59460002  0.1265     -2.13000011 ...  1.05585361  1.72075593\r\n",
      "   1.64087975]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]]\r\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.6201876266985327\r\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.15992088959767267\r\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.03876734487208966\r\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.020311935016742118\r\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021734079421573394\r\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.01854926347732544\r\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.020597360826827383\r\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.017558470081824522\r\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.02033948470410463\r\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.0173600075336603\r\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.019722660172831367\r\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.017309230823929492\r\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.019540146270112413\r\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.01751878270162986\r\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.01941602130898753\r\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.0169399707363202\r\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.019296562928404357\r\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.016905033101256076\r\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019223406986408943\r\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.016838372613375004\r\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019139040513215837\r\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.01717285940853449\r\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019021347139936848\r\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.01687213148062046\r\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.018871440904567372\r\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.0170330790659556\r\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.018690130698519783\r\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.016683671050346814\r\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.018496814385257864\r\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.016656189010693476\r\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.01833892268808307\r\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.016567152924835682\r\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.01801231407837288\r\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.01674475012203822\r\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.017567479806775983\r\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.01664264020151817\r\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.017120680306106806\r\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.016657247924460813\r\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.016495684816225153\r\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.01649739471479104\r\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.015715333545026747\r\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01659193677970996\r\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.014842250308877713\r\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01666847071968592\r\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.013954740206434115\r\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.01674410428565282\r\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.013221000799456158\r\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.01681078511935014\r\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.012646833875191372\r\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016809492873457763\r\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.012471951418430419\r\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.016790376832852\r\n",
      "[[ 0.0743      0.40869999  0.29910001 ...  2.17143393  0.20418364\r\n",
      "   0.22614196]\r\n",
      " [ 0.62800002  0.58170003  1.55400002 ...  0.84307814  0.78819847\r\n",
      "   2.02907324]\r\n",
      " [-0.51380002 -0.2491     -0.2656     ...  0.03863505  0.\r\n",
      "   0.        ]\r\n",
      " ...\r\n",
      " [ 0.13940001 -0.0636     -0.1112     ...  0.80191302  1.14496624\r\n",
      "   0.35648423]\r\n",
      " [ 0.66600001  0.2324      0.43920001 ...  0.6778475   0.\r\n",
      "   0.        ]\r\n",
      " [-0.85979998  1.02400005 -0.13609999 ...  2.13170838  0.\r\n",
      "   0.20217147]] [[ 1.06200004  0.55769998 -0.24789999 ...  0.          0.11024351\r\n",
      "   1.43054771]\r\n",
      " [-0.063       0.25639999 -0.52789998 ...  0.71296787  1.17863476\r\n",
      "   1.52827871]\r\n",
      " [-0.28749999  0.0322     -0.88630003 ...  0.791798    1.50880456\r\n",
      "   0.65090716]\r\n",
      " ...\r\n",
      " [ 0.142      -0.3696     -0.0093     ...  1.02717209  0.7639752\r\n",
      "   0.30989727]\r\n",
      " [ 0.1608     -1.04999995  0.25510001 ...  0.43850875  0.0486182\r\n",
      "   0.6049757 ]\r\n",
      " [-1.32599998  0.34779999 -0.3743     ...  1.26070809  0.87310863\r\n",
      "   0.9898954 ]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]]\r\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.6219292786475774\r\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.15001975343777582\r\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.03880424752227358\r\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.020067473157094076\r\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021677260211593396\r\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.018345517607835624\r\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.020581660288813954\r\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.017746462701604918\r\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02002695942851337\r\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.018204367074828882\r\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.01972452606502417\r\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.017492754241594903\r\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.019555691856186132\r\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.017063220246480063\r\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.01945658654880685\r\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.01709446683526039\r\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.019363316883509223\r\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.016873457469046116\r\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.0192294398423385\r\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.016978485223192435\r\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019058872089796775\r\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.016873223850360282\r\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.018988488292371906\r\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.016771037274828322\r\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.018898241473613558\r\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.016849668983083505\r\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.018727052805794252\r\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.016745833680033684\r\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.018551242915359704\r\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.016664100882525627\r\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.018238209213155346\r\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.016614159425863854\r\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.018036584319496476\r\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.01651716325432062\r\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.017590087058173644\r\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.01642478543978471\r\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.01707745498247646\r\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.016514988186267707\r\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.01651507375661183\r\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.016470076229709845\r\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.015834438048202445\r\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.016430659147982415\r\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.014973527474983319\r\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.016596032593112726\r\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.014107635081116413\r\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.016623281873762608\r\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.01335809426382184\r\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.0167038649893724\r\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.012851007786151525\r\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.016728999021534737\r\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.01267370202805142\r\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.016736783517094758\r\n",
      "[[ 1.06200004  0.55769998 -0.24789999 ...  0.          0.11024351\r\n",
      "   1.43054771]\r\n",
      " [ 0.0743      0.40869999  0.29910001 ...  2.17143393  0.20418364\r\n",
      "   0.22614196]\r\n",
      " [-0.51380002 -0.2491     -0.2656     ...  0.03863505  0.\r\n",
      "   0.        ]\r\n",
      " ...\r\n",
      " [ 0.13940001 -0.0636     -0.1112     ...  0.80191302  1.14496624\r\n",
      "   0.35648423]\r\n",
      " [-1.32599998  0.34779999 -0.3743     ...  1.26070809  0.87310863\r\n",
      "   0.9898954 ]\r\n",
      " [ 0.66600001  0.2324      0.43920001 ...  0.6778475   0.\r\n",
      "   0.        ]] [[ 0.62800002  0.58170003  1.55400002 ...  0.84307814  0.78819847\r\n",
      "   2.02907324]\r\n",
      " [ 2.04399991  1.70000005 -1.53900003 ...  0.          0.50698102\r\n",
      "   8.1433115 ]\r\n",
      " [-0.68839997 -0.42030001 -1.26400006 ...  2.46434116  0.61453593\r\n",
      "   0.50339454]\r\n",
      " ...\r\n",
      " [-0.30579999 -0.32030001 -0.90090001 ...  0.70123821  0.96587187\r\n",
      "   1.30574703]\r\n",
      " [-0.39359999 -1.10599995  0.83350003 ...  0.33356953  0.\r\n",
      "   0.18347034]\r\n",
      " [-0.85979998  1.02400005 -0.13609999 ...  2.13170838  0.\r\n",
      "   0.20217147]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " ...\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]\r\n",
      " [0. 0. 0. ... 0. 0. 0.]]\r\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.619898488392701\r\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.15383275426351106\r\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.03863210658970717\r\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.019914888132076997\r\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.021685967121172597\r\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.017885876461290397\r\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.0206920087740228\r\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.017264158106767215\r\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.019970700950235933\r\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.016892629389006358\r\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.019663053136822338\r\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.017922764715666954\r\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.019688018950054776\r\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.01681003411515401\r\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.01934475831788134\r\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.016651115093666773\r\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.01930623033360855\r\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.016808713428103007\r\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.01923818767976922\r\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.01661658244064221\r\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.019034620726833474\r\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.016593012027442455\r\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.018902387797228387\r\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.016686576490218822\r\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.01881491777965346\r\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.016475555773537893\r\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.018624617340596946\r\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.016530300992039535\r\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.018449516312496084\r\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.016377475184316818\r\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.018150291732839635\r\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.01631289032789377\r\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.01780053643459404\r\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.016263964084478524\r\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.01741484202746604\r\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.01634545509631817\r\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.01684735856346182\r\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.016329419440948047\r\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.016210947361950938\r\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.01632004102262167\r\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.015407055175888378\r\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.016340439566052876\r\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.014458267106297048\r\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.016372394318190906\r\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.013502092641853803\r\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.016424676976524867\r\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.012702750230862482\r\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.016483084227030095\r\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.012167749750251705\r\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.01651748878738055\r\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.011946699707894711\r\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.01651248479118714\r\n",
      "CV log_loss:  0.01641194737817573\r\n",
      "Memory usage of properties dataframe is : 159.15740966796875  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  79.71503067016602  MB\r\n",
      "This is  50.08565472161556 % of the initial size\r\n",
      "Memory usage of properties dataframe is : 37.60920715332031  MB\r\n",
      "___MEMORY USAGE AFTER COMPLETION:___\r\n",
      "Memory usage is:  4.860233306884766  MB\r\n",
      "This is  12.922987945667666 % of the initial size\r\n",
      "CV log_loss:  0.015125952005383494\r\n",
      "['train_features.csv', 'train_drug.csv', 'test_features.csv', 'train_targets_nonscored.csv', 'sample_submission.csv', 'train_targets_scored.csv']\r\n",
      "train_features shape is:  (23814, 1526)\r\n",
      "train_features shape is:  (23814, 1040)\r\n",
      "folds:               sig_id cp_time  ... wnt_inhibitor  kfold\r\n",
      "0      id_000644bb2      24  ...             0      5\r\n",
      "1      id_000779bfc      72  ...             0      0\r\n",
      "2      id_000a6266a      48  ...             0      6\r\n",
      "3      id_0015fd391      48  ...             0      0\r\n",
      "4      id_001626bd3      72  ...             0      4\r\n",
      "...             ...     ...  ...           ...    ...\r\n",
      "21943  id_fff8c2444      72  ...             0      5\r\n",
      "21944  id_fffb1ceed      24  ...             0      1\r\n",
      "21945  id_fffb70c0c      24  ...             0      5\r\n",
      "21946  id_fffcb9e7c      24  ...             0      1\r\n",
      "21947  id_ffffdd77b      72  ...             0      6\r\n",
      "\r\n",
      "[21948 rows x 1301 columns]\r\n",
      "train.shape: (21948, 1300)\r\n",
      "folds.shape: (21948, 1301)\r\n",
      "test.shape: (3624, 1094)\r\n",
      "target.shape: (21948, 207)\r\n",
      "sub.shape: (3982, 207)\r\n",
      "feat_cols len: 1096\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 0, train_loss: 0.48439385269318713, valid_loss: 0.027692762315273286\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 1, train_loss: 0.024153734563666138, valid_loss: 0.01914746396243572\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 2, train_loss: 0.021704880089885522, valid_loss: 0.017864801436662674\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 3, train_loss: 0.020606090789850876, valid_loss: 0.017640861235558986\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 4, train_loss: 0.020341121940081624, valid_loss: 0.017494667805731296\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 5, train_loss: 0.020272587812473986, valid_loss: 0.01746034439653158\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 6, train_loss: 0.020249916975494144, valid_loss: 0.017224423475563525\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 7, train_loss: 0.020293593723453632, valid_loss: 0.017722154296934606\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 8, train_loss: 0.020340671496731893, valid_loss: 0.01737962905317545\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 9, train_loss: 0.02025267593309182, valid_loss: 0.01724664952605963\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 10, train_loss: 0.020207358106058473, valid_loss: 0.017285155914723873\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 11, train_loss: 0.020175853199293825, valid_loss: 0.017597241662442684\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 12, train_loss: 0.020111090408600107, valid_loss: 0.0171701617911458\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 13, train_loss: 0.01999490349521848, valid_loss: 0.01696703426539898\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 14, train_loss: 0.019888472445562584, valid_loss: 0.01678540911525488\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 15, train_loss: 0.019740851051142427, valid_loss: 0.016866143345832824\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 16, train_loss: 0.01948803027166801, valid_loss: 0.016736831963062287\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 17, train_loss: 0.01926792415530503, valid_loss: 0.016634118221700193\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 18, train_loss: 0.018978934665368933, valid_loss: 0.016588881127536297\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 19, train_loss: 0.018575496901916403, valid_loss: 0.016421653628349304\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 20, train_loss: 0.018143531384871524, valid_loss: 0.016209864169359208\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 21, train_loss: 0.017685157322914015, valid_loss: 0.016150109507143497\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 22, train_loss: 0.01721766808380683, valid_loss: 0.016158634573221208\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 23, train_loss: 0.016838518174073728, valid_loss: 0.016174840815365313\r\n",
      "SEED: 0, FOLD: 0, EPOCH: 24, train_loss: 0.016650270018093036, valid_loss: 0.01611721348017454\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 0, train_loss: 0.48502639852160095, valid_loss: 0.02672994151711464\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 1, train_loss: 0.02418078700093185, valid_loss: 0.01887463666498661\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 2, train_loss: 0.021855964052940713, valid_loss: 0.017762180492281912\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 3, train_loss: 0.02067517770492301, valid_loss: 0.017237520553171636\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 4, train_loss: 0.02043695199195625, valid_loss: 0.01736771784722805\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 5, train_loss: 0.020318268639903492, valid_loss: 0.016946501545608044\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 6, train_loss: 0.020301392783417183, valid_loss: 0.017535175420343874\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 7, train_loss: 0.02029018319484328, valid_loss: 0.017193502336740493\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 8, train_loss: 0.020261371875701307, valid_loss: 0.01707215003669262\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 9, train_loss: 0.020276039560027673, valid_loss: 0.016988608688116073\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 10, train_loss: 0.020212038350449938, valid_loss: 0.017078704684972763\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 11, train_loss: 0.0202104266935668, valid_loss: 0.017590822391211985\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 12, train_loss: 0.02004209637236433, valid_loss: 0.016931544840335846\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 13, train_loss: 0.020001314808519518, valid_loss: 0.01704011969268322\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 14, train_loss: 0.01991648598238319, valid_loss: 0.016619834452867507\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 15, train_loss: 0.019729209493617623, valid_loss: 0.01662383835762739\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 16, train_loss: 0.01955346029479893, valid_loss: 0.016510026082396507\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 17, train_loss: 0.019244972502394597, valid_loss: 0.016332753486931324\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 18, train_loss: 0.01887752959618763, valid_loss: 0.016306565925478935\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 19, train_loss: 0.018514229421864967, valid_loss: 0.016123681552708147\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 20, train_loss: 0.01808381789572993, valid_loss: 0.01597429718822241\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 21, train_loss: 0.01761494220240789, valid_loss: 0.015906374640762805\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 22, train_loss: 0.017079664755384534, valid_loss: 0.01587900310754776\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 23, train_loss: 0.016681641302242572, valid_loss: 0.015878973565995692\r\n",
      "SEED: 0, FOLD: 1, EPOCH: 24, train_loss: 0.016487923666175935, valid_loss: 0.015888015031814574\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 0, train_loss: 0.4855840570692505, valid_loss: 0.026345438212156295\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 1, train_loss: 0.024056698100603357, valid_loss: 0.018863596245646475\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 2, train_loss: 0.021723435139980447, valid_loss: 0.017888567224144936\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 3, train_loss: 0.020663163656381524, valid_loss: 0.017566736824810504\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 4, train_loss: 0.020333551934787204, valid_loss: 0.01734920147806406\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 5, train_loss: 0.020249727011031032, valid_loss: 0.017353901863098143\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 6, train_loss: 0.02023301101258012, valid_loss: 0.017357056364417076\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 7, train_loss: 0.020330921170257386, valid_loss: 0.017483534999191763\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 8, train_loss: 0.020310041678397835, valid_loss: 0.017256600968539714\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 9, train_loss: 0.020265065716440173, valid_loss: 0.0171611762791872\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 10, train_loss: 0.020173186398282344, valid_loss: 0.017237178459763528\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 11, train_loss: 0.020167791214929957, valid_loss: 0.017215465493500233\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 12, train_loss: 0.020090034497635707, valid_loss: 0.017227638885378836\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 13, train_loss: 0.020050488754498715, valid_loss: 0.01700333219021559\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 14, train_loss: 0.019877842101616923, valid_loss: 0.01698274079710245\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 15, train_loss: 0.019680154452822646, valid_loss: 0.01672175873070955\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 16, train_loss: 0.019462572220636874, valid_loss: 0.016686548069119452\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 17, train_loss: 0.019227312117510914, valid_loss: 0.01651946648955345\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 18, train_loss: 0.018948051368906385, valid_loss: 0.016360707730054855\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 19, train_loss: 0.018581037409603596, valid_loss: 0.016317959874868393\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 20, train_loss: 0.018113383662183673, valid_loss: 0.016206867620348932\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 21, train_loss: 0.01762326657898775, valid_loss: 0.016183438263833522\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 22, train_loss: 0.017157684302046186, valid_loss: 0.016174579225480556\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 23, train_loss: 0.016793085864054508, valid_loss: 0.01615461066365242\r\n",
      "SEED: 0, FOLD: 2, EPOCH: 24, train_loss: 0.016618195021537698, valid_loss: 0.01615709710866213\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 0, train_loss: 0.4841341972097653, valid_loss: 0.027193042263388634\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 1, train_loss: 0.023997892898057593, valid_loss: 0.019062491357326506\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 2, train_loss: 0.02191142059963982, valid_loss: 0.01814196653664112\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 3, train_loss: 0.0206472782456145, valid_loss: 0.017796054258942606\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 4, train_loss: 0.02034967748739687, valid_loss: 0.01748611081391573\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 5, train_loss: 0.020337461006073726, valid_loss: 0.017773968204855918\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 6, train_loss: 0.020233657451815344, valid_loss: 0.017323281392455103\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 7, train_loss: 0.020305902716152523, valid_loss: 0.01747153915464878\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 8, train_loss: 0.020290956268606542, valid_loss: 0.017457306459546088\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 9, train_loss: 0.020282551506850995, valid_loss: 0.017214800119400023\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 10, train_loss: 0.020193128865592335, valid_loss: 0.01736537501215935\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 11, train_loss: 0.020138045709554842, valid_loss: 0.0173273316770792\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 12, train_loss: 0.020138670937443266, valid_loss: 0.017495925799012185\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 13, train_loss: 0.01994488071523556, valid_loss: 0.017372185997664928\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 14, train_loss: 0.019860329511923853, valid_loss: 0.016929992362856864\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 15, train_loss: 0.01957519334696588, valid_loss: 0.01696139793843031\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 16, train_loss: 0.019482436545547984, valid_loss: 0.016697258800268174\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 17, train_loss: 0.01923184299550089, valid_loss: 0.0166103046387434\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 18, train_loss: 0.018912330421866203, valid_loss: 0.016459067650139332\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 19, train_loss: 0.018538189785821096, valid_loss: 0.01640067145228386\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 20, train_loss: 0.018116295831848164, valid_loss: 0.01625759568065405\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 21, train_loss: 0.01761411605592893, valid_loss: 0.016254205219447614\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 22, train_loss: 0.017123249448126272, valid_loss: 0.01614822808653116\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 23, train_loss: 0.01676443390244124, valid_loss: 0.01616218637675047\r\n",
      "SEED: 0, FOLD: 3, EPOCH: 24, train_loss: 0.016576705240726877, valid_loss: 0.016159484125673772\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 0, train_loss: 0.48545404609774245, valid_loss: 0.027050400897860528\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 1, train_loss: 0.024070356712758947, valid_loss: 0.019088534340262413\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 2, train_loss: 0.021699534931860002, valid_loss: 0.01856907643377781\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 3, train_loss: 0.02070166568468217, valid_loss: 0.017882473990321158\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 4, train_loss: 0.0203085897908527, valid_loss: 0.017674962878227233\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 5, train_loss: 0.020364084084625957, valid_loss: 0.01770214185118675\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 6, train_loss: 0.020219808989236143, valid_loss: 0.017277632988989353\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 7, train_loss: 0.02030671171235795, valid_loss: 0.017739736214280127\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 8, train_loss: 0.020259917910103085, valid_loss: 0.017477201223373415\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 9, train_loss: 0.02023588751955908, valid_loss: 0.017502639889717102\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 10, train_loss: 0.020237994716078244, valid_loss: 0.017159569598734378\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 11, train_loss: 0.020155714368637726, valid_loss: 0.01716395139694214\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 12, train_loss: 0.020058419276662423, valid_loss: 0.01733283795416355\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 13, train_loss: 0.019961049172038933, valid_loss: 0.01713685419410467\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 14, train_loss: 0.019814730362117696, valid_loss: 0.017083205431699753\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 15, train_loss: 0.019685014613530262, valid_loss: 0.016881048306822778\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 16, train_loss: 0.019500818572380917, valid_loss: 0.016739426739513875\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 17, train_loss: 0.019205797793102913, valid_loss: 0.01678396712988615\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 18, train_loss: 0.01895839656975602, valid_loss: 0.01662432584911585\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 19, train_loss: 0.01857337848517765, valid_loss: 0.016444535106420518\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 20, train_loss: 0.018132619623120138, valid_loss: 0.016287244744598865\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 21, train_loss: 0.01768792280610524, valid_loss: 0.016249838471412658\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 22, train_loss: 0.01722114132780607, valid_loss: 0.01620434209704399\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 23, train_loss: 0.016869776432408768, valid_loss: 0.016215890385210513\r\n",
      "SEED: 0, FOLD: 4, EPOCH: 24, train_loss: 0.01668455314245962, valid_loss: 0.01619426507502794\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 0, train_loss: 0.4851766723860689, valid_loss: 0.027412449419498445\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 1, train_loss: 0.02431349320729979, valid_loss: 0.019274949356913568\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 2, train_loss: 0.02176771383909952, valid_loss: 0.017936788722872733\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 3, train_loss: 0.0208094699726421, valid_loss: 0.018212988674640655\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 4, train_loss: 0.020389272863058007, valid_loss: 0.017489356994628905\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 5, train_loss: 0.02033383344762585, valid_loss: 0.01828404054045677\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 6, train_loss: 0.020293364402692336, valid_loss: 0.017606412507593632\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 7, train_loss: 0.020289870865997813, valid_loss: 0.017510823495686053\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 8, train_loss: 0.02026810497045517, valid_loss: 0.017560213096439837\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 9, train_loss: 0.020300407089343688, valid_loss: 0.0173265715688467\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 10, train_loss: 0.020278938646827425, valid_loss: 0.017382182255387305\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 11, train_loss: 0.02017401508530792, valid_loss: 0.017288817055523396\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 12, train_loss: 0.020124311577908848, valid_loss: 0.01731479462236166\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 13, train_loss: 0.01996216854574729, valid_loss: 0.0171173357963562\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 14, train_loss: 0.01985858159051055, valid_loss: 0.01730778902769089\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 15, train_loss: 0.019680291857747806, valid_loss: 0.016859773844480515\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 16, train_loss: 0.019531894196458413, valid_loss: 0.01684754129499197\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 17, train_loss: 0.01919482629366067, valid_loss: 0.01680252864956856\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 18, train_loss: 0.018880143837661158, valid_loss: 0.016522614397108556\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 19, train_loss: 0.018553518949925494, valid_loss: 0.016377651877701283\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 20, train_loss: 0.018132833510535917, valid_loss: 0.016262155845761298\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 21, train_loss: 0.01763641091101632, valid_loss: 0.016261111721396448\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 22, train_loss: 0.017129731168146846, valid_loss: 0.016176299042999744\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 23, train_loss: 0.016749014431724742, valid_loss: 0.01620850548148155\r\n",
      "SEED: 0, FOLD: 5, EPOCH: 24, train_loss: 0.016588219859618312, valid_loss: 0.016200294084846974\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 0, train_loss: 0.4834665210322052, valid_loss: 0.026766022220253944\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 1, train_loss: 0.024089165830186436, valid_loss: 0.01926431346684694\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 2, train_loss: 0.02171090790102271, valid_loss: 0.017659941650927068\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 3, train_loss: 0.020826888657143328, valid_loss: 0.01873917654156685\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 4, train_loss: 0.020392300028886114, valid_loss: 0.017431881725788117\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 5, train_loss: 0.020278012227951264, valid_loss: 0.01750574938952923\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 6, train_loss: 0.020290208955098983, valid_loss: 0.01719635143876076\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 7, train_loss: 0.020329848831506812, valid_loss: 0.017233540788292884\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 8, train_loss: 0.020259693505711295, valid_loss: 0.017262623608112336\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 9, train_loss: 0.020233632895309907, valid_loss: 0.017120999433100224\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 10, train_loss: 0.020251224755125793, valid_loss: 0.01713705588132143\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 11, train_loss: 0.020192445857691116, valid_loss: 0.01712419845163822\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 12, train_loss: 0.020136688753557043, valid_loss: 0.017009647376835347\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 13, train_loss: 0.02004792393014139, valid_loss: 0.01704406052827835\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 14, train_loss: 0.019831289315507525, valid_loss: 0.016872292086482048\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 15, train_loss: 0.01970596368215522, valid_loss: 0.016767171286046503\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 16, train_loss: 0.019452196273471223, valid_loss: 0.01668622963130474\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 17, train_loss: 0.019273708086638225, valid_loss: 0.01653490860015154\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 18, train_loss: 0.0189106546367715, valid_loss: 0.016440867744386196\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 19, train_loss: 0.018515700449039336, valid_loss: 0.01621762551367283\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 20, train_loss: 0.018103649250554795, valid_loss: 0.016198220029473305\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 21, train_loss: 0.017611470757698526, valid_loss: 0.016114812456071378\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 22, train_loss: 0.017147710230074772, valid_loss: 0.016019972525537014\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 23, train_loss: 0.016793519790683473, valid_loss: 0.016011975631117822\r\n",
      "SEED: 0, FOLD: 6, EPOCH: 24, train_loss: 0.01660037690735593, valid_loss: 0.01601823017001152\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 0, train_loss: 0.48442767469250425, valid_loss: 0.027062615156173708\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 1, train_loss: 0.024125985484443554, valid_loss: 0.0193607447296381\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 2, train_loss: 0.021637710875698497, valid_loss: 0.019759035035967827\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 3, train_loss: 0.02074271539340214, valid_loss: 0.017582753747701643\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 4, train_loss: 0.020548641250855256, valid_loss: 0.017447238974273204\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 5, train_loss: 0.02025383193882144, valid_loss: 0.01750994276255369\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 6, train_loss: 0.02026511832666235, valid_loss: 0.01741391945630312\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 7, train_loss: 0.020287333829265064, valid_loss: 0.017387847155332564\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 8, train_loss: 0.0202849275071402, valid_loss: 0.01752403475344181\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 9, train_loss: 0.020259866820407563, valid_loss: 0.017433414943516256\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 10, train_loss: 0.020169887653723056, valid_loss: 0.01721979197114706\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 11, train_loss: 0.020107282477678085, valid_loss: 0.017372098341584204\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 12, train_loss: 0.020051025411709635, valid_loss: 0.01698310948908329\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 13, train_loss: 0.01995067797651907, valid_loss: 0.016924999356269836\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 14, train_loss: 0.019797719026706656, valid_loss: 0.017002818286418916\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 15, train_loss: 0.019725389537863992, valid_loss: 0.016751246079802513\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 16, train_loss: 0.019464743456670215, valid_loss: 0.016841644309461116\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 17, train_loss: 0.019223596875359412, valid_loss: 0.01659993201494217\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 18, train_loss: 0.018887278769596092, valid_loss: 0.016404819190502167\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 19, train_loss: 0.018517386535702108, valid_loss: 0.01628682926297188\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 20, train_loss: 0.01812056673779374, valid_loss: 0.016210663728415967\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 21, train_loss: 0.017568714783660004, valid_loss: 0.01613231223076582\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 22, train_loss: 0.017100779935210742, valid_loss: 0.016060772985219955\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 23, train_loss: 0.016713100479168144, valid_loss: 0.016066685616970063\r\n",
      "SEED: 1, FOLD: 0, EPOCH: 24, train_loss: 0.016536360587544586, valid_loss: 0.016090240851044654\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 0, train_loss: 0.4852802669312678, valid_loss: 0.026844861656427382\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 1, train_loss: 0.024116454950096657, valid_loss: 0.01918211579322815\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 2, train_loss: 0.021946517788634008, valid_loss: 0.017694689743220807\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 3, train_loss: 0.02066386996635369, valid_loss: 0.01742039542645216\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 4, train_loss: 0.020587040567580536, valid_loss: 0.017409013397991658\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 5, train_loss: 0.020335361158766715, valid_loss: 0.018002560064196586\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 6, train_loss: 0.020350281377227938, valid_loss: 0.01729896530508995\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 7, train_loss: 0.020299395128172273, valid_loss: 0.0170993697270751\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 8, train_loss: 0.020357290092779666, valid_loss: 0.01704514991492033\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 9, train_loss: 0.020306353780383965, valid_loss: 0.01735401798039675\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 10, train_loss: 0.020310319313893512, valid_loss: 0.017115004062652588\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 11, train_loss: 0.020222541603709565, valid_loss: 0.016985507383942604\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 12, train_loss: 0.020142762459257023, valid_loss: 0.017194291241467\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 13, train_loss: 0.020000988592197295, valid_loss: 0.016798263974487783\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 14, train_loss: 0.019896792805417864, valid_loss: 0.01690198842436075\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 15, train_loss: 0.019706804191275518, valid_loss: 0.01655479971319437\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 16, train_loss: 0.019527149418381608, valid_loss: 0.01641820926219225\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 17, train_loss: 0.019286996450553946, valid_loss: 0.016387987695634366\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 18, train_loss: 0.018905429866443686, valid_loss: 0.01611665725708008\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 19, train_loss: 0.01852014916790586, valid_loss: 0.01605746116489172\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 20, train_loss: 0.018161259705618937, valid_loss: 0.0159686291962862\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 21, train_loss: 0.017669700656314284, valid_loss: 0.01585057847201824\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 22, train_loss: 0.017193015493756653, valid_loss: 0.015795681178569793\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 23, train_loss: 0.01682401421879019, valid_loss: 0.0158121108263731\r\n",
      "SEED: 1, FOLD: 1, EPOCH: 24, train_loss: 0.016631404729876793, valid_loss: 0.015823153033852577\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 0, train_loss: 0.485451523746763, valid_loss: 0.026805334463715552\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 1, train_loss: 0.024114066937646898, valid_loss: 0.019468466490507125\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 2, train_loss: 0.021891032643362777, valid_loss: 0.01778689566999674\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 3, train_loss: 0.020633977337353896, valid_loss: 0.017691769711673258\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 4, train_loss: 0.02048966534385065, valid_loss: 0.017342788837850093\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 5, train_loss: 0.020313464320537183, valid_loss: 0.01727276887744665\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 6, train_loss: 0.020308829539892624, valid_loss: 0.017252466827630996\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 7, train_loss: 0.020358371808111262, valid_loss: 0.017512109763920307\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 8, train_loss: 0.020302129871383006, valid_loss: 0.017386861220002173\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 9, train_loss: 0.020269579475834257, valid_loss: 0.01752243161201477\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 10, train_loss: 0.02020153679511174, valid_loss: 0.017275000624358654\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 11, train_loss: 0.02019314376675353, valid_loss: 0.017365588657557964\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 12, train_loss: 0.020107451091412785, valid_loss: 0.017088861428201197\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 13, train_loss: 0.020011999916766776, valid_loss: 0.01716474466025829\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 14, train_loss: 0.019812938573510467, valid_loss: 0.016725969277322294\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 15, train_loss: 0.019659064570758618, valid_loss: 0.01684675857424736\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 16, train_loss: 0.019494860882864517, valid_loss: 0.016797158420085907\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 17, train_loss: 0.019253649514447264, valid_loss: 0.016742450073361396\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 18, train_loss: 0.018931758735759727, valid_loss: 0.016438922472298147\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 19, train_loss: 0.018571629645112827, valid_loss: 0.016275493651628493\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 20, train_loss: 0.018160313633935794, valid_loss: 0.016271009966731073\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 21, train_loss: 0.017625666255144036, valid_loss: 0.016204495392739774\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 22, train_loss: 0.017158352554503348, valid_loss: 0.016160100251436233\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 23, train_loss: 0.016844843526934685, valid_loss: 0.01613927897065878\r\n",
      "SEED: 1, FOLD: 2, EPOCH: 24, train_loss: 0.016601180410658826, valid_loss: 0.016162391416728497\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 0, train_loss: 0.48510406611069123, valid_loss: 0.027234017997980118\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 1, train_loss: 0.02405746131628549, valid_loss: 0.01926873855292797\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 2, train_loss: 0.021607171782121366, valid_loss: 0.017888121753931046\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 3, train_loss: 0.020688090372045023, valid_loss: 0.0179899650067091\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 4, train_loss: 0.020347443250875896, valid_loss: 0.017598584443330765\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 5, train_loss: 0.020295674703559096, valid_loss: 0.017388934567570687\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 6, train_loss: 0.020197757006603843, valid_loss: 0.017385482788085938\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 7, train_loss: 0.020246358512311567, valid_loss: 0.017468892522156238\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 8, train_loss: 0.020210579721903314, valid_loss: 0.017317192740738393\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 9, train_loss: 0.020156820995264314, valid_loss: 0.017310891337692737\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 10, train_loss: 0.02019653022035855, valid_loss: 0.017327123396098612\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 11, train_loss: 0.02013925876037604, valid_loss: 0.017852614000439644\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 12, train_loss: 0.020138840526849233, valid_loss: 0.017308330908417702\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 13, train_loss: 0.01995805951253492, valid_loss: 0.017055683843791483\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 14, train_loss: 0.01979697978466141, valid_loss: 0.01706337433308363\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 15, train_loss: 0.019705070790891746, valid_loss: 0.01699437253177166\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 16, train_loss: 0.019383752275080907, valid_loss: 0.016748568527400493\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 17, train_loss: 0.019241829028948634, valid_loss: 0.016844790801405905\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 18, train_loss: 0.018897452841506523, valid_loss: 0.016649233661592006\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 19, train_loss: 0.01848964808749504, valid_loss: 0.016466478705406188\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 20, train_loss: 0.018092959707438135, valid_loss: 0.016420050896704196\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 21, train_loss: 0.017580437740044936, valid_loss: 0.016279195733368397\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 22, train_loss: 0.01708078179128316, valid_loss: 0.01624279599636793\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 23, train_loss: 0.0167240727578803, valid_loss: 0.016239912137389182\r\n",
      "SEED: 1, FOLD: 3, EPOCH: 24, train_loss: 0.016532032604829796, valid_loss: 0.016230325438082217\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 0, train_loss: 0.485363154501027, valid_loss: 0.026643508300185203\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 1, train_loss: 0.024138068184763394, valid_loss: 0.019345303550362587\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 2, train_loss: 0.021972564321492805, valid_loss: 0.017903408259153365\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 3, train_loss: 0.02077694493187528, valid_loss: 0.017624709159135818\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 4, train_loss: 0.020368426772100583, valid_loss: 0.01736972339451313\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 5, train_loss: 0.020292277910372837, valid_loss: 0.01756545662879944\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 6, train_loss: 0.020249108296065105, valid_loss: 0.01745950251817703\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 7, train_loss: 0.020229041563714443, valid_loss: 0.01799368381500244\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 8, train_loss: 0.02022568754801134, valid_loss: 0.017538690865039827\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 9, train_loss: 0.02022218581314395, valid_loss: 0.01755059853196144\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 10, train_loss: 0.020245100162467177, valid_loss: 0.017385451570153237\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 11, train_loss: 0.020206891215678785, valid_loss: 0.017212693989276887\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 12, train_loss: 0.020093692732708796, valid_loss: 0.017068650759756564\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 13, train_loss: 0.019974370417343516, valid_loss: 0.01706123761832714\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 14, train_loss: 0.01984006929255667, valid_loss: 0.017001580111682416\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 15, train_loss: 0.01970420199997571, valid_loss: 0.01679576374590397\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 16, train_loss: 0.01945277254040144, valid_loss: 0.01678109023720026\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 17, train_loss: 0.019231704336141242, valid_loss: 0.016654732637107372\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 18, train_loss: 0.018902882604169197, valid_loss: 0.016571934148669244\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 19, train_loss: 0.018506031204648568, valid_loss: 0.016388825215399264\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 20, train_loss: 0.018116256481885504, valid_loss: 0.016283372789621352\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 21, train_loss: 0.01761745161623979, valid_loss: 0.01617932479828596\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 22, train_loss: 0.01713288591249662, valid_loss: 0.01617656905204058\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 23, train_loss: 0.016774268345717266, valid_loss: 0.016185260824859143\r\n",
      "SEED: 1, FOLD: 4, EPOCH: 24, train_loss: 0.016580774322101453, valid_loss: 0.0161723205819726\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 0, train_loss: 0.48385899244066405, valid_loss: 0.027981023862957956\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 1, train_loss: 0.024298945568552634, valid_loss: 0.019526289775967598\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 2, train_loss: 0.021772013495669883, valid_loss: 0.018035644814372063\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 3, train_loss: 0.020664785614832728, valid_loss: 0.017648460566997527\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 4, train_loss: 0.020398293628173622, valid_loss: 0.0184003983438015\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 5, train_loss: 0.020346198180297606, valid_loss: 0.01731034830212593\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 6, train_loss: 0.02029641284322252, valid_loss: 0.017329430505633354\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 7, train_loss: 0.020275363787299112, valid_loss: 0.017526488713920117\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 8, train_loss: 0.020262241186130615, valid_loss: 0.017293213307857512\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 9, train_loss: 0.020248643673804342, valid_loss: 0.017407677508890627\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 10, train_loss: 0.02021679208696294, valid_loss: 0.01718348652124405\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 11, train_loss: 0.020160722197825404, valid_loss: 0.01713109973818064\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 12, train_loss: 0.02002653822961713, valid_loss: 0.017229744307696818\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 13, train_loss: 0.019966595178964187, valid_loss: 0.01710935801267624\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 14, train_loss: 0.019807514588849075, valid_loss: 0.016992327757179737\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 15, train_loss: 0.019662215368074623, valid_loss: 0.01694820586591959\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 16, train_loss: 0.019457635653464974, valid_loss: 0.016695034950971605\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 17, train_loss: 0.019286856536759812, valid_loss: 0.016556571535766125\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 18, train_loss: 0.018895301841148713, valid_loss: 0.016582569777965545\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 19, train_loss: 0.018596893391844366, valid_loss: 0.016396051198244097\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 20, train_loss: 0.018157703187443367, valid_loss: 0.016291103810071945\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 21, train_loss: 0.017680207368772047, valid_loss: 0.016263028495013713\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 22, train_loss: 0.017227120387057464, valid_loss: 0.016205364018678666\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 23, train_loss: 0.016880055160901578, valid_loss: 0.01618805754929781\r\n",
      "SEED: 1, FOLD: 5, EPOCH: 24, train_loss: 0.016702334022744982, valid_loss: 0.016194308139383794\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 0, train_loss: 0.4855310201239424, valid_loss: 0.02703348398208618\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 1, train_loss: 0.024086431809225862, valid_loss: 0.01900111321359873\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 2, train_loss: 0.021812539471655477, valid_loss: 0.01766168463975191\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 3, train_loss: 0.020755475196911365, valid_loss: 0.0175820105522871\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 4, train_loss: 0.020335578948867564, valid_loss: 0.01727523498237133\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 5, train_loss: 0.02029367125764185, valid_loss: 0.017172815166413783\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 6, train_loss: 0.0203240909518636, valid_loss: 0.017250796146690846\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 7, train_loss: 0.020270276003751624, valid_loss: 0.01709419932216406\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 8, train_loss: 0.02030707595451754, valid_loss: 0.017236168570816515\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 9, train_loss: 0.02029855925665826, valid_loss: 0.01734044075012207\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 10, train_loss: 0.0202883638973747, valid_loss: 0.01740894578397274\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 11, train_loss: 0.020161687453486482, valid_loss: 0.017306652292609213\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 12, train_loss: 0.02006070396932615, valid_loss: 0.016935730688273906\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 13, train_loss: 0.0200107073297306, valid_loss: 0.01700799323618412\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 14, train_loss: 0.019836245193367914, valid_loss: 0.016827103570103644\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 15, train_loss: 0.019672598296991823, valid_loss: 0.016654603481292725\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 16, train_loss: 0.01954488150876801, valid_loss: 0.016666191555559637\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 17, train_loss: 0.019246863454681674, valid_loss: 0.016517350748181343\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 18, train_loss: 0.01899958427260522, valid_loss: 0.016327155567705632\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 19, train_loss: 0.018609150547255465, valid_loss: 0.016259095184504985\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 20, train_loss: 0.018192931002348055, valid_loss: 0.016196407452225683\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 21, train_loss: 0.017747919651724044, valid_loss: 0.016145815327763557\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 22, train_loss: 0.017294240817681057, valid_loss: 0.016100306063890457\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 23, train_loss: 0.01697734408840841, valid_loss: 0.016065845750272274\r\n",
      "SEED: 1, FOLD: 6, EPOCH: 24, train_loss: 0.01678699052252737, valid_loss: 0.01608658231794834\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 0, train_loss: 0.48486816983188497, valid_loss: 0.026278076991438866\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 1, train_loss: 0.024166130720555377, valid_loss: 0.01922906905412674\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 2, train_loss: 0.021650249442579796, valid_loss: 0.01803523290902376\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 3, train_loss: 0.0206230110837286, valid_loss: 0.017660594657063485\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 4, train_loss: 0.02053443251215682, valid_loss: 0.017498674467206002\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 5, train_loss: 0.020317233630082233, valid_loss: 0.017426765710115432\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 6, train_loss: 0.02030029830833276, valid_loss: 0.017551063001155852\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 7, train_loss: 0.020376147334875704, valid_loss: 0.01757968582212925\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 8, train_loss: 0.020309085457199286, valid_loss: 0.017271734587848187\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 9, train_loss: 0.020252395041134894, valid_loss: 0.01742541655898094\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 10, train_loss: 0.020131157559095596, valid_loss: 0.017253480963408948\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 11, train_loss: 0.020167064973387588, valid_loss: 0.017343560233712196\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 12, train_loss: 0.020088479873173092, valid_loss: 0.01714892853051424\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 13, train_loss: 0.019985406716563264, valid_loss: 0.017030960172414778\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 14, train_loss: 0.019844190337929595, valid_loss: 0.01701503202319145\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 15, train_loss: 0.019676952388416342, valid_loss: 0.016960905976593495\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 16, train_loss: 0.019473370582777625, valid_loss: 0.016792281866073608\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 17, train_loss: 0.01919230794673469, valid_loss: 0.016636516340076922\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 18, train_loss: 0.018866126330531374, valid_loss: 0.016531803235411643\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 19, train_loss: 0.01852032963541292, valid_loss: 0.01630405191332102\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 20, train_loss: 0.018092295668106908, valid_loss: 0.01625065803527832\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 21, train_loss: 0.017565806446989782, valid_loss: 0.016164783611893652\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 22, train_loss: 0.017039497528655998, valid_loss: 0.016172923184931277\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 23, train_loss: 0.016692781219018155, valid_loss: 0.016157829016447068\r\n",
      "SEED: 2, FOLD: 0, EPOCH: 24, train_loss: 0.016469555298740765, valid_loss: 0.01616799846291542\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 0, train_loss: 0.4850542452439767, valid_loss: 0.026906156912446022\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 1, train_loss: 0.0241775220247353, valid_loss: 0.018785630315542222\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 2, train_loss: 0.021735658235594528, valid_loss: 0.018224719241261483\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 3, train_loss: 0.02095639430696056, valid_loss: 0.017356483563780785\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 4, train_loss: 0.020433722304649092, valid_loss: 0.017140369787812235\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 5, train_loss: 0.02033540335338132, valid_loss: 0.017144496776163577\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 6, train_loss: 0.02026286977184873, valid_loss: 0.017217416130006314\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 7, train_loss: 0.020348940399431047, valid_loss: 0.017586875073611737\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 8, train_loss: 0.020243314836098224, valid_loss: 0.01695141803473234\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 9, train_loss: 0.020281592624730803, valid_loss: 0.01733783781528473\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 10, train_loss: 0.0202796820323078, valid_loss: 0.016980729214847088\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 11, train_loss: 0.020250854785649145, valid_loss: 0.016985738724470137\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 12, train_loss: 0.02016178780824554, valid_loss: 0.016952414363622666\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 13, train_loss: 0.01999508723819337, valid_loss: 0.016885198801755905\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 14, train_loss: 0.019944000621738078, valid_loss: 0.016805411726236345\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 15, train_loss: 0.019733378030106323, valid_loss: 0.0165459743142128\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 16, train_loss: 0.019521641728727997, valid_loss: 0.01657992895692587\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 17, train_loss: 0.019275032655418325, valid_loss: 0.016351950727403163\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 18, train_loss: 0.018969640325830907, valid_loss: 0.016146958991885185\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 19, train_loss: 0.01860566458151657, valid_loss: 0.016117741875350474\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 20, train_loss: 0.01814400499724612, valid_loss: 0.015996904633939265\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 21, train_loss: 0.017667027842886998, valid_loss: 0.01588776096701622\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 22, train_loss: 0.017233705864015486, valid_loss: 0.015897610448300837\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 23, train_loss: 0.01685968165596326, valid_loss: 0.015840275958180428\r\n",
      "SEED: 2, FOLD: 1, EPOCH: 24, train_loss: 0.016670683614250753, valid_loss: 0.015859158784151076\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 0, train_loss: 0.48415367665136755, valid_loss: 0.027412045449018478\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 1, train_loss: 0.024063657652459988, valid_loss: 0.019417768605053426\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 2, train_loss: 0.021857593373275128, valid_loss: 0.018408848382532598\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 3, train_loss: 0.020702540050862597, valid_loss: 0.01753821864724159\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 4, train_loss: 0.020409291926898113, valid_loss: 0.017620163038372993\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 5, train_loss: 0.020299841694179036, valid_loss: 0.017635776735842226\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 6, train_loss: 0.020276832578032197, valid_loss: 0.017143600806593894\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 7, train_loss: 0.020284869423022076, valid_loss: 0.017496638000011444\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 8, train_loss: 0.020282923389657013, valid_loss: 0.01716847497969866\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 9, train_loss: 0.020278159009457446, valid_loss: 0.017261438146233557\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 10, train_loss: 0.02020603854234527, valid_loss: 0.017225546464323997\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 11, train_loss: 0.020179651107410997, valid_loss: 0.017393435686826705\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 12, train_loss: 0.02012313036944996, valid_loss: 0.01711309276521206\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 13, train_loss: 0.019978478234134563, valid_loss: 0.01707295123487711\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 14, train_loss: 0.019809800966763172, valid_loss: 0.017120052762329578\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 15, train_loss: 0.01978215757681399, valid_loss: 0.016768329851329325\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 16, train_loss: 0.01952704355171343, valid_loss: 0.01659551080316305\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 17, train_loss: 0.01924731763599276, valid_loss: 0.016560356728732586\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 18, train_loss: 0.018957372070575247, valid_loss: 0.016502744890749453\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 19, train_loss: 0.018579159844286586, valid_loss: 0.016272956393659114\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 20, train_loss: 0.01812711579180291, valid_loss: 0.016294676139950753\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 21, train_loss: 0.017694960329301502, valid_loss: 0.016171912662684917\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 22, train_loss: 0.01722715793773025, valid_loss: 0.016131962649524212\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 23, train_loss: 0.016881176961117052, valid_loss: 0.016153255961835384\r\n",
      "SEED: 2, FOLD: 2, EPOCH: 24, train_loss: 0.016692663555597367, valid_loss: 0.01617963444441557\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 0, train_loss: 0.4860706499137846, valid_loss: 0.027728870511054993\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 1, train_loss: 0.024156102049006086, valid_loss: 0.02010614961385727\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 2, train_loss: 0.021672399423154843, valid_loss: 0.01809546060860157\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 3, train_loss: 0.02056863613832159, valid_loss: 0.01759229861199856\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 4, train_loss: 0.020316832933296152, valid_loss: 0.017452814243733884\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 5, train_loss: 0.020290119839566096, valid_loss: 0.017421778738498688\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 6, train_loss: 0.020248497348456157, valid_loss: 0.01733278203755617\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 7, train_loss: 0.020251304633459265, valid_loss: 0.01747028701007366\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 8, train_loss: 0.020265637561172046, valid_loss: 0.017416230365633965\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 9, train_loss: 0.020257743088161053, valid_loss: 0.017222298420965673\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 10, train_loss: 0.020190626319472483, valid_loss: 0.017200152836740017\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 11, train_loss: 0.020124377150620733, valid_loss: 0.01702382680028677\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 12, train_loss: 0.020142872291965548, valid_loss: 0.01744294844567776\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 13, train_loss: 0.019964656013311172, valid_loss: 0.017306344136595726\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 14, train_loss: 0.019854920593147377, valid_loss: 0.016968940794467927\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 15, train_loss: 0.019632952351148435, valid_loss: 0.016941053383052348\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 16, train_loss: 0.019494173123317513, valid_loss: 0.01675991255789995\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 17, train_loss: 0.019206694168906635, valid_loss: 0.016684068478643895\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 18, train_loss: 0.018941133346433946, valid_loss: 0.016561641059815883\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 19, train_loss: 0.018556425170529457, valid_loss: 0.016477284990251063\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 20, train_loss: 0.018158519032354256, valid_loss: 0.016311015859246256\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 21, train_loss: 0.017669756421629264, valid_loss: 0.016275601908564568\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 22, train_loss: 0.01724167924602421, valid_loss: 0.016256581097841262\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 23, train_loss: 0.016924461128101462, valid_loss: 0.01625418063253164\r\n",
      "SEED: 2, FOLD: 3, EPOCH: 24, train_loss: 0.016724322073567076, valid_loss: 0.01622328218072653\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 0, train_loss: 0.4837224703459513, valid_loss: 0.027444935142993926\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 1, train_loss: 0.02420095527912078, valid_loss: 0.020246467962861062\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 2, train_loss: 0.021716042428093704, valid_loss: 0.017779578864574434\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 3, train_loss: 0.020679588841439105, valid_loss: 0.018138010650873185\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 4, train_loss: 0.02034451602166202, valid_loss: 0.01746876485645771\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 5, train_loss: 0.02024442390823851, valid_loss: 0.017342572286725044\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 6, train_loss: 0.020253750286540206, valid_loss: 0.017757791802287103\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 7, train_loss: 0.020281361606048078, valid_loss: 0.017444337606430053\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 8, train_loss: 0.020305991084093138, valid_loss: 0.017252823933959008\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 9, train_loss: 0.020321934148162402, valid_loss: 0.017498029470443724\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 10, train_loss: 0.02028464076115566, valid_loss: 0.01737238571047783\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 11, train_loss: 0.020104152486235104, valid_loss: 0.017321136966347694\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 12, train_loss: 0.020063937457949935, valid_loss: 0.017328689321875573\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 13, train_loss: 0.019981747531161016, valid_loss: 0.01710120301693678\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 14, train_loss: 0.01987494629662053, valid_loss: 0.017126332372426986\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 15, train_loss: 0.019660402140041597, valid_loss: 0.016906319372355937\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 16, train_loss: 0.01944033100846268, valid_loss: 0.01678055100142956\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 17, train_loss: 0.019212977770639926, valid_loss: 0.01655254192650318\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 18, train_loss: 0.01890907625407994, valid_loss: 0.01655360121279955\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 19, train_loss: 0.018557719886302948, valid_loss: 0.016421303302049637\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 20, train_loss: 0.01811517709169258, valid_loss: 0.01625261578708887\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 21, train_loss: 0.017671166291954567, valid_loss: 0.016168805062770842\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 22, train_loss: 0.0172057236690505, valid_loss: 0.016169703491032125\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 23, train_loss: 0.016840708613091587, valid_loss: 0.016183846406638624\r\n",
      "SEED: 2, FOLD: 4, EPOCH: 24, train_loss: 0.01666894729952423, valid_loss: 0.016190900914371014\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 0, train_loss: 0.48514190393493695, valid_loss: 0.026503110006451606\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 1, train_loss: 0.024022804256401905, valid_loss: 0.01935315243899822\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 2, train_loss: 0.02176960661601858, valid_loss: 0.018023220971226692\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 3, train_loss: 0.02077761696664249, valid_loss: 0.01740205965936184\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 4, train_loss: 0.02025368238235412, valid_loss: 0.017857398390769958\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 5, train_loss: 0.020258304643995906, valid_loss: 0.017389487475156784\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 6, train_loss: 0.020247414644782236, valid_loss: 0.017620463892817496\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 7, train_loss: 0.020249609955820907, valid_loss: 0.01711488950997591\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 8, train_loss: 0.020249027682810415, valid_loss: 0.01761736400425434\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 9, train_loss: 0.02024413622459587, valid_loss: 0.017365004979074002\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 10, train_loss: 0.02014995197809878, valid_loss: 0.017377741076052187\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 11, train_loss: 0.020142154818793542, valid_loss: 0.017318532764911652\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 12, train_loss: 0.020075995817172284, valid_loss: 0.017097331285476684\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 13, train_loss: 0.019880465364881923, valid_loss: 0.017109153792262077\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 14, train_loss: 0.019854573684991623, valid_loss: 0.01685816381126642\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 15, train_loss: 0.01960950055900885, valid_loss: 0.016823312789201735\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 16, train_loss: 0.01944122780241123, valid_loss: 0.01690103929489851\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 17, train_loss: 0.019164919840539394, valid_loss: 0.016634560227394103\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 18, train_loss: 0.018839939782509998, valid_loss: 0.016662316769361495\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 19, train_loss: 0.018493019234465093, valid_loss: 0.016388846598565578\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 20, train_loss: 0.018057675728080223, valid_loss: 0.016302552781999113\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 21, train_loss: 0.017590755729803016, valid_loss: 0.01627098109573126\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 22, train_loss: 0.017122837074962604, valid_loss: 0.016297346279025078\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 23, train_loss: 0.016750661909681597, valid_loss: 0.01622001323848963\r\n",
      "SEED: 2, FOLD: 5, EPOCH: 24, train_loss: 0.016572900775338517, valid_loss: 0.016212271340191365\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 0, train_loss: 0.48489144815727564, valid_loss: 0.027150531560182573\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 1, train_loss: 0.02430176144453133, valid_loss: 0.0192553149163723\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 2, train_loss: 0.021818881727704385, valid_loss: 0.01772585146129131\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 3, train_loss: 0.020677305200472982, valid_loss: 0.017520168349146843\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 4, train_loss: 0.0203471541607461, valid_loss: 0.017387184239923956\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 5, train_loss: 0.02034342309244636, valid_loss: 0.017205365374684332\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 6, train_loss: 0.020254019267705023, valid_loss: 0.01711292803287506\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 7, train_loss: 0.020301762664196442, valid_loss: 0.017379735969007014\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 8, train_loss: 0.02030580091689314, valid_loss: 0.017354398556053637\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 9, train_loss: 0.020236555854378103, valid_loss: 0.017074335440993307\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 10, train_loss: 0.020210787045712372, valid_loss: 0.017200924828648566\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 11, train_loss: 0.02013944467010141, valid_loss: 0.0171798275411129\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 12, train_loss: 0.020102066387023245, valid_loss: 0.01723004087805748\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 13, train_loss: 0.01992419948738043, valid_loss: 0.016838188767433166\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 14, train_loss: 0.01982948058569918, valid_loss: 0.016966731287539005\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 15, train_loss: 0.019713948550475698, valid_loss: 0.016968592666089535\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 16, train_loss: 0.019503033080068576, valid_loss: 0.016676869355142118\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 17, train_loss: 0.01920952806312616, valid_loss: 0.016548786535859106\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 18, train_loss: 0.018905340674884464, valid_loss: 0.016458182297647\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 19, train_loss: 0.018526570516682807, valid_loss: 0.016206441372632982\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 20, train_loss: 0.018065766171634603, valid_loss: 0.016228415817022324\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 21, train_loss: 0.017566516729337827, valid_loss: 0.016126291491091252\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 22, train_loss: 0.01709868727872769, valid_loss: 0.01608388591557741\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 23, train_loss: 0.01672608673978014, valid_loss: 0.016078271493315695\r\n",
      "SEED: 2, FOLD: 6, EPOCH: 24, train_loss: 0.01654235744329334, valid_loss: 0.016069403439760207\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 0, train_loss: 0.4859616705881698, valid_loss: 0.028053779155015945\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 1, train_loss: 0.024132276954902273, valid_loss: 0.019411957636475564\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 2, train_loss: 0.0217574400459828, valid_loss: 0.01839333266019821\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 3, train_loss: 0.02067861883413224, valid_loss: 0.017366625517606735\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 4, train_loss: 0.020290113833485817, valid_loss: 0.01743428476154804\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 5, train_loss: 0.02031592133955485, valid_loss: 0.017304982393980026\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 6, train_loss: 0.02021332940428841, valid_loss: 0.017424211576581\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 7, train_loss: 0.020302989703862847, valid_loss: 0.017283084206283092\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 8, train_loss: 0.02025771826556345, valid_loss: 0.017429098971188068\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 9, train_loss: 0.02024162186905235, valid_loss: 0.017225090637803076\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 10, train_loss: 0.020201999377332577, valid_loss: 0.017110814042389393\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 11, train_loss: 0.02011937311007863, valid_loss: 0.01739087700843811\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 12, train_loss: 0.02001018796851035, valid_loss: 0.01715019013732672\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 13, train_loss: 0.01996595804028365, valid_loss: 0.01718944191932678\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 14, train_loss: 0.01981840235795699, valid_loss: 0.016889336705207824\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 15, train_loss: 0.0196204926996004, valid_loss: 0.01671332065016031\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 16, train_loss: 0.01947001127057335, valid_loss: 0.016818216405808925\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 17, train_loss: 0.019135572599107715, valid_loss: 0.01663384884595871\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 18, train_loss: 0.01888346237440904, valid_loss: 0.016589915566146373\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 19, train_loss: 0.018528828156643175, valid_loss: 0.016287576295435428\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 20, train_loss: 0.018049603106365317, valid_loss: 0.016237600669264794\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 21, train_loss: 0.017602771900746286, valid_loss: 0.0161914175003767\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 22, train_loss: 0.017125182411297648, valid_loss: 0.01612610749900341\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 23, train_loss: 0.01674754289137263, valid_loss: 0.016092384532094002\r\n",
      "SEED: 3, FOLD: 0, EPOCH: 24, train_loss: 0.016551143546797792, valid_loss: 0.016100779958069324\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 0, train_loss: 0.4861417747527158, valid_loss: 0.027770129889249803\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 1, train_loss: 0.024219207934477703, valid_loss: 0.019307345896959306\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 2, train_loss: 0.02173707644347431, valid_loss: 0.017802198752760887\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 3, train_loss: 0.020719319303222253, valid_loss: 0.017654405236244203\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 4, train_loss: 0.020404902787334252, valid_loss: 0.01735053688287735\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 5, train_loss: 0.020284645158011896, valid_loss: 0.017101610228419304\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 6, train_loss: 0.020299458382081012, valid_loss: 0.017260483503341674\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 7, train_loss: 0.020339800298315325, valid_loss: 0.017350846827030184\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 8, train_loss: 0.02032785368513088, valid_loss: 0.017056914009153844\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 9, train_loss: 0.020281925100554415, valid_loss: 0.017470537535846233\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 10, train_loss: 0.020221569442323277, valid_loss: 0.017031304873526096\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 11, train_loss: 0.02013947999700397, valid_loss: 0.01715419802814722\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 12, train_loss: 0.020069879904186644, valid_loss: 0.01690587654709816\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 13, train_loss: 0.01995565965264833, valid_loss: 0.01684484861791134\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 14, train_loss: 0.019929069848287673, valid_loss: 0.016870530284941197\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 15, train_loss: 0.01972780892384701, valid_loss: 0.016776291988790035\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 16, train_loss: 0.019507547282949598, valid_loss: 0.01660904847085476\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 17, train_loss: 0.019213726389266196, valid_loss: 0.016395120210945607\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 18, train_loss: 0.01889921952875293, valid_loss: 0.01614853385835886\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 19, train_loss: 0.01855930419904845, valid_loss: 0.016143499314785002\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 20, train_loss: 0.018163655212997985, valid_loss: 0.01605254963040352\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 21, train_loss: 0.01766554509498635, valid_loss: 0.015949584282934665\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 22, train_loss: 0.017176569376115492, valid_loss: 0.01592178698629141\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 23, train_loss: 0.01679643504574996, valid_loss: 0.015934085436165334\r\n",
      "SEED: 3, FOLD: 1, EPOCH: 24, train_loss: 0.016621659592199488, valid_loss: 0.01591241292655468\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 0, train_loss: 0.4858682448183801, valid_loss: 0.026805311739444733\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 1, train_loss: 0.024001996457374015, valid_loss: 0.019134532511234283\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 2, train_loss: 0.02161137113461689, valid_loss: 0.01798176597803831\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 3, train_loss: 0.020610781842652633, valid_loss: 0.01747062221169472\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 4, train_loss: 0.02030092375162913, valid_loss: 0.01755542665719986\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 5, train_loss: 0.02028586653371652, valid_loss: 0.017420303635299207\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 6, train_loss: 0.020238766890196574, valid_loss: 0.017318734750151634\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 7, train_loss: 0.020291360158498594, valid_loss: 0.017415930964052678\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 8, train_loss: 0.02023774999998459, valid_loss: 0.017023269571363926\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 9, train_loss: 0.020165168952678336, valid_loss: 0.017407420687377453\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 10, train_loss: 0.02020639178602874, valid_loss: 0.017178812809288502\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 11, train_loss: 0.020149799342564986, valid_loss: 0.01714486762881279\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 12, train_loss: 0.019996509513481944, valid_loss: 0.01718133971095085\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 13, train_loss: 0.019978071531268203, valid_loss: 0.01710093770176172\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 14, train_loss: 0.019823417409646268, valid_loss: 0.016905344240367413\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 15, train_loss: 0.019725237459856635, valid_loss: 0.0168005670979619\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 16, train_loss: 0.019483706628789708, valid_loss: 0.01675428230315447\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 17, train_loss: 0.019180786041986374, valid_loss: 0.016496958546340465\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 18, train_loss: 0.01888966967104649, valid_loss: 0.016514841318130493\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 19, train_loss: 0.018511137120476386, valid_loss: 0.016256023980677128\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 20, train_loss: 0.018085663118177935, valid_loss: 0.016168062500655653\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 21, train_loss: 0.0175619790279845, valid_loss: 0.016204304546117782\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 22, train_loss: 0.017120132640916472, valid_loss: 0.016160065606236458\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 23, train_loss: 0.016738095162373012, valid_loss: 0.01614675596356392\r\n",
      "SEED: 3, FOLD: 2, EPOCH: 24, train_loss: 0.016568453190531456, valid_loss: 0.016181712187826634\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 0, train_loss: 0.48441974063511606, valid_loss: 0.027022151052951814\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 1, train_loss: 0.023972949519956193, valid_loss: 0.019235337376594542\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 2, train_loss: 0.021655129281436505, valid_loss: 0.01866318479180336\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 3, train_loss: 0.020633744240618077, valid_loss: 0.017785884216427805\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 4, train_loss: 0.020243211744391188, valid_loss: 0.01753781273961067\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 5, train_loss: 0.020286207929963156, valid_loss: 0.01760460689663887\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 6, train_loss: 0.02028374110354858, valid_loss: 0.017552823945879936\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 7, train_loss: 0.020248913909403646, valid_loss: 0.017667432203888892\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 8, train_loss: 0.0202434099830535, valid_loss: 0.017425943836569785\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 9, train_loss: 0.020145189751978633, valid_loss: 0.017352539002895354\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 10, train_loss: 0.020148246618760687, valid_loss: 0.017221601754426955\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 11, train_loss: 0.020087117633345176, valid_loss: 0.017383530251681805\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 12, train_loss: 0.01999974847600168, valid_loss: 0.01715034406632185\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 13, train_loss: 0.019924358344402444, valid_loss: 0.016990171931684016\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 14, train_loss: 0.019790415810484463, valid_loss: 0.017173174396157266\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 15, train_loss: 0.019612751647728643, valid_loss: 0.01681908715516329\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 16, train_loss: 0.019392453425494182, valid_loss: 0.016693899370729924\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 17, train_loss: 0.019167687807257482, valid_loss: 0.016536215655505657\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 18, train_loss: 0.01881704015695319, valid_loss: 0.016506660990417005\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 19, train_loss: 0.018433774352631195, valid_loss: 0.016399859599769116\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 20, train_loss: 0.01793018068332656, valid_loss: 0.01631598949432373\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 21, train_loss: 0.017462394992206372, valid_loss: 0.016255501843988895\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 22, train_loss: 0.016829029701295354, valid_loss: 0.016228556148707867\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 23, train_loss: 0.016428243014074508, valid_loss: 0.016275654733181\r\n",
      "SEED: 3, FOLD: 3, EPOCH: 24, train_loss: 0.01619487981863168, valid_loss: 0.016244042962789536\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 0, train_loss: 0.4861161773707591, valid_loss: 0.02748900033533573\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 1, train_loss: 0.02426222367148821, valid_loss: 0.019405171424150467\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 2, train_loss: 0.021745021942825543, valid_loss: 0.018104449063539505\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 3, train_loss: 0.020566637672129132, valid_loss: 0.017461805865168573\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 4, train_loss: 0.02038128468759206, valid_loss: 0.01730835221707821\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 5, train_loss: 0.020186116956934636, valid_loss: 0.01739023342728615\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 6, train_loss: 0.02027783512460942, valid_loss: 0.017478742226958274\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 7, train_loss: 0.020201839912099902, valid_loss: 0.017239472046494484\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 8, train_loss: 0.02018165813923693, valid_loss: 0.017481045499444008\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 9, train_loss: 0.02023651623198775, valid_loss: 0.017610482200980188\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 10, train_loss: 0.02015675783005296, valid_loss: 0.01749756306409836\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 11, train_loss: 0.02014163968970581, valid_loss: 0.017472163885831834\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 12, train_loss: 0.02007358604852034, valid_loss: 0.017228339910507203\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 13, train_loss: 0.01990948649135982, valid_loss: 0.017057106122374533\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 14, train_loss: 0.01977874428591355, valid_loss: 0.017021681927144528\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 15, train_loss: 0.01958928541058586, valid_loss: 0.017025641910731793\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 16, train_loss: 0.019482117146253586, valid_loss: 0.016753026992082597\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 17, train_loss: 0.019118813164278763, valid_loss: 0.01660404969006777\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 18, train_loss: 0.018778061012731117, valid_loss: 0.016558302491903307\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 19, train_loss: 0.018454207500227453, valid_loss: 0.016418914645910263\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 20, train_loss: 0.018000559304796514, valid_loss: 0.016343988217413424\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 21, train_loss: 0.017472213178518273, valid_loss: 0.016339814886450768\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 22, train_loss: 0.01700253460911058, valid_loss: 0.016281757317483426\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 23, train_loss: 0.016605539450130494, valid_loss: 0.016258421316742898\r\n",
      "SEED: 3, FOLD: 4, EPOCH: 24, train_loss: 0.01641592029862258, valid_loss: 0.016309230402112008\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 0, train_loss: 0.48650364644926825, valid_loss: 0.02937846854329109\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 1, train_loss: 0.02418797655778677, valid_loss: 0.0192717669904232\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 2, train_loss: 0.021665724539229658, valid_loss: 0.018220142051577567\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 3, train_loss: 0.02073491244342457, valid_loss: 0.017570450752973556\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 4, train_loss: 0.02034593652300283, valid_loss: 0.017547162026166917\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 5, train_loss: 0.020256653238011867, valid_loss: 0.017232734262943267\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 6, train_loss: 0.020250438364083263, valid_loss: 0.01717661164700985\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 7, train_loss: 0.020193082185424104, valid_loss: 0.017530574910342694\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 8, train_loss: 0.02027452269531026, valid_loss: 0.017562439292669298\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 9, train_loss: 0.020191281273656964, valid_loss: 0.01746709641069174\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 10, train_loss: 0.020176900690104686, valid_loss: 0.017353923916816713\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 11, train_loss: 0.020088967239978363, valid_loss: 0.017247214131057263\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 12, train_loss: 0.019971787118587363, valid_loss: 0.01711837690323591\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 13, train_loss: 0.019895816069780563, valid_loss: 0.01695764362812042\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 14, train_loss: 0.01978521136452957, valid_loss: 0.016978894248604774\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 15, train_loss: 0.019594865033821185, valid_loss: 0.016829152032732962\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 16, train_loss: 0.019447329536485834, valid_loss: 0.016725648045539856\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 17, train_loss: 0.019136079568035747, valid_loss: 0.016696153320372106\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 18, train_loss: 0.018853162535700668, valid_loss: 0.016520921289920807\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 19, train_loss: 0.018487020389360634, valid_loss: 0.01645285677164793\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 20, train_loss: 0.018045051200758843, valid_loss: 0.016371884793043138\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 21, train_loss: 0.01755291353404319, valid_loss: 0.016273358836770058\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 22, train_loss: 0.017019044075693403, valid_loss: 0.016292951218783856\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 23, train_loss: 0.01664023632247026, valid_loss: 0.016315447241067885\r\n",
      "SEED: 3, FOLD: 5, EPOCH: 24, train_loss: 0.016473894786773895, valid_loss: 0.016282137259840967\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 0, train_loss: 0.48570303949622473, valid_loss: 0.027356135174632073\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 1, train_loss: 0.02429626941630224, valid_loss: 0.019204769134521484\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 2, train_loss: 0.02170403206682935, valid_loss: 0.017762992568314075\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 3, train_loss: 0.020852800952840825, valid_loss: 0.01739810585975647\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 4, train_loss: 0.02036223032188659, valid_loss: 0.017265491634607315\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 5, train_loss: 0.020265248179638467, valid_loss: 0.017130673862993717\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 6, train_loss: 0.020260149473641194, valid_loss: 0.01726648334413767\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 7, train_loss: 0.020250073729120955, valid_loss: 0.017625163830816745\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 8, train_loss: 0.02020394737471123, valid_loss: 0.01724620759487152\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 9, train_loss: 0.020242919030339538, valid_loss: 0.01717070322483778\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 10, train_loss: 0.020186550091622638, valid_loss: 0.01704027008265257\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 11, train_loss: 0.020125769636257975, valid_loss: 0.017142894119024275\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 12, train_loss: 0.020063749026684535, valid_loss: 0.017101938985288143\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 13, train_loss: 0.01997448303768424, valid_loss: 0.017104139253497123\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 14, train_loss: 0.019927410016051767, valid_loss: 0.016888307332992555\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 15, train_loss: 0.019653551229814284, valid_loss: 0.01684849675744772\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 16, train_loss: 0.019488515801170246, valid_loss: 0.016538952589035035\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 17, train_loss: 0.019228709823622996, valid_loss: 0.016588534899055957\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 18, train_loss: 0.01893893216236108, valid_loss: 0.016448003277182578\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 19, train_loss: 0.01854182665749472, valid_loss: 0.016284976415336133\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 20, train_loss: 0.018122293783643214, valid_loss: 0.016231108792126178\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 21, train_loss: 0.017645272090524234, valid_loss: 0.016211279034614563\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 22, train_loss: 0.01720760928048771, valid_loss: 0.016129248887300492\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 23, train_loss: 0.016850158755927266, valid_loss: 0.016115427762269974\r\n",
      "SEED: 3, FOLD: 6, EPOCH: 24, train_loss: 0.016662124405000485, valid_loss: 0.01611274804919958\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 0, train_loss: 0.48648449923006853, valid_loss: 0.027592979297041895\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 1, train_loss: 0.024340472443878245, valid_loss: 0.019218099638819694\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 2, train_loss: 0.021820799023115716, valid_loss: 0.018016720190644264\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 3, train_loss: 0.021087452161068818, valid_loss: 0.01777870811522007\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 4, train_loss: 0.02058371603742343, valid_loss: 0.017525011450052263\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 5, train_loss: 0.02037377564274535, valid_loss: 0.017560154125094415\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 6, train_loss: 0.020376406850762106, valid_loss: 0.017363493777811528\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 7, train_loss: 0.020293669369654592, valid_loss: 0.017429804429411888\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 8, train_loss: 0.020250658409631982, valid_loss: 0.017384400218725206\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 9, train_loss: 0.020298342695649788, valid_loss: 0.01760283686220646\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 10, train_loss: 0.020325667674646896, valid_loss: 0.017142557241022587\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 11, train_loss: 0.02021194673871913, valid_loss: 0.01752396922558546\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 12, train_loss: 0.020078358044024226, valid_loss: 0.01702599212527275\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 13, train_loss: 0.019996325719822832, valid_loss: 0.017053296007215977\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 14, train_loss: 0.019863666092254678, valid_loss: 0.016936554461717605\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 15, train_loss: 0.01971783826038951, valid_loss: 0.01683183543384075\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 16, train_loss: 0.01948324171509467, valid_loss: 0.01671122096478939\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 17, train_loss: 0.01925635395482892, valid_loss: 0.016621430702507496\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 18, train_loss: 0.018966098347691453, valid_loss: 0.016435909681022168\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 19, train_loss: 0.01857199642781903, valid_loss: 0.01633357524871826\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 20, train_loss: 0.01819442281005334, valid_loss: 0.016225213445723058\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 21, train_loss: 0.017703587531435246, valid_loss: 0.016204623356461525\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 22, train_loss: 0.017306847326761604, valid_loss: 0.01616865012794733\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 23, train_loss: 0.01694372628948518, valid_loss: 0.016170653104782103\r\n",
      "SEED: 4, FOLD: 0, EPOCH: 24, train_loss: 0.016802391208404183, valid_loss: 0.016194346472620965\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 0, train_loss: 0.4854813919966521, valid_loss: 0.02808937765657902\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 1, train_loss: 0.024241761184063088, valid_loss: 0.019954268857836722\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 2, train_loss: 0.0220435171128333, valid_loss: 0.01802720509469509\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 3, train_loss: 0.020756831468672167, valid_loss: 0.01783558204770088\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 4, train_loss: 0.02042199735890846, valid_loss: 0.017216690964996815\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 5, train_loss: 0.020279241801828753, valid_loss: 0.017108234502375128\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 6, train_loss: 0.020284940228879857, valid_loss: 0.01695551201701164\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 7, train_loss: 0.020237782615281287, valid_loss: 0.0177689902856946\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 8, train_loss: 0.020217809015188087, valid_loss: 0.017165420837700368\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 9, train_loss: 0.020291198881304994, valid_loss: 0.017282936312258242\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 10, train_loss: 0.020236422529532797, valid_loss: 0.017251186035573483\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 11, train_loss: 0.020134887499671406, valid_loss: 0.017024817131459712\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 12, train_loss: 0.020110892068569353, valid_loss: 0.016874501220881938\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 13, train_loss: 0.019996755230588976, valid_loss: 0.0171959276124835\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 14, train_loss: 0.01991139229412387, valid_loss: 0.016674780808389186\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 15, train_loss: 0.0197074779112931, valid_loss: 0.01655807577073574\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 16, train_loss: 0.019519325041649292, valid_loss: 0.01636190269142389\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 17, train_loss: 0.019231778943315656, valid_loss: 0.01623410977423191\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 18, train_loss: 0.018900936051290864, valid_loss: 0.016337696500122546\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 19, train_loss: 0.018558995988296002, valid_loss: 0.01615246422588825\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 20, train_loss: 0.01810801446716599, valid_loss: 0.01597784213721752\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 21, train_loss: 0.01759730970651722, valid_loss: 0.01593258149921894\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 22, train_loss: 0.01704129211123095, valid_loss: 0.015877346247434615\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 23, train_loss: 0.016707684958771785, valid_loss: 0.015859312154352666\r\n",
      "SEED: 4, FOLD: 1, EPOCH: 24, train_loss: 0.016481777399677, valid_loss: 0.015849479995667933\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 0, train_loss: 0.48586773364266167, valid_loss: 0.026793498173356058\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 1, train_loss: 0.02422587022337378, valid_loss: 0.019347065314650535\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 2, train_loss: 0.02176467874119071, valid_loss: 0.01816142674535513\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 3, train_loss: 0.020670942493340595, valid_loss: 0.017705973833799363\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 4, train_loss: 0.020332524160138605, valid_loss: 0.017286247201263906\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 5, train_loss: 0.020182565146056163, valid_loss: 0.01726822856813669\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 6, train_loss: 0.020258787461892278, valid_loss: 0.017213106714189054\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 7, train_loss: 0.020234635061755473, valid_loss: 0.017715395390987397\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 8, train_loss: 0.020195120438730636, valid_loss: 0.017502771504223347\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 9, train_loss: 0.020224651625874092, valid_loss: 0.017106965482234955\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 10, train_loss: 0.020197257729006463, valid_loss: 0.017520726211369037\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 11, train_loss: 0.0201301591559535, valid_loss: 0.017160445861518384\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 12, train_loss: 0.020089264325544136, valid_loss: 0.017171042971313\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 13, train_loss: 0.01994904637539468, valid_loss: 0.016781682819128035\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 14, train_loss: 0.019806849877951907, valid_loss: 0.016859106980264188\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 15, train_loss: 0.019665542901271864, valid_loss: 0.016796796545386315\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 16, train_loss: 0.019422749006727927, valid_loss: 0.01673838261514902\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 17, train_loss: 0.019162657854407013, valid_loss: 0.01653799995779991\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 18, train_loss: 0.018862177250387312, valid_loss: 0.01641395315527916\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 19, train_loss: 0.018482814384561008, valid_loss: 0.016242699623107912\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 20, train_loss: 0.018017246349885756, valid_loss: 0.016204214878380297\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 21, train_loss: 0.017504784595347992, valid_loss: 0.016179576478898526\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 22, train_loss: 0.01701690377604191, valid_loss: 0.01613845631480217\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 23, train_loss: 0.01662327453088598, valid_loss: 0.016133109778165816\r\n",
      "SEED: 4, FOLD: 2, EPOCH: 24, train_loss: 0.016417550556615097, valid_loss: 0.01610789217054844\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 0, train_loss: 0.4874221816988421, valid_loss: 0.027087286114692688\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 1, train_loss: 0.02403157040932957, valid_loss: 0.01919497661292553\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 2, train_loss: 0.021686706096440755, valid_loss: 0.018160159289836882\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 3, train_loss: 0.020670408104248597, valid_loss: 0.01747156374156475\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 4, train_loss: 0.020407254079065355, valid_loss: 0.017534120827913283\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 5, train_loss: 0.020280710111061733, valid_loss: 0.01803814060986042\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 6, train_loss: 0.020282068764980957, valid_loss: 0.01739111475646496\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 7, train_loss: 0.02030674131194345, valid_loss: 0.017213009707629682\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 8, train_loss: 0.020270241259717617, valid_loss: 0.017405218333005904\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 9, train_loss: 0.02020698409451514, valid_loss: 0.017412448935210705\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 10, train_loss: 0.02018974799992276, valid_loss: 0.017143494077026845\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 11, train_loss: 0.02012006010936231, valid_loss: 0.01728799946606159\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 12, train_loss: 0.020036861439850055, valid_loss: 0.017238966189324856\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 13, train_loss: 0.020066054271800176, valid_loss: 0.01722659286111593\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 14, train_loss: 0.019863483147556277, valid_loss: 0.016965116560459136\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 15, train_loss: 0.019616737290203166, valid_loss: 0.01703462466597557\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 16, train_loss: 0.01946326948347546, valid_loss: 0.016849502325057983\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 17, train_loss: 0.01925038921387017, valid_loss: 0.016769621111452578\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 18, train_loss: 0.01894835203404532, valid_loss: 0.016594000086188318\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 19, train_loss: 0.01857192473498737, valid_loss: 0.01640221640467644\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 20, train_loss: 0.0181652768990215, valid_loss: 0.016306559406220913\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 21, train_loss: 0.017705937391337082, valid_loss: 0.01625762764364481\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 22, train_loss: 0.017249465014050606, valid_loss: 0.016264461427927018\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 23, train_loss: 0.0169251870655385, valid_loss: 0.016193805448710918\r\n",
      "SEED: 4, FOLD: 3, EPOCH: 24, train_loss: 0.016743811512632028, valid_loss: 0.01624277237802744\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 0, train_loss: 0.48505547747877586, valid_loss: 0.027289929762482644\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 1, train_loss: 0.024105221970754417, valid_loss: 0.019530944749712943\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 2, train_loss: 0.02173783389382622, valid_loss: 0.018087461814284323\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 3, train_loss: 0.02066067327447489, valid_loss: 0.01752949081361294\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 4, train_loss: 0.02034807003730414, valid_loss: 0.017293707951903343\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 5, train_loss: 0.020273708808077436, valid_loss: 0.01759194791316986\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 6, train_loss: 0.02025064456016839, valid_loss: 0.017399344444274902\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 7, train_loss: 0.02032485961609957, valid_loss: 0.01758312910795212\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 8, train_loss: 0.020280996958414715, valid_loss: 0.01728625636547804\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 9, train_loss: 0.020211039681215674, valid_loss: 0.017331901714205743\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 10, train_loss: 0.02023649671856238, valid_loss: 0.017282224223017693\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 11, train_loss: 0.020155377743043462, valid_loss: 0.01738218266516924\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 12, train_loss: 0.020100660368698797, valid_loss: 0.017240177355706692\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 13, train_loss: 0.019983130779598846, valid_loss: 0.017354904413223265\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 14, train_loss: 0.019863626444522216, valid_loss: 0.017060091681778432\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 15, train_loss: 0.01969472920762844, valid_loss: 0.016886072531342507\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 16, train_loss: 0.01943267150750371, valid_loss: 0.01680486299097538\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 17, train_loss: 0.019242207716111424, valid_loss: 0.016698302365839482\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 18, train_loss: 0.018948580537523543, valid_loss: 0.016614075116813183\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 19, train_loss: 0.018500713517471234, valid_loss: 0.016383701153099537\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 20, train_loss: 0.01810161706034829, valid_loss: 0.016330348886549474\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 21, train_loss: 0.017616124696978906, valid_loss: 0.016297354362905025\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 22, train_loss: 0.01714468155005554, valid_loss: 0.01627189017832279\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 23, train_loss: 0.016752347768265372, valid_loss: 0.01621806550770998\r\n",
      "SEED: 4, FOLD: 4, EPOCH: 24, train_loss: 0.016604393264468834, valid_loss: 0.016206761784851552\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 0, train_loss: 0.4868869958128653, valid_loss: 0.02754642017185688\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 1, train_loss: 0.024109760755482986, valid_loss: 0.01911005422472954\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 2, train_loss: 0.02172495308173757, valid_loss: 0.017983309179544448\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 3, train_loss: 0.02075994160457128, valid_loss: 0.017510126754641534\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 4, train_loss: 0.020309285887954186, valid_loss: 0.017617026194930076\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 5, train_loss: 0.02028255296402237, valid_loss: 0.01750649347901344\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 6, train_loss: 0.020267402715220744, valid_loss: 0.017563988231122495\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 7, train_loss: 0.020265517629632333, valid_loss: 0.017563489824533464\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 8, train_loss: 0.020232454069009444, valid_loss: 0.01739858854562044\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 9, train_loss: 0.02021029051773402, valid_loss: 0.01736284077167511\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 10, train_loss: 0.020208817875000083, valid_loss: 0.017257049009203912\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 11, train_loss: 0.020146703841734906, valid_loss: 0.017262937612831594\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 12, train_loss: 0.020038492445434843, valid_loss: 0.0171284544095397\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 13, train_loss: 0.019918091468462327, valid_loss: 0.017165916934609414\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 14, train_loss: 0.019803787955418737, valid_loss: 0.01687412053346634\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 15, train_loss: 0.019627454988405006, valid_loss: 0.01692492913454771\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 16, train_loss: 0.01938986826409288, valid_loss: 0.01684180159121752\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 17, train_loss: 0.019182229712352054, valid_loss: 0.016658799797296522\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 18, train_loss: 0.018850832417303204, valid_loss: 0.016540219858288765\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 19, train_loss: 0.018464455298673944, valid_loss: 0.016514821089804173\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 20, train_loss: 0.01804512718274277, valid_loss: 0.01628930389881134\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 21, train_loss: 0.017577765604519114, valid_loss: 0.01619004223495722\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 22, train_loss: 0.01710292343430373, valid_loss: 0.01621967412531376\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 23, train_loss: 0.01672597714148614, valid_loss: 0.01617979899048805\r\n",
      "SEED: 4, FOLD: 5, EPOCH: 24, train_loss: 0.016534451160858683, valid_loss: 0.01618980035185814\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 0, train_loss: 0.4865719797659893, valid_loss: 0.02771926425397396\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 1, train_loss: 0.024187165306133476, valid_loss: 0.019623883441090582\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 2, train_loss: 0.02181928922884724, valid_loss: 0.017797789312899114\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 3, train_loss: 0.020702620942880508, valid_loss: 0.017405958250164985\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 4, train_loss: 0.020348659494803065, valid_loss: 0.017286877818405627\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 5, train_loss: 0.020237389179009968, valid_loss: 0.017007095031440256\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 6, train_loss: 0.020241495589313863, valid_loss: 0.017334331199526785\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 7, train_loss: 0.020288803253550917, valid_loss: 0.01718953888863325\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 8, train_loss: 0.02025030884055459, valid_loss: 0.017166362069547178\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 9, train_loss: 0.0202264883460439, valid_loss: 0.01711308516561985\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 10, train_loss: 0.020245130192868565, valid_loss: 0.01722128801047802\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 11, train_loss: 0.020146851256793857, valid_loss: 0.017066425010561943\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 12, train_loss: 0.020030777939424222, valid_loss: 0.01721904080361128\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 13, train_loss: 0.019948764571121762, valid_loss: 0.016946023255586626\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 14, train_loss: 0.01982857840021654, valid_loss: 0.016993591785430907\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 15, train_loss: 0.019645927689310644, valid_loss: 0.016947240345180036\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 16, train_loss: 0.019503776693627947, valid_loss: 0.01666475649923086\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 17, train_loss: 0.019208978316714975, valid_loss: 0.01642577938735485\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 18, train_loss: 0.018907294462935455, valid_loss: 0.016383116133511066\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 19, train_loss: 0.0185968410350433, valid_loss: 0.0164004922285676\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 20, train_loss: 0.018136944076945993, valid_loss: 0.01616513032466173\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 21, train_loss: 0.017626488264523395, valid_loss: 0.01611859183758497\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 22, train_loss: 0.017189265450551397, valid_loss: 0.016112845428287982\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 23, train_loss: 0.016877691280476902, valid_loss: 0.016086423099040986\r\n",
      "SEED: 4, FOLD: 6, EPOCH: 24, train_loss: 0.016666265521325222, valid_loss: 0.016095725819468498\r\n",
      "len target_cols:  206\r\n",
      "CV log_loss:  0.014596953733071657\r\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "# if len(test) == 3982:\n",
    "    # submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "    # submission.to_csv('submission.csv', index=False)\n",
    "# else:\n",
    "# ! python ../input/baselineinference/nn-fe-transfer-1836.py\n",
    "# ! python ../input/baselineinference/resnet-pca-1850.py\n",
    "! python ../input/baselineinference/tabnet-base-1848.py  # 1844\n",
    "! python ../input/baselineinference/dae-nn-1849.py\n",
    "! python ../input/baselineinference/nn-rankgauss-pca-1838.py\n",
    "\n",
    "\n",
    "submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "submission.iloc[:, 1:] = 0\n",
    "for sub in glob.glob('./*.csv'):\n",
    "    submission.iloc[:, 1:] += pd.read_csv(sub).iloc[:, 1:] * (1/len(glob.glob('./*.csv')))\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 5286.997695,
   "end_time": "2020-11-30T02:12:50.741700",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-30T00:44:43.744005",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
