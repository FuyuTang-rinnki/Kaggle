{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os, sys, random, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, deep=True, verbose=True, categories=True):\n",
    "    # All types that we want to change for \"lighter\" ones.\n",
    "    # int8 and float16 are not include because we cannot reduce\n",
    "    # those data types.\n",
    "    # float32 is not include because float16 has too low precision.\n",
    "    numeric2reduce = [\"int16\", \"int32\", \"int64\", \"float64\"]\n",
    "    start_mem = 0\n",
    "    if verbose:\n",
    "        start_mem = memory_usage_mb(df, deep=deep)\n",
    "\n",
    "    for col, col_type in df.dtypes.iteritems():\n",
    "        best_type = None\n",
    "        if col_type == \"object\":\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "            best_type = \"category\"\n",
    "        elif col_type in numeric2reduce:\n",
    "            downcast = \"integer\" if \"int\" in str(col_type) else \"float\"\n",
    "            df[col] = pd.to_numeric(df[col], downcast=downcast)\n",
    "            best_type = df[col].dtype.name\n",
    "        # Log the conversion performed.\n",
    "        if verbose and best_type is not None and best_type != str(col_type):\n",
    "            print(f\"Column '{col}' converted from {col_type} to {best_type}\")\n",
    "\n",
    "    if verbose:\n",
    "        end_mem = memory_usage_mb(df, deep=deep)\n",
    "        diff_mem = start_mem - end_mem\n",
    "        percent_mem = 100 * diff_mem / start_mem\n",
    "        print(f\"Memory usage decreased from\"\n",
    "              f\" {start_mem:.2f}MB to {end_mem:.2f}MB\"\n",
    "              f\" ({diff_mem:.2f}MB, {percent_mem:.2f}% reduction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "Reduce Memory\n",
      "Mem. usage decreased to 542.35 Mb (69.4% reduction)\n",
      "Mem. usage decreased to 473.07 Mb (68.9% reduction)\n",
      "Mem. usage decreased to 25.86 Mb (42.7% reduction)\n",
      "Mem. usage decreased to 25.44 Mb (42.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "print('Load Data')\n",
    "train_df = pd.read_csv('../input/train_transaction.csv')\n",
    "test_df = pd.read_csv('../input/test_transaction.csv')\n",
    "test_df['isFraud'] = 0\n",
    "train_identity = pd.read_csv('../input/train_identity.csv')\n",
    "test_identity = pd.read_csv('../input/test_identity.csv')\n",
    "print('Reduce Memory')\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df  = reduce_mem_usage(test_df)\n",
    "train_identity = reduce_mem_usage(train_identity)\n",
    "test_identity  = reduce_mem_usage(test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_split(dataframe):\n",
    "    \n",
    "    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "\n",
    "    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n",
    " \n",
    "    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n",
    "\n",
    "    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n",
    "    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n",
    "    dataframe['id_12'] = dataframe['id_12'].map({'Found':1, 'NotFound':0})\n",
    "    dataframe['id_15'] = dataframe['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n",
    "    dataframe['id_16'] = dataframe['id_16'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    dataframe['id_23'] = dataframe['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n",
    "\n",
    "    dataframe['id_27'] = dataframe['id_27'].map({'Found':1, 'NotFound':0})\n",
    "    dataframe['id_28'] = dataframe['id_28'].map({'New':2, 'Found':1})\n",
    "\n",
    "    dataframe['id_29'] = dataframe['id_29'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    dataframe['id_35'] = dataframe['id_35'].map({'T':1, 'F':0})\n",
    "    dataframe['id_36'] = dataframe['id_36'].map({'T':1, 'F':0})\n",
    "    dataframe['id_37'] = dataframe['id_37'].map({'T':1, 'F':0})\n",
    "    dataframe['id_38'] = dataframe['id_38'].map({'T':1, 'F':0})\n",
    "\n",
    "    dataframe['id_34'] = dataframe['id_34'].fillna(':0')\n",
    "    dataframe['id_34'] = dataframe['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n",
    "    dataframe['id_34'] = np.where(dataframe['id_34']==0, np.nan, dataframe['id_34'])\n",
    "    \n",
    "    dataframe['id_33'] = dataframe['id_33'].fillna('0x0')\n",
    "    dataframe['id_33_0'] = dataframe['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n",
    "    dataframe['id_33_1'] = dataframe['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n",
    "    dataframe['id_33'] = np.where(dataframe['id_33']=='0x0', np.nan, dataframe['id_33'])\n",
    "    \n",
    "    for feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n",
    "        dataframe[feature + '_count_dist'] = dataframe[feature].map(dataframe[feature].value_counts(dropna=False))\n",
    "    \n",
    "    dataframe['DeviceType'].map({'desktop':1, 'mobile':0})\n",
    "    \n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "\n",
    "    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n",
    "    dataframe['had_id'] = 1\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity = id_split(train_identity)\n",
    "test_identity = id_split(test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3663549</td>\n",
       "      <td>18403224</td>\n",
       "      <td>31.953125</td>\n",
       "      <td>W</td>\n",
       "      <td>10409</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3663550</td>\n",
       "      <td>18403263</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>W</td>\n",
       "      <td>4272</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3663551</td>\n",
       "      <td>18403310</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>W</td>\n",
       "      <td>4476</td>\n",
       "      <td>574.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3663552</td>\n",
       "      <td>18403310</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>W</td>\n",
       "      <td>10989</td>\n",
       "      <td>360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3663553</td>\n",
       "      <td>18403317</td>\n",
       "      <td>67.937500</td>\n",
       "      <td>W</td>\n",
       "      <td>18018</td>\n",
       "      <td>452.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
       "0        3663549       18403224       31.953125         W  10409  111.0   \n",
       "1        3663550       18403263       49.000000         W   4272  111.0   \n",
       "2        3663551       18403310      171.000000         W   4476  574.0   \n",
       "3        3663552       18403310      285.000000         W  10989  360.0   \n",
       "4        3663553       18403317       67.937500         W  18018  452.0   \n",
       "\n",
       "   card3       card4  card5  card6  ...  V331  V332  V333  V334 V335 V336  \\\n",
       "0  150.0        visa  226.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "1  150.0        visa  226.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "2  150.0        visa  226.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "3  150.0        visa  166.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "4  150.0  mastercard  117.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       "\n",
       "   V337  V338  V339  isFraud  \n",
       "0   NaN   NaN   NaN        0  \n",
       "1   NaN   NaN   NaN        0  \n",
       "2   NaN   NaN   NaN        0  \n",
       "3   NaN   NaN   NaN        0  \n",
       "4   NaN   NaN   NaN        0  \n",
       "\n",
       "[5 rows x 394 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new features trans\n",
    "def gen_new(train_trans,test_trans):\n",
    "\n",
    "    # New feature - log of transaction amount.\n",
    "    train_trans['TransactionAmt_Log'] = np.log1p(train_trans['TransactionAmt'])\n",
    "    test_trans['TransactionAmt_Log'] = np.log1p(test_trans['TransactionAmt'])\n",
    "\n",
    "    # New feature - decimal part of the transaction amount.\n",
    "    train_trans['TransactionAmt_decimal'] = ((train_trans['TransactionAmt'] - train_trans['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "    test_trans['TransactionAmt_decimal'] = ((test_trans['TransactionAmt'] - test_trans['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "\n",
    "    \n",
    "    # New feature - day of week in which a transaction happened.\n",
    "    train_trans['Transaction_day_of_week'] = np.floor((train_trans['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "    test_trans['Transaction_day_of_week'] = np.floor((test_trans['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "\n",
    "    # New feature - hour of the day in which a transaction happened.\n",
    "    train_trans['Transaction_hour'] = np.floor(train_trans['TransactionDT'] / 3600) % 24\n",
    "    test_trans['Transaction_hour'] = np.floor(test_trans['TransactionDT'] / 3600) % 24\n",
    "    \n",
    "    #train_trans['cents'] = np.round(train_trans['TransactionAmt'] - np.floor(train_trans['TransactionAmt']),2 )\n",
    "    #test_trans['cents'] = np.round(test_trans['TransactionAmt'] - np.floor(test_trans['TransactionAmt']),2 )\n",
    "    \n",
    "    \n",
    "    #New feature - emaildomain with suffix\n",
    "    emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple','uknown':'uknown'}\n",
    "    us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "    for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "        train_trans[c] = train_trans[c].fillna('uknown')\n",
    "        test_trans[c] = test_trans[c].fillna('uknown')\n",
    "        \n",
    "        train_trans[c + '_bin'] = train_trans[c].map(emails)\n",
    "        test_trans[c + '_bin'] = test_trans[c].map(emails)\n",
    "    \n",
    "        train_trans[c + '_suffix'] = train_trans[c].apply(lambda x: str(x).split('.')[-1])\n",
    "        test_trans[c + '_suffix'] = test_trans[c].apply(lambda x: str(x).split('.')[-1])\n",
    "        \n",
    "        train_trans[c + '_prefix'] = train_trans[c].apply(lambda x: str(x).split('.')[0])\n",
    "        test_trans[c + '_prefix'] = test_trans[c].apply(lambda x: str(x).split('.')[0])\n",
    "\n",
    "        train_trans[c + '_suffix_us'] = train_trans[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "        test_trans[c + '_suffix_us'] = test_trans[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    train_trans['email_check'] = np.where((train_trans['P_emaildomain']==train_trans['R_emaildomain'])&(train_trans['P_emaildomain']!='uknown'),1,0)\n",
    "    test_trans['email_check'] = np.where((test_trans['P_emaildomain']==test_trans['R_emaildomain'])&(test_trans['P_emaildomain']!='uknown'),1,0)\n",
    "    \n",
    "    #New feature - dates\n",
    "    START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "    dates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n",
    "    us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
    "\n",
    "    for df in [train_trans, test_trans]:\n",
    "        # Temporary\n",
    "        df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "        df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n",
    "        df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n",
    "        df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n",
    "\n",
    "        df['DT_hour'] = df['DT'].dt.hour\n",
    "        df['DT_day_week'] = df['DT'].dt.dayofweek\n",
    "        df['DT_day'] = df['DT'].dt.day\n",
    "        df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)\n",
    "        # Possible solo feature\n",
    "        df['is_december'] = df['DT'].dt.month\n",
    "        df['is_december'] = (df['is_december']==12).astype(np.int8)\n",
    "\n",
    "        # Holidays\n",
    "        df['is_holiday'] = (df['DT'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n",
    "   \n",
    "    #New feature - binary encoded 1/0 gen new\n",
    "    i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "    for df in [train_trans, test_trans]:\n",
    "        df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n",
    "        df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)\n",
    "\n",
    "    #New feature - ProductCD and M4 Target mean\n",
    "    for col in ['ProductCD','M4']:\n",
    "        temp_dict = train_trans.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n",
    "        temp_dict.index = temp_dict[col].values\n",
    "        temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "        train_trans[col+'_target_mean'] = train_trans[col].map(temp_dict)\n",
    "        test_trans[col+'_target_mean']  = test_trans[col].map(temp_dict)\n",
    "    \n",
    "    #New feature - use it for aggregations\n",
    "    train_trans['uid1'] = train_trans['card1'].astype(str)+'_'+train_trans['card2'].astype(str) \n",
    "    test_trans['uid1'] = test_trans['card1'].astype(str)+'_'+test_trans['card2'].astype(str)\n",
    "\n",
    "    train_trans['uid2'] = train_trans['uid1'].astype(str)+'_'+train_trans['card3'].astype(str)+'_'+train_trans['card5'].astype(str)\n",
    "    test_trans['uid2'] = test_trans['uid1'].astype(str)+'_'+test_trans['card3'].astype(str)+'_'+test_trans['card5'].astype(str)\n",
    "\n",
    "    train_trans['uid3'] = train_trans['uid2'].astype(str)+'_'+train_trans['addr1'].astype(str)+'_'+train_trans['addr2'].astype(str)\n",
    "    test_trans['uid3'] = test_trans['uid2'].astype(str)+'_'+test_trans['addr1'].astype(str)+'_'+test_trans['addr2'].astype(str)\n",
    "\n",
    "    # Check if the Transaction Amount is common or not (we can use freq encoding here)\n",
    "    # In our dialog with a model we are telling to trust or not to these values   \n",
    "    # Clip Values\n",
    "    train_trans['TransactionAmt'] = train_trans['TransactionAmt'].clip(0,5000)\n",
    "    test_trans['TransactionAmt']  = test_trans['TransactionAmt'].clip(0,5000)\n",
    "\n",
    "    train_trans['TransactionAmt_check'] = np.where(train_trans['TransactionAmt'].isin(test_trans['TransactionAmt']), 1, 0)\n",
    "    test_trans['TransactionAmt_check']  = np.where(test_trans['TransactionAmt'].isin(train_trans['TransactionAmt']), 1, 0)\n",
    "\n",
    "\n",
    "    return train_trans,test_trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = gen_new(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeblock_frequency_encoding(train_df, test_df, periods, columns, \n",
    "                                 with_proportions=True, only_proportions=False):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            train_df[new_col] = train_df[col].astype(str)+'_'+train_df[period].astype(str)\n",
    "            test_df[new_col]  = test_df[col].astype(str)+'_'+test_df[period].astype(str)\n",
    "\n",
    "            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n",
    "            fq_encode = temp_df[new_col].value_counts().to_dict()\n",
    "\n",
    "            train_df[new_col] = train_df[new_col].map(fq_encode)\n",
    "            test_df[new_col]  = test_df[new_col].map(fq_encode)\n",
    "            \n",
    "            if only_proportions:\n",
    "                train_df[new_col] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col]  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "            if with_proportions:\n",
    "                train_df[new_col+'_proportions'] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col+'_proportions']  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "def uid_aggregation_and_normalization(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            \n",
    "            new_norm_col_name = col+'_'+main_column+'_std_norm'\n",
    "            norm_cols = []\n",
    "            \n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "                norm_cols.append(new_col_name)\n",
    "            \n",
    "            train_df[new_norm_col_name] = (train_df[main_column]-train_df[norm_cols[0]])/train_df[norm_cols[1]]\n",
    "            test_df[new_norm_col_name]  = (test_df[main_column]-test_df[norm_cols[0]])/test_df[norm_cols[1]]          \n",
    "            \n",
    "            del train_df[norm_cols[0]], train_df[norm_cols[1]]\n",
    "            del test_df[norm_cols[0]], test_df[norm_cols[1]]\n",
    "                                              \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encoding(train_df, test_df, columns, self_encoding=False):\n",
    "    for col in columns:\n",
    "        temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "        fq_encode = temp_df[col].value_counts(dropna=False).to_dict()\n",
    "        if self_encoding:\n",
    "            train_df[col] = train_df[col].map(fq_encode)\n",
    "            test_df[col]  = test_df[col].map(fq_encode)            \n",
    "        else:\n",
    "            train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
    "            test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. More interaction between card features + fill nans\n",
    "i_cols = ['TransactionID','card1','card2','card3','card4','card5','card6']\n",
    "\n",
    "full_df = pd.concat([train_df[i_cols], test_df[i_cols]])\n",
    "\n",
    "## I've used frequency encoding before so we have ints here\n",
    "## we will drop very rare cards\n",
    "full_df['card6'] = np.where(full_df['card6']==30, np.nan, full_df['card6'])\n",
    "full_df['card6'] = np.where(full_df['card6']==16, np.nan, full_df['card6'])\n",
    "\n",
    "i_cols = ['card2','card3','card4','card5','card6']\n",
    "\n",
    "## We will find best match for nan values and fill with it 把23456都补上好多了\n",
    "for col in i_cols:\n",
    "    temp_df = full_df.groupby(['card1',col])[col].agg(['count']).reset_index()\n",
    "    temp_df = temp_df.sort_values(by=['card1','count'], ascending=False).reset_index(drop=True)\n",
    "    del temp_df['count']\n",
    "    temp_df = temp_df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    temp_df.index = temp_df['card1'].values\n",
    "    temp_df = temp_df[col].to_dict()\n",
    "    full_df[col] = np.where(full_df[col].isna(), full_df['card1'].map(temp_df), full_df[col])\n",
    "    \n",
    "    \n",
    "i_cols = ['card1','card2','card3','card4','card5','card6']\n",
    "for col in i_cols:\n",
    "    train_df[col] = full_df[full_df['TransactionID'].isin(train_df['TransactionID'])][col].values\n",
    "    test_df[col] = full_df[full_df['TransactionID'].isin(test_df['TransactionID'])][col].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dependency(independent_var, dependent_var):\n",
    "    \n",
    "    independent_uniques = []\n",
    "    temp_df = pd.concat([train_df[[independent_var, dependent_var]], test_df[[independent_var, dependent_var]]])\n",
    "    \n",
    "    for value in temp_df[independent_var].unique():\n",
    "        independent_uniques.append(temp_df[temp_df[independent_var] == value][dependent_var].value_counts().shape[0])\n",
    "\n",
    "    values = pd.Series(data=independent_uniques, index=temp_df[independent_var].unique())\n",
    "    \n",
    "    N = len(values)\n",
    "    N_dependent = len(values[values == 1])\n",
    "    N_notdependent = len(values[values > 1])\n",
    "    N_null = len(values[values == 0])\n",
    "        \n",
    "    print(f'In {independent_var}, there are {N} unique values')\n",
    "    print(f'{N_dependent}/{N} have one unique {dependent_var} value')\n",
    "    print(f'{N_notdependent}/{N} have more than one unique {dependent_var} values')\n",
    "    print(f'{N_null}/{N} have only missing {dependent_var} values\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dependency('ProductCD','had_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['ProductCD','had_id']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "5.920768278891869e-06\n"
     ]
    }
   ],
   "source": [
    "print(train_df['C5'].isnull().sum()/train_df.shape[0])\n",
    "print(test_df['C5'].isnull().sum()/test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[~test_df['id_31'].isnull()]['C9'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 find dependency and fillna\n",
    "#'dist1', 'C3',只有test有C3的缺失,且只在dist1不缺失的时候缺失，dist1不缺失的时候C3全都是0\n",
    "test_df['C3'] = test_df['C3'].fillna(0)\n",
    "#'R_emaildomain', 'C5',只有test有C5的缺失，基本上都是在R_emaildomain不缺失的时候缺失，R_emaildomain缺失的C5缺失只有3个\n",
    "test_df['C5'] = test_df['C5'].fillna(0)\n",
    "#'id_30','C7',只有test有C7的缺失，只在id_30不缺失的时候缺失，id_30不缺失的C7缺失只有3个，其他都是0（Device）\n",
    "test_df['C7'] = test_df['C7'].fillna(0)\n",
    "#'id_31','C9',只有test有C9的缺失，只在id_31不缺失的时候缺失，id_31不缺失的C9缺失只有3个，其他都是0（Browser）\n",
    "test_df['C9'] = test_df['C9'].fillna(0)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare cards 5993\n",
      "No intersection in Train 10396\n",
      "Intersection in Train 580144\n",
      "####################\n",
      "No intersection in Train card2 0\n",
      "Intersection in Train card2 590540\n",
      "####################\n",
      "No intersection in Train card3 47\n",
      "Intersection in Train card3 590493\n",
      "####################\n",
      "No intersection in Train card4 0\n",
      "Intersection in Train card4 590540\n",
      "####################\n",
      "No intersection in Train card5 176\n",
      "Intersection in Train card5 590364\n",
      "####################\n",
      "No intersection in Train card6 30\n",
      "Intersection in Train card6 590510\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "#2. Keep intersactions\n",
    "for col in ['card1']: \n",
    "    valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    valid_card = valid_card[col].value_counts()\n",
    "    valid_card_std = valid_card.values.std()\n",
    "\n",
    "    invalid_cards = valid_card[valid_card<=2]\n",
    "    print('Rare cards',len(invalid_cards))\n",
    "\n",
    "    valid_card = valid_card[valid_card>2]\n",
    "    valid_card = list(valid_card.index)\n",
    "\n",
    "    print('No intersection in Train', len(train_df[~train_df[col].isin(test_df[col])]))\n",
    "    print('Intersection in Train', len(train_df[train_df[col].isin(test_df[col])]))\n",
    "    \n",
    "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "\n",
    "    train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)\n",
    "    print('#'*20)\n",
    "\n",
    "for col in ['card2','card3','card4','card5','card6']: \n",
    "    print('No intersection in Train', col, len(train_df[~train_df[col].isin(test_df[col])]))\n",
    "    print('Intersection in Train', col, len(train_df[train_df[col].isin(test_df[col])]))\n",
    "    \n",
    "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "    print('#'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "Most common uIds:\n",
      "########## uid1\n",
      "7919_194.0     14891\n",
      "9500_321.0     14112\n",
      "15885_545.0    10332\n",
      "17188_321.0    10312\n",
      "15066_170.0     7918\n",
      "12695_490.0     7079\n",
      "6019_583.0      6766\n",
      "12544_321.0     6760\n",
      "2803_100.0      6126\n",
      "7585_553.0      5325\n",
      "Name: uid1, dtype: int64\n",
      "########## uid2\n",
      "9500_321.0_150.0_226.0     14112\n",
      "15885_545.0_185.0_138.0    10332\n",
      "17188_321.0_150.0_226.0    10312\n",
      "7919_194.0_150.0_166.0      8844\n",
      "15066_170.0_150.0_102.0     7918\n",
      "12695_490.0_150.0_226.0     7079\n",
      "6019_583.0_150.0_226.0      6766\n",
      "12544_321.0_150.0_226.0     6760\n",
      "2803_100.0_150.0_226.0      6126\n",
      "7919_194.0_150.0_202.0      6047\n",
      "Name: uid2, dtype: int64\n",
      "########## uid3\n",
      "15885_545.0_185.0_138.0_nan_nan       9900\n",
      "17188_321.0_150.0_226.0_299.0_87.0    5862\n",
      "12695_490.0_150.0_226.0_325.0_87.0    5766\n",
      "9500_321.0_150.0_226.0_204.0_87.0     4647\n",
      "3154_408.0_185.0_224.0_nan_nan        4398\n",
      "12839_321.0_150.0_226.0_264.0_87.0    3538\n",
      "16132_111.0_150.0_226.0_299.0_87.0    3523\n",
      "15497_490.0_150.0_226.0_299.0_87.0    3419\n",
      "9500_321.0_150.0_226.0_272.0_87.0     2715\n",
      "5812_408.0_185.0_224.0_nan_nan        2639\n",
      "Name: uid3, dtype: int64\n",
      "########## uid4\n",
      "15885_545.0_185.0_138.0_nan_nan_hotmail.com     4002\n",
      "15885_545.0_185.0_138.0_nan_nan_gmail.com       3830\n",
      "17188_321.0_150.0_226.0_299.0_87.0_gmail.com    2235\n",
      "12695_490.0_150.0_226.0_325.0_87.0_gmail.com    2045\n",
      "9500_321.0_150.0_226.0_204.0_87.0_gmail.com     1947\n",
      "3154_408.0_185.0_224.0_nan_nan_hotmail.com      1890\n",
      "3154_408.0_185.0_224.0_nan_nan_gmail.com        1537\n",
      "12839_321.0_150.0_226.0_264.0_87.0_gmail.com    1473\n",
      "15775_481.0_150.0_102.0_330.0_87.0_uknown       1453\n",
      "15497_490.0_150.0_226.0_299.0_87.0_gmail.com    1383\n",
      "Name: uid4, dtype: int64\n",
      "########## uid5\n",
      "12695_490.0_150.0_226.0_325.0_87.0_uknown      5446\n",
      "17188_321.0_150.0_226.0_299.0_87.0_uknown      5322\n",
      "9500_321.0_150.0_226.0_204.0_87.0_uknown       4403\n",
      "15885_545.0_185.0_138.0_nan_nan_hotmail.com    4002\n",
      "15885_545.0_185.0_138.0_nan_nan_gmail.com      3830\n",
      "12839_321.0_150.0_226.0_264.0_87.0_uknown      3365\n",
      "16132_111.0_150.0_226.0_299.0_87.0_uknown      3212\n",
      "15497_490.0_150.0_226.0_299.0_87.0_uknown      3027\n",
      "9500_321.0_150.0_226.0_272.0_87.0_uknown       2601\n",
      "7664_490.0_150.0_226.0_264.0_87.0_uknown       2396\n",
      "Name: uid5, dtype: int64\n",
      "########## uid6\n",
      "15885.0_0.0    7398\n",
      "7919.0_0.0     4170\n",
      "6019.0_nan     3962\n",
      "nan_0.0        3754\n",
      "9500.0_0.0     3414\n",
      "3154.0_0.0     3016\n",
      "15066.0_0.0    2995\n",
      "9633.0_0.0     2968\n",
      "nan_nan        2794\n",
      "17188.0_0.0    2434\n",
      "Name: uid6, dtype: int64\n",
      "########## uid7\n",
      "15775_481.0_150.0_102.0_330.0_87.0_uknown_nan    1453\n",
      "12695_490.0_150.0_226.0_325.0_87.0_uknown_nan     928\n",
      "17188_321.0_150.0_226.0_299.0_87.0_uknown_nan     923\n",
      "9500_321.0_150.0_226.0_204.0_87.0_uknown_nan      622\n",
      "16132_111.0_150.0_226.0_299.0_87.0_uknown_nan     622\n",
      "12839_321.0_150.0_226.0_264.0_87.0_uknown_nan     580\n",
      "7207_111.0_150.0_226.0_204.0_87.0_uknown_nan      551\n",
      "7664_490.0_150.0_226.0_264.0_87.0_uknown_nan      545\n",
      "15497_490.0_150.0_226.0_299.0_87.0_uknown_nan     480\n",
      "9112_250.0_150.0_226.0_441.0_87.0_uknown_nan      439\n",
      "Name: uid7, dtype: int64\n",
      "########## cid_1\n",
      "15775_481.0_150.0_102.0_330.0_87.0_uknown_-129.0       1414\n",
      "9500_321.0_150.0_226.0_126.0_87.0_aol.com_85.0          404\n",
      "8528_215.0_150.0_226.0_387.0_87.0_uknown_159.0          207\n",
      "7207_111.0_150.0_226.0_204.0_87.0_uknown_465.0          189\n",
      "12741_106.0_150.0_226.0_143.0_87.0_gmail.com_202.0      156\n",
      "13597_198.0_150.0_226.0_191.0_87.0_yahoo.com_48.0       145\n",
      "4121_361.0_150.0_226.0_476.0_87.0_hotmail.com_8.0       141\n",
      "8900_385.0_150.0_226.0_231.0_87.0_uknown_60.0           132\n",
      "9323_111.0_150.0_226.0_191.0_87.0_charter.net_50.0      109\n",
      "3898_281.0_150.0_226.0_181.0_87.0_hotmail.com_188.0     106\n",
      "Name: cid_1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#3.generate accurate userids and cardids\n",
    "#uid1=card1_card2\n",
    "#uid2=card_1235\n",
    "#uid3=card_1235_addr12\n",
    "#uid4=card_1235_Pemail\n",
    "#uid5=card_1235_Remail\n",
    "\n",
    "train_df['uid4'] = train_df['uid3'].astype(str)+'_'+train_df['P_emaildomain'].astype(str)\n",
    "test_df['uid4'] = test_df['uid3'].astype(str)+'_'+test_df['P_emaildomain'].astype(str)\n",
    "\n",
    "train_df['uid5'] = train_df['uid3'].astype(str)+'_'+train_df['R_emaildomain'].astype(str)\n",
    "test_df['uid5'] = test_df['uid3'].astype(str)+'_'+test_df['R_emaildomain'].astype(str)\n",
    "\n",
    "train_df['uid6'] = train_df['card1'].astype(str)+'_'+train_df['D15'].astype(str)\n",
    "test_df['uid6'] = test_df['card1'].astype(str)+'_'+test_df['D15'].astype(str)\n",
    "\n",
    "#try to generate more accuracy card_id and user_id\n",
    "#uid1\\2 不太有使用的价值了\n",
    "\n",
    "#guess_card_id\n",
    "train_df['TransactionDTday'] = (train_df['TransactionDT']/(60*60*24)).map(int)\n",
    "test_df['TransactionDTday'] = (test_df['TransactionDT']/(60*60*24)).map(int)\n",
    "train_df['D1minusday'] = train_df['D1'] - train_df['TransactionDTday'] #发卡日\n",
    "test_df['D1minusday'] = test_df['D1'] - test_df['TransactionDTday']\n",
    "train_df['D4minusday'] = train_df['D4'] - train_df['TransactionDTday'] #发卡日\n",
    "test_df['D4minusday'] = test_df['D4'] - test_df['TransactionDTday']\n",
    "\n",
    "#这个应该对D1\\D2\\D3\\D8有效果,D2没必要动，D3/D8应该有别的用法\n",
    "train_df['cid_1'] = train_df['uid4'].astype(str)+'_'+train_df['D1minusday'].astype(str)\n",
    "test_df['cid_1'] = test_df['uid4'].astype(str)+'_'+test_df['D1minusday'].astype(str)\n",
    "\n",
    "#guess_user_id 用D4\n",
    "train_df['uid7'] = train_df['uid4'].astype(str)+'_'+train_df['D4minusday'].astype(str)\n",
    "test_df['uid7'] = test_df['uid4'].astype(str)+'_'+test_df['D4minusday'].astype(str)\n",
    "\n",
    "print('#'*10)\n",
    "print('Most common uIds:')\n",
    "new_columns = ['uid1','uid2','uid3','uid4','uid5','uid6','uid7','cid_1']\n",
    "for col in new_columns:\n",
    "    print('#'*10, col)\n",
    "    print(train_df[col].value_counts()[:10])\n",
    "\n",
    "# Do Global frequency encoding \n",
    "\n",
    "i_cols = ['card1','card2','card3','card5'] + new_columns\n",
    "train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. period counts\n",
    "for col in ['DT_M','DT_W','DT_D']:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "            \n",
    "    train_df[col+'_total'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_total']  = test_df[col].map(fq_encode)\n",
    "        \n",
    "#User period counts\n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "i_cols = ['uid4','uid5','uid6','uid7','cid_1']\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        new_column = col + '_' + period\n",
    "            \n",
    "        temp_df = pd.concat([train_df[[col,period]], test_df[[col,period]]])\n",
    "        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n",
    "        fq_encode = temp_df[new_column].value_counts().to_dict()\n",
    "            \n",
    "        train_df[new_column] = (train_df[col].astype(str) + '_' + train_df[period].astype(str)).map(fq_encode)\n",
    "        test_df[new_column]  = (test_df[col].astype(str) + '_' + test_df[period].astype(str)).map(fq_encode)\n",
    "        \n",
    "        train_df[new_column] /= train_df[period+'_total']\n",
    "        test_df[new_column]  /= test_df[period+'_total']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Prepare bank type feature\n",
    "for df in [train_df, test_df]:\n",
    "    df['bank_type'] = df['card3'].astype(str) +'_'+ df['card5'].astype(str)\n",
    "\n",
    "encoding_mean = {\n",
    "    1: ['DT_D','DT_hour','_hour_dist','DT_hour_mean'],\n",
    "    2: ['DT_W','DT_day_week','_week_day_dist','DT_day_week_mean'],\n",
    "    3: ['DT_M','DT_day_month','_month_day_dist','DT_day_month_mean'],\n",
    "    }\n",
    "\n",
    "encoding_best = {\n",
    "    1: ['DT_D','DT_hour','_hour_dist_best','DT_hour_best'],\n",
    "    2: ['DT_W','DT_day_week','_week_day_dist_best','DT_day_week_best'],\n",
    "    3: ['DT_M','DT_day_month','_month_day_dist_best','DT_day_month_best'],   \n",
    "    }\n",
    "\n",
    "train_df['DT_day_month'] = (train_df['DT'].dt.day).astype(np.int8)\n",
    "test_df['DT_day_month'] = (test_df['DT'].dt.day).astype(np.int8)\n",
    "# Some ugly code here (even worse than in other parts)\n",
    "for col in ['card3','card5','bank_type']:\n",
    "    for df in [train_df, test_df]:\n",
    "        for encode in encoding_mean:\n",
    "            encode = encoding_mean[encode].copy()\n",
    "            new_col = col + '_' + encode[0] + encode[2]\n",
    "            df[new_col] = df[col].astype(str) +'_'+ df[encode[0]].astype(str)\n",
    "\n",
    "            temp_dict = df.groupby([new_col])[encode[1]].agg(['mean']).reset_index().rename(\n",
    "                                                                    columns={'mean': encode[3]})\n",
    "            temp_dict.index = temp_dict[new_col].values\n",
    "            temp_dict = temp_dict[encode[3]].to_dict()\n",
    "            df[new_col] = df[encode[1]] - df[new_col].map(temp_dict)\n",
    "\n",
    "        for encode in encoding_best:\n",
    "            encode = encoding_best[encode].copy()\n",
    "            new_col = col + '_' + encode[0] + encode[2]\n",
    "            df[new_col] = df[col].astype(str) +'_'+ df[encode[0]].astype(str)\n",
    "            temp_dict = df.groupby([col,encode[0],encode[1]])[encode[1]].agg(['count']).reset_index().rename(\n",
    "                                                                    columns={'count': encode[3]})\n",
    "\n",
    "            temp_dict.sort_values(by=[col,encode[0],encode[3]], inplace=True)\n",
    "            temp_dict = temp_dict.drop_duplicates(subset=[col,encode[0]], keep='last')\n",
    "            temp_dict[new_col] = temp_dict[col].astype(str) +'_'+ temp_dict[encode[0]].astype(str)\n",
    "            temp_dict.index = temp_dict[new_col].values\n",
    "            temp_dict = temp_dict[encode[1]].to_dict()\n",
    "            df[new_col] = df[encode[1]] - df[new_col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. BankType timeblock_frequency_encoding\n",
    "i_cols = ['bank_type'] \n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "\n",
    "# We have few options to encode it here:\n",
    "# - Just count transactions\n",
    "# (but some timblocks have more transactions than others)\n",
    "# - Devide to total transactions per timeblock (proportions)\n",
    "# - Use both\n",
    "# - Use only proportions\n",
    "train_df, test_df = timeblock_frequency_encoding(train_df, test_df, periods, i_cols, \n",
    "                                 with_proportions=False, only_proportions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timehist1_2(col,product):\n",
    "    N = 8000 if col in ['TransactionAmt'] else 9999999999999999 # clip trans amount for better view\n",
    "    train_df[(train_df['isFraud'] == 0) & (train_df['ProductCD'] == product)].set_index('TransactionDT')[col].clip(0, N).plot(style='.', title='Hist ' + col, figsize=(15, 3))\n",
    "    train_df[(train_df['isFraud'] == 1) & (train_df['ProductCD'] == product)].set_index('TransactionDT')[col].clip(0, N).plot(style='.', title='Hist ' + col, figsize=(15, 3))\n",
    "    test_df[test_df['ProductCD'] == product].set_index('TransactionDT')[col].clip(0, N).plot(style='.', title=col + ' values over time (blue=no-fraud, orange=fraud, green=test)', figsize=(15, 3))\n",
    "    plt.show()\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "products=train_df.ProductCD.unique().tolist()\n",
    "col='D1'\n",
    "for prod in products: \n",
    "    print(\"Product code:\", prod)\n",
    "    timehist1_2(col, prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just make use of Ds with ProductCD\n",
    "train_df['TransactionDTday'] = (train_df['TransactionDT']/(60*60*24)).map(int)\n",
    "test_df['TransactionDTday'] = (test_df['TransactionDT']/(60*60*24)).map(int)\n",
    "train_df['D1minusday'] = train_df['D1'] - train_df['TransactionDTday']\n",
    "test_df['D1minusday'] = test_df['D1'] - test_df['TransactionDTday']\n",
    "#train_df[(train_df.isFraud==1) & (train_df.D1minusday==78)][['card1','card2','card3','card4','card5','card6','addr1','addr2','dist1','dist2','P_emaildomain','R_emaildomain','TransactionDTday']]\n",
    "col='D1minusday'\n",
    "for prod in ['W','H','C','S','R']: \n",
    "    print(\"Product code:\", prod)\n",
    "    timehist1_2(col, prod)\n",
    "train_df = train_df.drop(['D1minusday','TransactionDTday'],axis=1)\n",
    "test_df = test_df.drop(['D1minusday','TransactionDTday'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHCSR = {'W':1,'H':0,'C':1,'S':1,'R':0}\n",
    "def uid_CD_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
    "    train_df['TransactionDTday'] = (train_df['TransactionDT']/(60*60*24)).map(int)\n",
    "    test_df['TransactionDTday'] = (test_df['TransactionDT']/(60*60*24)).map(int)\n",
    "    train_df['productCD_sep'] = train_df['ProductCD'].map(WHCSR)\n",
    "    test_df['productCD_sep'] = test_df['ProductCD'].map(WHCSR)\n",
    "    for main_column in main_columns:  \n",
    "        train_df['Dminusday'] = train_df[main_column] - train_df['TransactionDTday']\n",
    "        test_df['Dminusday'] = test_df[main_column] - test_df['TransactionDTday']\n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_minusday_'+agg_type\n",
    "                Dminus = [col,'Dminusday']\n",
    "                temp_df = pd.concat([train_df[Dminus], test_df[Dminus]])\n",
    "                temp_df = temp_df.groupby([col])['Dminusday'].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "                \n",
    "    train_df = train_df.drop(['Dminusday','TransactionDTday'],axis=1)\n",
    "    test_df = test_df.drop(['Dminusday','TransactionDTday'],axis=1)\n",
    "    \n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Ds uid aggregations (maybe not useful)\n",
    "i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "uids = ['uid3','uid4','uid5','bank_type','cid1','uid6','uid7']\n",
    "aggregations = ['mean','min']\n",
    "'''\n",
    "####### uIDs ProductCD aggregations\n",
    "up_cols = ['D1','D2','D4','D5','D8','D10','D11','D15']\n",
    "train_df, test_df = uid_CD_aggregation(train_df, test_df, up_cols, uids, aggregations)\n",
    "\n",
    "#i_cols = [c for c in i_cols if c not in up_cols]\n",
    "####### uIDs aggregations\n",
    "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    "'''\n",
    "####### Cleaning Neagtive values and columns transformations\n",
    "for df in [train_df, test_df]:\n",
    "\n",
    "    for col in i_cols:\n",
    "        df[col] = df[col].clip(0) \n",
    "    \n",
    "    # Lets transform D8 and D9 column\n",
    "    # As we almost sure it has connection with hours\n",
    "    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n",
    "    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0)\n",
    "    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n",
    "    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n",
    "    df['D8'] = df['D8'].fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i_cols = ['D'+str(i) for i in range(1,16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
    "            temp_mean.index = temp_mean[period].values\n",
    "            temp_mean = temp_mean['mean'].to_dict()\n",
    "\n",
    "            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
    "            temp_std.index = temp_std[period].values\n",
    "            temp_std = temp_std['std'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
    "            dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "            dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
    "            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n",
    "    return dt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Ds period calculation (maybe not useful)\n",
    "#这个没有必要呢，LB也没有提高，CV有提升但看起来像过拟合……？\n",
    "\n",
    "####### Values Normalization\n",
    "i_cols.remove('D1')\n",
    "i_cols.remove('D2')\n",
    "i_cols.remove('D9')\n",
    "periods = ['DT_D','DT_W','DT_M']\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df = values_normalization(df, periods, i_cols)\n",
    "\n",
    "\n",
    "for col in ['D1','D2']:\n",
    "    for df in [train_df, test_df]:\n",
    "        df[col+'_scaled'] = df[col]/train_df[col].max()\n",
    "        \n",
    "####### Global Self frequency encoding\n",
    "# self_encoding=True because \n",
    "# we don't need original values anymore\n",
    "i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. TransAmt uids/cids aggregations and calculations（need more fe）\n",
    "i_cols = ['TransactionAmt','TransactionAmt_decimal']\n",
    "#uids = ['card1','card2','card3','card5','uid1','uid2','uid3','uid4','uid5','bank_type','uid6']\n",
    "uids = ['card1','card2','card3','card5','uid3','uid4','uid5','bank_type','uid6','uid7','cid_1']\n",
    "aggregations = ['mean','std','min']\n",
    "\n",
    "# uIDs aggregations\n",
    "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    "\n",
    "for df in [train_df,test_df]:\n",
    "    df['transAmt_mut_C1'] = df['TransactionAmt'] * df['C1']\n",
    "    df['transAmt_mut_C13'] = df['TransactionAmt'] * df['C13']\n",
    "    df['transAmt_mut_C14'] = df['TransactionAmt'] * df['C14']\n",
    "    df['transAmt_dec_diff'] = df['TransactionAmt_decimal'] - ((df['uid4_TransactionAmt_mean']-df['uid4_TransactionAmt_mean'].astype(int)) * 1000).astype(int)\n",
    "    df['Transdiff_in_uid'] = df['transAmt_dec_diff']*df['uid4_TransactionAmt_mean']/1000\n",
    "\n",
    "# TransactionAmt Normalization-period scaling\n",
    "periods = ['DT_D','DT_W','DT_M']\n",
    "for df in [train_df, test_df]:\n",
    "    df = values_normalization(df, periods, i_cols)\n",
    "# Product type\n",
    "train_df['product_type'] = train_df['ProductCD'].astype(str)+'_'+train_df['TransactionAmt'].astype(str)\n",
    "test_df['product_type'] = test_df['ProductCD'].astype(str)+'_'+test_df['TransactionAmt'].astype(str)\n",
    "\n",
    "i_cols = ['product_type']\n",
    "periods = ['DT_D','DT_W','DT_M']\n",
    "train_df, test_df = timeblock_frequency_encoding(train_df, test_df, periods, i_cols, \n",
    "                                                 with_proportions=False, only_proportions=True)\n",
    "train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_value_freq(sel_col,cum_per):\n",
    "    dfpercount = pd.DataFrame(columns=['col_name','num_values_'+str(round(cum_per,2))])\n",
    "    for col in sel_col:\n",
    "        col_value = train_df[col].value_counts(normalize=True)\n",
    "        colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n",
    "        colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n",
    "        if len(colpercount.loc[colpercount['cum_per_count'] < cum_per,] ) < 2:\n",
    "            num_col_99 = len(colpercount.loc[colpercount['per_count'] > (1- cum_per),]) #返回大头\n",
    "        else:\n",
    "            num_col_99 = len(colpercount.loc[colpercount['cum_per_count']< cum_per,] ) #返回小头\n",
    "        dfpercount=dfpercount.append({'col_name': col,'num_values_'+str(round(cum_per,2)): num_col_99},ignore_index = True)\n",
    "    dfpercount['unique_values'] = train_df[sel_col].nunique().values\n",
    "    dfpercount['unique_value_to_num_values'+str(round(cum_per,2))+'_ratio'] = 100 * (dfpercount['num_values_'+str(round(cum_per,2))]/dfpercount.unique_values)\n",
    "    #dfpercount['percent_missing'] = percent_na(train_transaction[sel_col])['percent_missing'].round(3).values\n",
    "    return dfpercount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  col_name num_values_0.96  unique_values unique_value_to_num_values0.96_ratio\n",
      "0       V1               1              2                                   50\n",
      "1       V2               2              9                              22.2222\n",
      "2       V3               2             10                                   20\n",
      "3       V4               2              7                              28.5714\n",
      "4       V5               2              7                              28.5714\n"
     ]
    }
   ],
   "source": [
    "#10. V cols\n",
    "#Understand V cols\n",
    "v_cols = ['V'+str(i) for i in range(1,340)]\n",
    "cum_per = 0.965\n",
    "colfreq=column_value_freq(v_cols,cum_per)\n",
    "print(colfreq.head())\n",
    "colfreq_bool = colfreq[colfreq.unique_values==2]['col_name'].values\n",
    "colfreq_pseudobool = colfreq[(colfreq.unique_values !=2) & (colfreq['num_values_'+str(round(cum_per,2))] <= 2)]\n",
    "colfreq_pseudobool_cat = colfreq_pseudobool[colfreq_pseudobool.unique_values <=15]['col_name'].values\n",
    "colfreq_pseudobool_num = colfreq_pseudobool[colfreq_pseudobool.unique_values >15]['col_name'].values\n",
    "colfreq_cat = colfreq[(colfreq.unique_values >15) & (colfreq['num_values_'+str(round(cum_per,2))] <= 15) & (colfreq['num_values_'+str(round(cum_per,2))]> 2)]['col_name'].values\n",
    "colfreq_num = colfreq[colfreq['num_values_'+str(round(cum_per,2))]>15]['col_name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_low2 = ['V304','V84','V252','V16','V194','V193','V17','V98','V15','V111','V101','V22','V18','V116','V334','V284',\n",
    " 'V32','V191','V297','V328','V104','V110','V196','V269','V31','V50','V114','V21','V302','V325','V113','V121','V118',\n",
    " 'V65','V14','V41','V240','V1','V89','V119','V27','V122','V68','V120','V241','V117','V305','V88','V107','V28']\n",
    "low_bool = [c for c in v_low2 if (c in colfreq_bool)]\n",
    "low_pesudoboolcat = [c for c in v_low2 if (c in colfreq_pseudobool_cat)]\n",
    "low_pesudoboolnum = [c for c in v_low2 if (c in colfreq_pseudobool_num)]\n",
    "low_cat = [c for c in v_low2 if (c in colfreq_cat)]\n",
    "low_num = [c for c in v_low2 if (c in colfreq_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['V96', 'V97', 'V99', 'V129', 'V135', 'V139', 'V140', 'V151',\n",
       "       'V152', 'V167', 'V168', 'V170', 'V171', 'V176', 'V177', 'V178',\n",
       "       'V179', 'V217', 'V218', 'V219', 'V221', 'V222', 'V228', 'V229',\n",
       "       'V230', 'V231', 'V232', 'V233', 'V234', 'V280', 'V282', 'V283',\n",
       "       'V285', 'V294', 'V319', 'V322', 'V323', 'V324', 'V326', 'V329',\n",
       "       'V336', 'V337', 'V338', 'V339'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colfreq_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['V96', 'V97', 'V99', 'V129', 'V135', 'V139', 'V140', 'V151',\n",
       "       'V152', 'V167', 'V168', 'V170', 'V171', 'V176', 'V177', 'V178',\n",
       "       'V179', 'V217', 'V218', 'V219', 'V221', 'V222', 'V228', 'V229',\n",
       "       'V230', 'V231', 'V232', 'V233', 'V234', 'V280', 'V282', 'V283',\n",
       "       'V285', 'V294', 'V319', 'V322', 'V323', 'V324', 'V326', 'V329',\n",
       "       'V336', 'V337', 'V338', 'V339'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colfreq_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#以Group的PCA作为新特征\n",
    "#PCA/ICA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA, FastICA,SparsePCA,KernelPCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "\n",
    "v_group_numcat_1 = ['V17', 'V18']\n",
    "v_group_numcat_2 = ['V37', 'V38', 'V39', 'V40', 'V44', 'V45']\n",
    "v_group_numcat_3 = ['V55','V56', 'V59', 'V60']\n",
    "v_group_numcat_4 = ['V77', 'V78', 'V80', 'V81', 'V86', 'V87']\n",
    "v_group_numcat_5 = ['V95', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106','V96', 'V97', 'V99', 'V129', 'V135']\n",
    "v_group_numcat_6 = ['V139', 'V140', 'V151','V152','V138', 'V146', 'V147', 'V148', 'V149', 'V153', 'V154', 'V155','V156', 'V157', 'V158', 'V161', 'V162', 'V163']\n",
    "v_group_numcat_7 = ['V167', 'V168', 'V170', 'V171', 'V176', 'V177', 'V178','V179', 'V169', 'V172','V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187','V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V195', 'V196','V198', 'V199', 'V200', 'V201']\n",
    "v_group_numcat_8 = ['V217', 'V218', 'V219', 'V221', 'V222', 'V228', 'V229','V230', 'V231', 'V232', 'V233', 'V234', 'V220', 'V223', 'V224', 'V225', 'V226', 'V227', 'V235', 'V236', 'V237', 'V238', 'V239', 'V242','V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250','V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258','V259', 'V261', 'V262', 'V269'] \n",
    "v_group_numcat_9 = ['V279', 'V281', 'V287', 'V290','V280', 'V282', 'V283','V285', 'V294', 'V319']\n",
    "v_group_numcat_10 = ['V291', 'V292', 'V293', 'V295', 'V296', 'V298', 'V299', 'V302','V303', 'V304', 'V311']\n",
    "v_group_numcat_11 = ['V327', 'V328', 'V330', 'V334','V322', 'V323', 'V324', 'V326', 'V329','V336', 'V337', 'V338', 'V339']\n",
    "vs = [v_group_numcat_1,v_group_numcat_2,v_group_numcat_3,v_group_numcat_4,v_group_numcat_5,v_group_numcat_6,v_group_numcat_7,v_group_numcat_8,v_group_numcat_9,v_group_numcat_10,v_group_numcat_11]\n",
    "for vcols in vs:\n",
    "    print('now in the',vcols)\n",
    "    train_df[vcols] = train_df[vcols].fillna(-999)\n",
    "    test_df[vcols] = test_df[vcols].fillna(-999)\n",
    "    #train_df[~train_df.isin([np.inf,np.nan])] = -999\n",
    "    #test_df[~test_df.isin([np.inf,np.nan])] = -999\n",
    "    \n",
    "    #PCA\n",
    "    print('PCA……')\n",
    "    pca = PCA(n_components = 1,random_state=42)\n",
    "    train_df['pca_' + vcols[0]] = pca.fit_transform(train_df[vcols])\n",
    "    test_df['pca_' + vcols[0]] = pca.transform(test_df[vcols])\n",
    "\n",
    "    # ICA\n",
    "    print('ICA……')\n",
    "    ica = FastICA(n_components = 1,random_state=42)\n",
    "    train_df['ica_' + vcols[0]] = ica.fit_transform(train_df[vcols])\n",
    "    test_df['ica_' + vcols[0]] = ica.transform(test_df[vcols])\n",
    "\n",
    "    # GRP\n",
    "    grp = GaussianRandomProjection(n_components = 1,eps=0.1, random_state=42)\n",
    "    train_df['grp_' + vcols[0]] = grp.fit_transform(train_df[vcols])\n",
    "    test_df['grp_' + vcols[0]] = grp.transform(test_df[vcols])\n",
    "\n",
    "    # SRP\n",
    "    srp = SparseRandomProjection(n_components = 1,dense_output=True, random_state=42)\n",
    "    train_df['srp_' + vcols[0]] = srp.fit_transform(train_df[vcols])\n",
    "    test_df['srp_' + vcols[0]] = srp.transform(test_df[vcols])\n",
    "    \n",
    "    # tSVD\n",
    "    print('tSVD……')\n",
    "    tsvd = TruncatedSVD(n_components = 1,random_state=42)\n",
    "    train_df['tsvd_' + vcols[0]] = tsvd.fit_transform(train_df[vcols])\n",
    "    test_df['tsvd_' + vcols[0]] = tsvd.transform(test_df[vcols])\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cliping v_num_cats\n",
    "vcol_spike = ['V96', 'V97','V167', 'V168','V177', 'V178','V179', 'V217', 'V218', 'V219','V231','V280', 'V282','V294', 'V322', 'V323', 'V324']\n",
    "cols = list(colfreq_pseudobool_num) + vcol_spike\n",
    "for df in [train_df, test_df]:\n",
    "    for col in cols :\n",
    "        max_value = train_df[train_df['DT_M']==train_df['DT_M'].min()][col].max()\n",
    "        df[col] = df[col].clip(None,max_value) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V126' 'V127' 'V128' 'V130' 'V131' 'V132' 'V133' 'V134' 'V136' 'V137'\n",
      " 'V143' 'V144' 'V145' 'V150' 'V159' 'V160' 'V164' 'V165' 'V166' 'V202'\n",
      " 'V203' 'V204' 'V205' 'V206' 'V207' 'V208' 'V209' 'V210' 'V211' 'V212'\n",
      " 'V213' 'V214' 'V215' 'V216' 'V263' 'V264' 'V265' 'V266' 'V267' 'V268'\n",
      " 'V270' 'V271' 'V272' 'V273' 'V274' 'V275' 'V276' 'V277' 'V278' 'V306'\n",
      " 'V307' 'V308' 'V309' 'V310' 'V312' 'V313' 'V314' 'V315' 'V316' 'V317'\n",
      " 'V318' 'V320' 'V321' 'V331' 'V332' 'V333' 'V335']\n",
      "V130    1.069942e-100\n",
      "V136     3.228515e-87\n",
      "V317     1.542568e-65\n",
      "V133     9.980904e-61\n",
      "V127     1.833679e-60\n",
      "            ...      \n",
      "V206     2.495735e-01\n",
      "V332     2.967873e-01\n",
      "V333     2.998484e-01\n",
      "V331     4.952229e-01\n",
      "V335     5.364810e-01\n",
      "Length: 67, dtype: float64\n",
      "67 67\n",
      "[3.95243705e-01 1.20604713e-01 9.08724136e-02 7.99145695e-02\n",
      " 5.93916129e-02 5.00332241e-02 4.54584312e-02 2.89428818e-02\n",
      " 2.32736617e-02 1.84120687e-02 1.45453003e-02 1.14526355e-02\n",
      " 7.81445065e-03 7.34786437e-03 5.85068362e-03 4.37949141e-03\n",
      " 4.02093888e-03 3.46559896e-03 3.18676729e-03 2.44932594e-03\n",
      " 2.27222589e-03 2.24909807e-03 1.99991560e-03 1.95987640e-03\n",
      " 1.71973338e-03 1.53540490e-03 1.51142993e-03 1.03556294e-03\n",
      " 9.71562292e-04 8.96170111e-04 8.77193459e-04 7.01838736e-04\n",
      " 6.96799764e-04 6.61501420e-04 5.95271089e-04 5.05089251e-04\n",
      " 4.25160153e-04 3.66978537e-04 3.32092303e-04 3.14215803e-04\n",
      " 3.07523162e-04 2.74325442e-04 2.09382351e-04 1.37651414e-04\n",
      " 1.31921650e-04 1.06460734e-04 9.48509909e-05 8.59092947e-05\n",
      " 7.21478151e-05 6.68975933e-05 5.36152094e-05 4.34649260e-05\n",
      " 3.15145841e-05 2.53136017e-05 2.00119609e-05 1.58136115e-05\n",
      " 1.04702022e-05 9.50144927e-06 5.76505723e-06 4.72613777e-06\n",
      " 2.37874562e-06 2.01718484e-06 5.65667558e-07 2.11127749e-07\n",
      " 7.10596417e-08 2.13648711e-08 9.31242653e-09]\n",
      "V216    0.000000e+00\n",
      "V215    0.000000e+00\n",
      "V333    0.000000e+00\n",
      "V316    0.000000e+00\n",
      "V265    0.000000e+00\n",
      "            ...     \n",
      "V310    9.931409e-60\n",
      "V130    1.223724e-54\n",
      "V204    6.822684e-54\n",
      "V209    3.987460e-53\n",
      "V312    5.351877e-44\n",
      "Length: 67, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0         0.000023\n",
       " 1         0.000009\n",
       " 2         0.000009\n",
       " 3         0.000015\n",
       " 4         0.000141\n",
       "             ...   \n",
       " 590535    0.000013\n",
       " 590536    0.000009\n",
       " 590537    0.000009\n",
       " 590538    0.000012\n",
       " 590539    0.000016\n",
       " Name: V335, Length: 590540, dtype: float64, 0         0.000009\n",
       " 1        -0.000035\n",
       " 2        -0.000038\n",
       " 3         0.000018\n",
       " 4        -0.000004\n",
       "             ...   \n",
       " 506686    0.000009\n",
       " 506687    0.000007\n",
       " 506688    0.000009\n",
       " 506689    0.000009\n",
       " 506690    0.000009\n",
       " Name: V335, Length: 506691, dtype: float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dealing with V cols\n",
    "#Scaling with pca - Numerical V cols - scaling仍需谨慎\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "v_cols = colfreq_num\n",
    "print(v_cols)\n",
    "test_group = list(v_cols)\n",
    "train_df['group_sum'] = train_df[test_group].to_numpy().sum(axis=1)\n",
    "train_df['group_mean'] = train_df[test_group].to_numpy().mean(axis=1)\n",
    "    \n",
    "test_df['group_sum'] = test_df[test_group].to_numpy().sum(axis=1)\n",
    "test_df['group_mean'] = test_df[test_group].to_numpy().mean(axis=1)\n",
    "compact_cols = ['group_sum','group_mean']\n",
    " \n",
    "for col in test_group:\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(train_df[[col]].fillna(0))\n",
    "    train_df[col] = sc.transform(train_df[[col]].fillna(0))\n",
    "    test_df[col] = sc.transform(test_df[[col]].fillna(0))\n",
    "    \n",
    "sc_test_group = test_group\n",
    "\n",
    "# check -> same obviously\n",
    "features_check = []\n",
    "from scipy.stats import ks_2samp #检查两个分布是否相同的函数\n",
    "for col in sc_test_group:\n",
    "    features_check.append(ks_2samp(train_df[col], test_df[col])[1])\n",
    "    \n",
    "features_check = pd.Series(features_check, index=sc_test_group).sort_values() \n",
    "print(features_check)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "#PCA还是必要的-是正交线性去噪\n",
    "pca = PCA(random_state=42)\n",
    "pca.fit(train_df[sc_test_group])\n",
    "print(len(sc_test_group), pca.transform(train_df[sc_test_group]).shape[-1])\n",
    "train_df[sc_test_group] = pca.transform(train_df[sc_test_group])\n",
    "test_df[sc_test_group] = pca.transform(test_df[sc_test_group])\n",
    "\n",
    "sc_variance =pca.explained_variance_ratio_\n",
    "print(sc_variance)\n",
    "\n",
    "# check\n",
    "features_check = []\n",
    "\n",
    "for col in sc_test_group:\n",
    "    features_check.append(ks_2samp(train_df[col], test_df[col])[1])\n",
    "    \n",
    "features_check = pd.Series(features_check, index=sc_test_group).sort_values() \n",
    "print(features_check)\n",
    "train_df[col], test_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Wanna some lag features\n",
    "'''\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def create_lag(df_code):\n",
    "    rollings=['60d','90d']\n",
    "    for rolling_type in rollings:\n",
    "        df_code.index = df_code.DT\n",
    "        rolled = df_code.drop(['DT'],axis=1).rolling(rolling_type)\n",
    "        df_code = df_code.join(rolled.median().add_suffix(f'_lag_{rolling_type}_median'))\n",
    "        df_code = df_code.join(rolled.max().add_suffix(f'_lag_{rolling_type}_max'))\n",
    "        df_code = df_code.join(rolled.mean().add_suffix(f'_lag_{rolling_type}_mean'))\n",
    "    return df_code.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def uid_rolling(df,main_columns,uidi):\n",
    "    N_THREADS=multiprocessing.cpu_count()\n",
    "    df_codes = df.groupby([uidi])\n",
    "    all_df = []\n",
    "    df_codes = [df_code[1][['DT']+main_columns] for df_code in df_codes]\n",
    "    pool = Pool(N_THREADS)\n",
    "    all_df = pool.map(create_lag, df_codes)\n",
    "    new_df = pd.concat(all_df)  \n",
    "    new_df.drop(main_columns,axis=1,inplace=True)\n",
    "    pool.close()\n",
    "    \n",
    "    return new_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "i_cols=['C1','C2','C4','C6','C7','C8','C10','C11','C12','C13','C14','V144','V145','V150','V159','V160']\n",
    "uids = ['uid1','uid2','uid3','uid4','uid5','bank_type']\n",
    "for col in uids:\n",
    "    train_df = uid_rolling(train_df, i_cols, col)\n",
    "    test_df = uid_rolling(test_df, i_cols, col)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df.reset_index(drop=True)\n",
    "#test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Cs frequency encode and clip\n",
    "i_cols = ['C'+str(i) for i in range(1,15)]\n",
    "\n",
    "####### Global Self frequency encoding\n",
    "# self_encoding=False because \n",
    "# I want to keep original values\n",
    "train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=False)\n",
    "\n",
    "####### uIDs aggregations\n",
    "#i_cols = ['C1','C2','C4','C6','C7','C11','C12','C14']\n",
    "#train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    "\n",
    "####### Clip max values-这就跟丢掉冬天一样了\n",
    "for df in [train_df, test_df]:\n",
    "    for col in i_cols:\n",
    "        max_value = train_df[train_df['DT_M']==train_df['DT_M'].max()][col].max()\n",
    "        df[col] = df[col].clip(None,max_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#想查看下分布呢\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.distplot(train_df['V145'], kde=True, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(test_df['V145'],kde=True, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对看起来没处理好的Vcols做一下处理：\n",
    "'''\n",
    "train_df[[f'V{i}_diff1' for i in range(126, 138)]] = train_df.groupby(['card1', 'ProductCD', 'addr1'])[[f'V{i}' for i in range(126, 138)]].diff() \n",
    "test_df[[f'V{i}_diff1' for i in range(126, 138)]] = test_df.groupby(['card1', 'ProductCD', 'addr1'])[[f'V{i}' for i in range(126, 138)]].diff() \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set shape before merge: (590540, 665)\n",
      "train_set shape after merge: (590540, 665)\n",
      "test_set shape before merge: (506691, 665)\n",
      "test_set shape after merge: (506691, 665)\n"
     ]
    }
   ],
   "source": [
    "#13. More combinations\n",
    "## Identity columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in ['id_33']:\n",
    "    train_identity[col] = train_identity[col].fillna('unseen_before_label')\n",
    "    test_identity[col]  = test_identity[col].fillna('unseen_before_label')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train_identity[col])+list(test_identity[col]))\n",
    "    train_identity[col] = le.transform(train_identity[col])\n",
    "    test_identity[col]  = le.transform(test_identity[col])\n",
    "    \n",
    "print('train_set shape before merge:',train_df.shape)\n",
    "train_df1 = train_df.merge(train_identity,how='left',on=['TransactionID'])\n",
    "print('train_set shape after merge:',train_df.shape)\n",
    "\n",
    "print('test_set shape before merge:',test_df.shape)\n",
    "test_df1 = test_df.merge(test_identity,how='left',on=['TransactionID'])\n",
    "print('test_set shape after merge:',test_df.shape)\n",
    "\n",
    "# New feature - mean of sth\n",
    "columns_a = ['TransactionAmt', 'id_02', 'D15']\n",
    "columns_b = ['card1', 'card4', 'addr1']\n",
    "for col_a in columns_a:\n",
    "    for col_b in columns_b:\n",
    "        for df in [train_df1, test_df1]:\n",
    "            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('mean')\n",
    "            df[f'{col_a}_to_std_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('std')\n",
    "del columns_a,columns_b\n",
    "gc.collect()\n",
    "\n",
    "# Some arbitrary features interaction 试做联合特征(?????)\n",
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n",
    "                    'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1','card1__id_02']:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    train_df1[feature] = train_df1[f1].astype(str) + '_' + train_df1[f2].astype(str)\n",
    "    test_df1[feature] = test_df1[f1].astype(str) + '_' + test_df1[f2].astype(str)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train_df1[feature].astype(str).values) + list(test_df1[feature].astype(str).values))\n",
    "    train_df1[feature] = le.transform(list(train_df1[feature].astype(str).values))\n",
    "    test_df1[feature] = le.transform(list(test_df1[feature].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df1\n",
    "test_df = test_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['had_id'] = train_df['had_id'].fillna(0)\n",
    "test_df['had_id'] = test_df['had_id'].fillna(0)\n",
    "def uid_sep_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_sep_'+agg_type\n",
    "                \n",
    "                train_df[col+'_sep'] = train_df[col].astype(str)+train_df['had_id'].astype(str)\n",
    "                test_df[col+'_sep'] = test_df[col].astype(str)+test_df['had_id'].astype(str)\n",
    "                \n",
    "                temp_df = pd.concat([train_df[[col+'_sep', main_column]], test_df[[col+'_sep',main_column]]])\n",
    "                temp_df = temp_df.groupby([col+'_sep'])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "                \n",
    "                temp_df.index = list(temp_df[col+'_sep'])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "                \n",
    "                train_df[new_col_name] = train_df[col+'_sep'].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col+'_sep'].map(temp_df)\n",
    "                del train_df[col+'_sep'],test_df[col+'_sep']\n",
    "    return train_df, test_df\n",
    "\n",
    "def values_sep_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_sep_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            dt_df[period+'_sep'] = dt_df[period].astype(str)+dt_df['had_id'].astype(str)     \n",
    "                \n",
    "            temp_min = dt_df.groupby([period+'_sep'])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period+'_sep'].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period+'_sep'])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period+'_sep'].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            temp_mean = dt_df.groupby([period+'_sep'])[col].agg(['mean']).reset_index()\n",
    "            temp_mean.index = temp_mean[period+'_sep'].values\n",
    "            temp_mean = temp_mean['mean'].to_dict()\n",
    "\n",
    "            temp_std = dt_df.groupby([period+'_sep'])[col].agg(['std']).reset_index()\n",
    "            temp_std.index = temp_std[period+'_sep'].values\n",
    "            temp_std = temp_std['std'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period+'_sep'].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period+'_sep'].map(temp_max)\n",
    "            dt_df['temp_mean'] = dt_df[period+'_sep'].map(temp_mean)\n",
    "            dt_df['temp_std'] = dt_df[period+'_sep'].map(temp_std)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "            dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
    "            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std'],dt_df[period+'_sep']\n",
    "    return dt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.1 TransAmt seperated by had_id(Online/Traditional)\n",
    "#分online/traditional来groupbyuid\n",
    "i_cols = ['TransactionAmt','TransactionAmt_decimal']\n",
    "uids = ['uid3','uid4','uid5','bank_type','uid6','uid7','cid_1']\n",
    "aggregations = ['mean','std','min']\n",
    "\n",
    "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    "\n",
    "#分online/traditional来normalization\n",
    "periods = ['DT_D','DT_W','DT_M']\n",
    "for df in [train_df, test_df]:\n",
    "    df = values_sep_normalization(df, periods, i_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Encoding\n",
      "Encoding card4\n",
      "{'visa': 722693, 'mastercard': 348803, 'american express': 16078, 'discover': 9572}\n",
      "Encoding card6\n",
      "{'debit': 828379, 'credit': 268753, 'charge card': 16}\n",
      "Encoding ProductCD\n",
      "{'W': 800657, 'C': 137785, 'R': 73346, 'H': 62397, 'S': 23046}\n",
      "Encoding P_emaildomain\n",
      "{'gmail.com': 435803, 'yahoo.com': 182784, 'uknown': 163648, 'hotmail.com': 85649, 'anonymous.com': 71062, 'aol.com': 52337, 'comcast.net': 14474, 'icloud.com': 12316, 'outlook.com': 9934, 'att.net': 7647, 'msn.com': 7480, 'sbcglobal.net': 5767, 'live.com': 5720, 'verizon.net': 5011, 'ymail.com': 4075, 'bellsouth.net': 3437, 'yahoo.com.mx': 2827, 'me.com': 2713, 'cox.net': 2657, 'optonline.net': 1937, 'live.com.mx': 1470, 'charter.net': 1443, 'mail.com': 1156, 'rocketmail.com': 1105, 'gmail': 993, 'earthlink.net': 979, 'outlook.es': 863, 'mac.com': 862, 'hotmail.fr': 674, 'hotmail.es': 627, 'frontier.com': 594, 'roadrunner.com': 583, 'juno.com': 574, 'windstream.net': 552, 'web.de': 518, 'aim.com': 468, 'embarqmail.com': 464, 'twc.com': 439, 'frontiernet.net': 397, 'netzero.com': 387, 'centurylink.net': 386, 'q.com': 362, 'yahoo.fr': 344, 'hotmail.co.uk': 334, 'suddenlink.net': 323, 'netzero.net': 319, 'cfl.rr.com': 318, 'cableone.net': 311, 'prodigy.net.mx': 303, 'gmx.de': 298, 'sc.rr.com': 277, 'yahoo.es': 272, 'protonmail.com': 159, 'ptd.net': 140, 'yahoo.de': 137, 'hotmail.de': 130, 'live.fr': 106, 'yahoo.co.uk': 103, 'yahoo.co.jp': 101, 'servicios-ta.com': 80, 'scranton.edu': 2}\n",
      "Encoding R_emaildomain\n",
      "{'uknown': 824070, 'gmail.com': 118885, 'hotmail.com': 53166, 'anonymous.com': 39644, 'yahoo.com': 21405, 'aol.com': 7239, 'outlook.com': 5011, 'comcast.net': 3513, 'icloud.com': 2820, 'yahoo.com.mx': 2743, 'msn.com': 1698, 'live.com.mx': 1464, 'live.com': 1444, 'verizon.net': 1202, 'sbcglobal.net': 1163, 'me.com': 1095, 'att.net': 870, 'cox.net': 854, 'outlook.es': 853, 'bellsouth.net': 795, 'hotmail.fr': 667, 'hotmail.es': 595, 'web.de': 514, 'mac.com': 430, 'ymail.com': 405, 'optonline.net': 350, 'mail.com': 341, 'hotmail.co.uk': 317, 'yahoo.fr': 315, 'prodigy.net.mx': 303, 'gmx.de': 297, 'charter.net': 263, 'gmail': 196, 'earthlink.net': 170, 'embarqmail.com': 140, 'yahoo.de': 139, 'hotmail.de': 130, 'rocketmail.com': 126, 'yahoo.es': 124, 'juno.com': 111, 'frontier.com': 110, 'live.fr': 105, 'windstream.net': 104, 'yahoo.co.jp': 104, 'roadrunner.com': 101, 'yahoo.co.uk': 82, 'servicios-ta.com': 80, 'aim.com': 77, 'protonmail.com': 75, 'ptd.net': 70, 'scranton.edu': 69, 'twc.com': 61, 'cfl.rr.com': 57, 'suddenlink.net': 55, 'cableone.net': 46, 'q.com': 45, 'frontiernet.net': 38, 'centurylink.net': 28, 'netzero.com': 24, 'netzero.net': 19, 'sc.rr.com': 14}\n",
      "Encoding M4\n",
      "{'M0': 357789, 'M2': 122947, 'M1': 97306}\n",
      "Mem. usage decreased to 1058.51 Mb (55.1% reduction)\n",
      "Mem. usage decreased to 914.92 Mb (54.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "#14. Category Encoding\n",
    "print('Category Encoding')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "## card4, card6, ProductCD\n",
    "# Converting Strings to ints(or floats if nan in column) using frequency encoding\n",
    "# We will be able to use these columns as category or as numerical feature\n",
    "\n",
    "\n",
    "for col in ['card4', 'card6', 'ProductCD']:\n",
    "    print('Encoding', col)\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(col_encoded) #多分类用出现次数作为编码\n",
    "    test_df[col]  = test_df[col].map(col_encoded)\n",
    "    print(col_encoded)\n",
    "    del temp_df,col_encoded\n",
    "    gc.collect()\n",
    "\n",
    "## M columns\n",
    "# Converting Strings to ints(or floats if nan in column)\n",
    "\n",
    "for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n",
    "    train_df[col] = train_df[col].map({'T':1, 'F':0})\n",
    "    test_df[col]  = test_df[col].map({'T':1, 'F':0})\n",
    "\n",
    "for col in ['P_emaildomain', 'R_emaildomain','M4']:\n",
    "    print('Encoding', col)\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(col_encoded)\n",
    "    test_df[col]  = test_df[col].map(col_encoded)\n",
    "    print(col_encoded)\n",
    "    del temp_df,col_encoded\n",
    "    gc.collect()\n",
    "    \n",
    "i_cols = ['TransactionAmt']\n",
    "uids = ['card2__id_20','card1__id_02']\n",
    "aggregations = ['mean','std']\n",
    "\n",
    "# uIDs aggregations\n",
    "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    " \n",
    "    \n",
    "## Reduce Mem One More Time\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df  = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df  = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export\n",
    "train_df.to_pickle('train_transaction_15.pkl')\n",
    "test_df.to_pickle('test_transaction_15.pkl')\n",
    "\n",
    "#train_identity.to_pickle('train_identity_new01.pkl')\n",
    "#test_identity.to_pickle('test_identity_new01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#c_features = ['V'+str(i) for i in range(103,105)]\n",
    "#c_features = [c for c in c_features if c not in v_low2]\n",
    "c_features = ['V17']\n",
    "for i in c_features:\n",
    "    cor = np.corrcoef(train_df['TransactionDT'], train_df[i])[0,1]\n",
    "    train_df.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n",
    "    test_df.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['is_december']==1]['DT_M'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['is_december']==1]['DT_M'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[train_df['C14']==1429]['DT_day_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['duplicate_count']#看起来是个挺好的特征呀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['had_id'].isnull().sum()/test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
